# Structure equation models

Structural Equation Modeling (SEM): is an extremely broad and flexible framework for data analysis, perhaps better thought of as a family of related methods rather than as a single technique. Measuring latent constructs is challenging and we must also incorporate estimates of measurement error into our models. SEM excels at both of these tasks. SEM is especially suited for causal analysis.

Path analysis (structural equations)

Path diagrams = Communicates the structure of our model. Useful for structural equation models.

Rectangular = any variable that we can observe directly (observed variable), measured variables

Circle / Ovals = Cannot be observed (Latent variable)

Arrow = directed effect. One variable impacts the other. Hypothetical causal relationship.

Numbers by the arrows = regression coefficient. Correlations coefficients.

![](images/paste-C79DE382.png){width="352"}

Triangle is the constant in the Linear Model.

![](images/paste-948ED17C.png){width="81"}

Double handed arrows = indicate covariances or correlations without a causal interpretation.

SEM -\> style="ram" to get circles around the measurement arrow

Double handed arrow between two independent variables = they are correlated to each other. Covariance

![](images/paste-D2D686C6.png){width="350"}

Residual error term = measurement errors. We expect that the factor will not perfectly predict the observed variables.

The bidirected arrows, the ones with two side arrows (in this representation, a connecting line with no arrows) represent covariances among variables.

OLS = Ordinary least-squares (OLS) models assume that the analyst is fitting a model of a relationship between one or more explanatory variables and a continuous or at least interval outcome variable that minimizes the sum of square errors, where an error is the difference between the actual and the predicted value of the outcome variable. The most common analytic method that utilizes OLS models is linear regression (with a single or multiple predictor variables).

OLS: b --\> beta \* k

Latent variables = unobserved variables or unmeasured variables in SEM lingo. These are theoretical concepts which can be inferred but not directly measured.

Linear regression model = Y = alpha + betaX + error

The model has to account for randomization. If there is a factor that that cannot be explained, it is included in the error. When this unmeasured factor that is in the error is correlated to another independent variable, there is endogeneity.

Endogeneity variables = correlated with the error terms. Arises when the marginal distribution of the independent variable is not independent of the conditional distribution of the dependent variable given the independent.

Exogenous variables = not driven by other factors (observable or observable)

Sources of endogeneity =

    1. Omitted variables: relevant variables left out of the model,  attributing to effect to those that were included. 
    2. Simultaneity: where x causes y and y causes x
    3. Selection bias: sampling bias

------------------------------------------------------------------------

Multiple regression

We can assume that the independent variables are correlated.

The residual error in multiple regression analysis is actually an unobserved, latent variable. The residual error is 1, to achieve identification.

Estimator = MLM in the model, protects for non-linearity, non-normality and elasticity of the raw data. Need the raw data, not only the covariance.

Structural equation = y = beta\*x + e; var(y)=cov(y,y)

Beta\^2\* var(x) + var(e-) = y

------------------------------------------------------------------------

Divide true variance of the true / observed variance = reliability

Cov(1,2) = sum (1- mean(1) \* (2-mean(2) / n

Covariance is a measure of how much two random variables vary together. It's similar to variance, but where variance tells you how a single variable varies, co variance tells you how two variables vary together.

Difference between covariance and correlation.

![](images/paste-BCC7A4B7.png){width="353"}

------------------------------------------------------------------------

### Reliability

In statistics reliability is the consistency of a set of measurements or measuring instrument, often used to describe a test. This can either be whether the measurements of the same instrument give or are likely to give the same measurement (test-retest), or in the case of more subjective instruments, such as personality or trait inventories, whether two independent assessors give similar scores (inter-rater-reliability). Reliability is inversely related to random error.

$X = T + e$

$Reliability of X = var(T)  / var(T) + var(e)$

$var(Tintake) / var(Ointake) = 1 - var(error) / var(Ointake)$

In words, reliability is defined as a proportion of observed variance that is true variance. Reliability is interpreted as a proportion---reliability cannot be negative.

Reliability = 1- Latent variable variance / variance variable 1

K = variance(true)/variance(Observed(true+error))

    1- Measurement error variable 1 / measurement error variable 2 (latent varible)

<https://www.uwo.ca/fhs/tc/labs/07.Reliability.pdf>

Various kinds of reliability coefficients, with values ranging between 0.00 (much error) and 1.00 (no error), are usually used to indicate the amount of error in the scores." [2] For example, measurements of people's height and weight are often extremely reliable. \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_

**Testing the model**

fitMeasures(fitLV) [ c("chisq", "df", pvalue", "rmsea")]

MI = if i liberate one parameter, then \_\_ there would

Model chi-square test. We test whether the fitted model is correct. HO: moment restrictions implied by the model hold. The fit is correct If \> 0.05 we cannot reject the model. I accept the model.

Therefore, the chi-square test allows researchers to evaluate the fitness of a model by using the null hypothesis significance test approach.

The Root Mean Square Error of Approximation (RMSEA) = fit index: how the covariance fit in the model. Difference between observed and the fitted.

The RMSEA is widely used in Structural Equation Modeling to provide a mechanism for adjusting for sample size where chi-square statistics are used. Measures the discrepancy due to the approximation per degree of freedom.

The objective is to have the RMSEA as low as possible.

------------------------------------------------------------------------

**MLM estimation**

Raw data is needed.

Independent variables have variances and covariances, unless the model specification puts them to zero.

The square of the standardized loading is the reliability of the variable. For example, the reliability k of AM1 is .59\*\*2 = 0.35 .

Measurement equation = TSE =\~ 1*SE1 + 1*SE2

Regression equation PERF \~ TS + TSE + VERB

![](images/paste-8481FC40.png)

Degrees of freedom (df) = Number of observations available for model estimation - Number of observations used to estimate parameters

df for empty model = ( k(k-1) ) / 2

"Number of free parameters" refers to all of the things that this model estimated freely

Parameter is a regression coefficient when standardized is called a beta coefficient.

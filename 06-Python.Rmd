---
editor_options: 
  markdown: 
    wrap: 72
---

```{r, echo=FALSE, warning=FALSE, message=FALSE}
library(knitr)  
library(reticulate)
os <- import("os")
os$listdir(".")
options(reticulate.repl.quiet = FALSE)
use_python("/usr/local/bin/python")
```

```{r, echo=FALSE}
knitr::opts_chunk$set(
  echo = TRUE, 
  warning = FALSE, 
  cache.lazy = FALSE, 
  message=FALSE
)
```

```{python}
# This Python file uses the following encoding: utf-8
import os, sys
```

```{python, warning=FALSE, message=FALSE, comment=FALSE}
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from matplotlib.pyplot import figure
import seaborn as sns
import os
```

# Basics Python

*Class given by: Alberto Santini*

```{r}
y = 5 + 5
y
```

Printing characters

```{r}
print('Hello, readers!')
```

Printing numbers

```{r}
print(15)
```

Printing length of a value

```{python, warning=FALSE, message=FALSE}
Length = len('Danielle')
print(Length)
```

------------------------------------------------------------------------

## Data set

Loading the data set & viewing head + tails:

```{python}
sns.set_context('paper')

tips = sns.load_dataset('tips')

tips.head()
tips.tail()
```

1.  Length of the variable
2.  Shape: Number of rows and number of columns
3.  Type of variables + basic info
4.  Descriptive statistics variable

```{python}
len(tips)

tips.shape

tips.info()

tips.describe()
```

------------------------------------------------------------------------

## Matrixes

Series:

Panda series method:

```{python}
Serie1 = pd.Series(
  [4200, 8000, 6500],
  index=["Amsterdam", "Toronto", "Tokyo"]
  )
Serie1
```

Python dictionary method:

```{python}
Serie2 = pd.Series({"Amsterdam": 5, "Tokyo": 8})
Serie2
```

Data frame:

```{python}
Combined_serie = pd.DataFrame({
  "Revenue": Serie1,
  "Employee_count": Serie2
  })
  
Combined_serie
```

Sub-setting by row:

```{python}
Combined_serie["Tokyo":]
```

**Creating our own functions:**

Saying hello + name

```{python, warning=FALSE, message=FALSE}
def printing_name(name):
  print('Good morning,', name)
printing_name('Danielle')
```

Multiple arguments:

Saying hello + name + location

```{python, warning=FALSE, message=FALSE}
def welcome(name, location):
  print("Good morning", name, "Welcome to", location)
welcome("Danielle,", "class.")
```

**Bar plot**

```{python}
sns.set_context('paper')

tips = sns.load_dataset("tips")
tips.head()

sns.barplot(x = 'day', y = 'total_bill', hue = 'sex', data = tips, palette = 'Blues', edgecolor = 'w')
plt.show()

tips.groupby(['day','sex']).mean()
```

**1-hot-encoding:**

Transforming categorical features to values from 0 or 1. F.e. You can be
from origin: America or Europe. If the observation is from America it
receives a 1 for America and a 0 for Europe.

**Reading from a csv file with pandas:**

```{python}
import pandas as pd

d = pd.read_csv('www/auto-mpg.csv')
```

```{python}
d.mpg.mean().round() # rounding by amount of decimals
```

```{python}
d.dtypes # describes the type of variables
pd.to_numeric(d.hp) # transforms to numerical
```

```{python}
len(d) # amount of rows
d.shape # amount of rows & columns
d.columns # gives the names of the columns
mpg = d.mpg # shows me only the values from one variable
mpg = d['mpg'] # alternative way to show only one value
d.head() # shows the first 5 observations of a data set
d.tail() # shows the last 5 observations of a data set
```

**Basic statistics:**

```{python}
d.mpg.mean()
d.mpg.median()
d.mpg.max()
d.mpg.min()
d.mean() # mean for all columns
d.describe().round(2).head() # statistics for the whole data set, rounded to two decimals
```

```{python}
d[['year']] # defines a list of one variable
d[['year', 'cylinders']] # defines a list of multiple variables
d.year.unique() # gives me the unique values of that column, not the repetitions
```

------------------------------------------------------------------------

## Filtering a data set

```{python}
d77 = d[d.year == 77] #filters the data set to all observations that are equal to a certain value of a variable 

d77 = d[d.year != 77] #filters the data set to all observations that are NOT equal to a certain value of a variable 

d77 = d[d.year <= 77] # filters those that are smaller than a value

d77 = d[d.year >= 77] # filters those that are large than a value

d77 = d[(d.year >= 77) & (d.year <= 90)] # in between certain values

d77 = d[(d.year == 80) | (d.year == 90)] # those from one value OR another

d77 = d[~(d.year == 70)] # excludes the values
```

**Aggregating**

```{python}
dm = d.groupby('year').mean()

dm = d.groupby(['year', 'cylinders']).mean()

dm = d.groupby(['year', 'cylinders']).median()

dm = d.groupby(['year', 'cylinders']).mean()['mpg'] # for selected variables only
```

**Pivot table**

```{python}
d.pivot_table(index='year', columns='cylinders', values='mpg').round()

d.pivot_table(index='year', columns='cylinders', values='mpg').round().fillna('')
```

**Creating new columns starting from existing columns**

```{python}
d['nam_of_new_column'] = d.mpg * 2
```

Through the package numpy:

```{python}
import numpy as np

d['sqrt_of_mpg_2'] = np.sqrt(d.mpg)

d['log(10)_of_mpg'] = np.log10(d.mpg)
```

**Dropping / deleting columns**

```{python}
d['double_mpg'] = d.mpg * 2

del(d['double_mpg']) # deleting columns

d.drop(columns=['sqrt_of_mpg_2', 'log(10)_of_mpg', 'nam_of_new_column']).head() # dropping columns
```

------------------------------------------------------------------------

## Data imputation

```{python}
pd.to_numeric(d.hp, errors='coerce').head()

d.hp = pd.to_numeric(d.hp, errors='coerce')

d[d.hp.isna()] # transform values to NA

d[-(d.hp.isna())] # delete missing values
```

------------------------------------------------------------------------

## Data visualization

Getting the correlation matrix

**Correlation matrix**

```{python}
d.corr().round(decimals=2)
```

Pair plot

```{python}
#sns.pairplot(d);

#sns.pairplot(d, hue='origin'); # with color
```


```{python}
tips = sns.load_dataset("tips")
sns.set_theme(style="whitegrid")

sns.boxplot(data=tips, x='sex', y='tip')
plt.show()
```


```{python}
sns.boxplot(data=tips, x='sex', y='tip', color='black', boxprops=dict(alpha=.6))
plt.show()

sns.boxplot(x="day", y="total_bill", hue="smoker",
  data=tips, palette="Set3")
plt.show()
```

```{python}
tips['weekend'] = tips.day.isin(['Sat', 'Sun'])

sns.boxplot(data=tips, x='day', y='tip', hue='weekend', dodge=False)
plt.show()
```


```{python}
sns.catplot(data=tips, x='smoker', y='tip', hue='sex', col='time', kind='box');
plt.show()
```

### Other categorical plots

```{python}
sns.stripplot(data=tips, x='sex', y='tip');
plt.show()
```

```{python}
sns.swarmplot(data=tips, x='sex', y='tip');
plt.show()

sns.swarmplot(data=tips, x='sex', y='tip', hue='smoker');
plt.show()

sns.swarmplot(data=tips, x='sex', y='tip', hue='smoker', dodge=True);
plt.show()
```


```{python}
sns.boxplot(data=tips, x='sex', y='tip', fliersize=8)
sns.swarmplot(data=tips, x='sex', y='tip', color='black', alpha=0.6);
plt.show()
```


```{python}
sns.boxenplot(data=tips, x='sex', y='tip');
plt.show()

sns.boxenplot(data=tips, x='sex', y='tip', hue='smoker');
plt.show()
```





***

```{python}
sns.displot(data=tips, x='tip');
plt.show()

sns.displot(data=tips, x='tip', kind='kde');
plt.show()
```

**Multiple plots**

```{python}
fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12,6))
sns.boxenplot(data=tips, x='sex', y='tip', ax=axes.flat[0]);
sns.violinplot(data=tips, x='sex', y='tip', ax=axes.flat[1]);
plt.show()
```


**Multidimensional KDE for numeric variables**

```{python}
plt.rcParams['figure.figsize'] = (10,8)
sns.kdeplot(data=tips, x='tip', y='total_bill', shade=True, cbar=True);
plt.show()
```

```{python}
sns.histplot(d.mpg); # Histogram
plt.show()
```

```{python}
sns.histplot(d.mpg, bins=20); # with binwidth
plt.show()

sns.histplot(data=d, x='mpg', bins=20); 

sns.histplot(data=d, x='mpg', bins=20, cumulative=True); # cumulative values
```

```{python}
#d.origin = pd.Categorical(d.origin.replace({1: 'america', 2:'europe' 3: 'japan'}))

sns.relplot(data=d, x='hp', y='mpg', hue='origin', palette='tab10')
plt.show()
```

```{python}
corr = d.corr()
sns.heatmap(corr) # simple
plt.show()

sns.heatmap(corr, cmap=plt.cm.RdYlGn, annot=True, linewidths=1, square=True, vmin=-1, vmax=1) # designed
plt.show()
```

### Preparing the data

```{r, echo=FALSE}
library(tidyverse)
py_install("scikit-learn")
```

```{python}
from sklearn.model_selection import train_test_split
```


Creates the input points and the output columns

```{python}
label = 'mpg'
features = [column for column in d.columns if column != label]
```

Setting the train / test split

```{python}
X = d[features]
y = d[label]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)

len(X) # Size of the dataset
len(X_train) # Size of the training set
len(X_test) # Size of the test set (In this case 25% of the dataset)
```

There is also a way to avoid the randomness of the test selection:

```{python}
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0) # We fix the random state to a certain value. 
```


### Creating the models

- Linear regression (no hyperparameters)
- Quadratic regression (no hyperparameters)
- Quadratic regression with LASSO (1 hyperparameter)

```{python}
from sklearn.linear_model import LinearRegression, Lasso
```


```{python}
from sklearn.preprocessing import StandardScaler, PolynomialFeatures # automatically takes into account standardization

from sklearn.model_selection import GridSearchCV, ShuffleSplit

from sklearn.pipeline import make_pipeline

from sklearn.metrics import mean_squared_error
```

**Pipeline**

A pipeline is a sequence of steps applied to the data. f.e. the first step is scaling the data and is training the model on the training data.

Due to the package sklearn (StandardScaler), the training data is first standardized and then this is used for the training data and test data.

```{python}
# Linear regression
linreg = make_pipeline(StandardScaler(), LinearRegression() 
)

# Quadratic regression
quadreg = make_pipeline(StandardScaler(), PolynomialFeatures(degree=2), 
LinearRegression() 
)

# Quadratic regression with LASSO
quadlasso_empty = make_pipeline(StandardScaler(), PolynomialFeatures(degree=2), 
Lasso(max_iter=10000)
)
```


**Training (+HP tuning)**

```{python}
linreg.fit(X_train, y_train) # fitting linear regression

quadreg.fit(X_train, y_train) # fitting the quadratic regression
```

Logarithmic scale:
Standard scale but raise to the power of e. 

```{python}
# Creating the grid, and deciding which hyper parameters to try
np.linspace(-3, 0, 4) # Linearly spaced
np.logspace(-3, 0, 4) # Logarithm scale

quadlasso_empty.get_params() # tells us all the parameters that we have
```

We want to use lasso__alpha as the name. 

```{python}
grid = {'lasso__alpha': np.logspace(-3, 0, 20)}
# Log space tells it how many parameters to try. The first number: lowest value, second number: highest value, third number how many we want to generate

holdout = ShuffleSplit(test_size=.25, n_splits=1, random_state=0) # validation method holdout
```

Grid search object:

```{python}
quadlasso = GridSearchCV(estimator =   quadlasso_empty, 
param_grid = grid, 
cv = holdout, 
scoring = 'neg_mean_squared_error' # negative to understand which is the best value
)

quadlasso.fit(X_train, y_train) #automatically refits
```


## Model selection

**Mean Squared Error Method**

```{python}
mean_squared_error(y_test, linreg.predict(X_test))

mean_squared_error(y_test, quadreg.predict(X_test))

mean_squared_error(y_test, quadlasso.predict(X_test))
```

'We choose the model with the lowest error. In this example, it is the last model.' 

**IMPORTANT**
It is important to note that before using the model in production, I must retrain it on the entire data set. 

We use the model on the entire data set:

```{python}
winner = quadlasso.best_estimator_ # selects the best hyper parameter

winner.fit(X, y)
```

As an example, below a car has been included which was not part of our data set. We can test the model on the prediction accuracy. 

```{python}
seat_marbella = [4, 899 * 0.061, 41, 680 * 2.2, 19.2, 83, 2] # features loaded to compare

y_marbella = (100 * 3.78) / (1.61 * 5.1) # labels loaded
```

Now we ask the model to predict the full efficiency of the car:

```{python}
#prediction = winner.predict([seat_marbella])
#prediction[0]
```

The real value is:

```{r}
#y_marbella
```

We can compare the accuracy of the prediction and the real value. 

**Leave-One-Out Cross validation (LOOCV):**

```{python}
from sklearn.model_selection import LeaveOneOut, cross_val_score

d = pd.read_csv('www/auto-mpg.csv')
label = 'mpg'
features = [col for col in d.columns if col != label]
X = d[features]
y = d[label]
```

```{python}
linreg = make_pipeline(StandardScaler(), LinearRegression() 
)

quadreg = make_pipeline(StandardScaler(), PolynomialFeatures(degree=2), 
LinearRegression() 
)

quadlasso_empty = make_pipeline(StandardScaler(), PolynomialFeatures(degree=2), 
Lasso(max_iter=10000)
)
```

Creating a function: a set of instructions that we combine to provide some inputs to retrieve some outputs. This functions computers the $\hat {ERR}_{\hat f}$ and will take as its input the model $\hat f$:

```{python}
def get_mse(model):
  mses = cross_val_score(estimator=model,
  X=X, 
  y=y, 
  cv=LeaveOneOut(),
  scoring='neg_mean_squared_error'
  )
  return np.average(-mses) # takes the average of the mean square errors
```

Calculating the mean squared error through the leave one out method:

```{python}
get_mse(linreg)
get_mse(quadreg)
```

```{python, include=FALSE}
#grid = {'lasso__alpha': np.logspace(-3, 0, 4)}
quadlasso = GridSearchCV(estimator=quadlasso_empty, 
param_grid=grid, 
cv=LeaveOneOut(), scoring='neg_mean_squared_error',
n_jobs=4 
)

#get_mse(quadlasso)
```


**K-Fold Cross validation**

K = 5 folds. The model used is the same as above. 

```{python, include=FALSE}
from sklearn.model_selection import KFold
#kfold = KFold(n_splits=5, shuffle=True, random_state=1)

def get_mse(model):
  mses = cross_val_score(estimator=model, X=X, y=y, cv=kfold, scoring='neg_mean_squared_error')
  return np.average(-mses)
#get_mse(linreg)
#get_mse(quadreg)
```

```{python, include=FALSE}
#grid = {'lasso_alpha': np.logspace(-3,0,4)}
#quadlasso = GridSearchCV(estimator=quadlasso_empty, param_grid=grid, cv=kfold, #scoring='neg_mean_squared_error')

#get_mse(quadlasso)
```



### SVC

Make list to create the model

svc = make_pipeline(
  transformer,
  GridSearchCV(
    estimator=SVC(kernel="linear"),
    param_grid={
      "C": np.logspace(-3, 3, 50)
    },
    cv=kfold,
    n_jobs=4
  )
)

```{python}

```

The default of scoring = the estimator's score method, the accuracy


Fit model

```{python}
#svc.fit(X_tv, y_tv)
```

Gives us the best estimator:

```{python}
#svc[1].best_estimator_
```

If it is at the edge of the grid, make another grid closer to the extremes. 

```{python}
#pred = svc.predict(X_test)
```

```{python}
from sklearn.metrics import accuracy_score
from sklearn.metrics import recall_score

#accuracy_score(y_test, pred)
#recall_score(y_test, pred)
```

For parameter tuning, we ask scoring to be: 

```{python}

```

svc = make_pipeline(
  transformer,
  GridSearchCV(
    estimator=SVC(kernel="linear"),
    param_grid={
      "C": np.logspace(-3, 3, 50)
    },
    cv=kfold,
    scoring = 'recall',
    n_jobs=4
  )
)

svc.fit(X_tv, y_tv)

svc[1].best_estimator_

pred = svc.predict(X_test)
accuracy_score(y_test, pred)
recall_score(y_test, pred)

RBF Kernel = 

```{python}

```

svc_rbf = make_pipeline(
  transformer,
  GridSearchCV(
    estimator=SVC(kernel="rbf"),
    param_grid={
      "C": np.logspace(-3, 3, 20),
      "gamma": np.logspace(-3,3,20)
    },
    cv=kfold,
    scoring = 'recall',
    n_jobs=4
  )
)

svc_rbf.fit(X_tv, y_tv)

svc_rbf[1].best_estimator_

pred = svc_rbf.predict(X_test)
accuracy_score(y_test, pred)
recall_score(y_test, pred)

Polynomial Kernel = 

```{python}

```

svc_poly = make_pipeline(
  transformer,
  GridSearchCV(
    estimator=SVC(kernel="poly"),
    param_grid={
      "C": np.logspace(-3, 3, 20),
      "gamma": np.logspace(-3,3,20),
      "degree": [2,3]
    },
    cv=kfold,
    scoring = 'recall',
    n_jobs=4
  )
)

svc_poly.fit(X_tv, y_tv)

svc_poly[1].best_estimator_

pred = svc_poly.predict(X_test)
accuracy_score(y_test, pred)
recall_score(y_test, pred)

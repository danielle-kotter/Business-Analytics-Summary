# Simple regressions

```{r, echo=FALSE}
knitr::opts_chunk$set(echo = TRUE, class.source = "watch-out", options(scipen=999), out.width = "100%", comment = "", warning=FALSE, reticulate.repl.quiet = FALSE) 
```

*Class given by: Walter Garcia-Fontes* \ 

*Literature used: [@JCurwin2013]*

```{r include=FALSE}
data <- cars
x <- cars$speed
y <- cars$dist
d <- rep(0,50)
```

## Basics regressions

> Regressions, correlation and dummy's

Y = Dependent\
X = Explanatory

**Correlation**

```{r}
cor(x, y)
```

**Creating the regression:**

1.  To plot the regression model
2.  Evaluates the coefficient of the model
3.  Only the first column estimation

```{r}
model <- lm(y ~ x, data = data)
summary(model)$coef
summary(model)$coef[,1]
```

### Summarizing regressions:

1.  Using stargazer package

```{r message=FALSE, warning=FALSE}
library(stargazer)

stargazer(lm(y~x, data=data), type="text")

# Multiple models adjacent

model1 <- lm(y~x, data=data)
model2 <- lm(x~y, data=data)

stargazer(model1, model2, type="text")

```

2.  Using summary function:

```{r}
summary(lm(y~x))
```

**Confidence interval around slope**

```{r}
confint(lm(y~x), level=0.95)
```

**Sub-sampling regression**

Specify dimensions [,]. First is row. Column, second.

1.  Selects the rows where age is larger than 5.
2.  Lower than 5.

```{r}
sub1 <- summary(lm(y~x, data=data["speed">=5,]))
sub2 <- summary(lm(y~x, data=data["speed"<=5,]))
```

### Visualizing model

**Plotting regression**

```{r}
plot(y~x,data=data, 
     main="Title",
     ylab="yname",
     xlab="xname"
     )
```

**Including regression line:**

```{r}
plot(y~x,data=data, 
     main="Title",
     ylab="yname",
     xlab="xname"
     )
abline(lm(y~x, data=data), col="blue")
```

#### Regression + dummy

Y = Constant0 + B0 \* X - Diff in means + B1 \* variable1\*2

**Omitting the intercept:**

Shows the means separately and not the difference between means. Tests whether the expected counts are different from zero.

```{r}
lm(y ~ x - 1, data = data)
```

## Prediction

```{r}
model <- lm(y~x)

newdata <- data.frame(variablename = c(1:50))
pred <- predict(model, newdata = newdata)
```

**Prediction confidence interval:**

1.  One value
2.  Multiple values from a existing data frame

```{r}
pred1 <- predict(model, data.frame(valuename = x), interval = "confidence", level=0.95)
pred2 <- predict(model, newdata = newdata, interval = "confidence", level=0.95)
```

**Prediction interval**

1.  One value
2.  Multiple values from a existing data frame

```{r}
pred1 <- predict(model, data.frame(valuename = x), interval="predict",level=0.95)
pred2 <- predict(model, newdata, interval="predict",level=0.95)
```

### Confidence and prediction plotting

Adds: observed values, fitted line, confidence interval & predicted interval.

```{r message=FALSE, warning=FALSE}
library(HH)
fit <- lm(y~x, data = data)
ci.plot(fit)
```

### Prediction with dummy variables

Prediction = ð›¼1+ð›¼2Constant Dummy+ð›½1ð‘†ð‘–ð‘§ð‘’+ð›½2Slope Dummy

### Prediction intervals examples

**Prediction**

```{r}
fit <- lm(y ~ x + d + d, data = data)

pred <- predict(fit, data.frame(VariableX = c(10), Dummy = c(1), Slopedummy = c(10)) )
```

**Confidence interval prediction**

```{r}
fit <- lm(y ~ x + d + d, data = data)

pred <- predict(fit, data.frame(VariableX = c(10), Dummy = c(1), Slopedummy = c(10), interval="confidence"))
```

**Prediction interval**

```{r}
fit <- lm(y ~ x + d + d, data = data)

pred <- predict(fit, data.frame(VariableX = c(10), Dummy = c(1), Slopedummy = c(10), interval="predict"))
```

## Data problems

**Residual plots**

```{r}
plot(fit)
```

**Influential measure test**

```{r}
im <- influence.measures(fit)
```

### Multicollinearity

F-test

```{r}
fit <- lm(y~x + d, data = data)

anova(fit)
```

### Variance inflation factors

The variance inflation factor (VIF) is $1 / 1âˆ’R2$.

>
A simple approach to identify collinearity among explanatory variables is the use of variance inflation factors (VIF). It is the ratio of variance in a model with multiple terms, divided by the variance of a model with one term alone. The higher the value, the higher the collinearity. A VIF for a single explanatory variable is obtained using the r-squared value of the regression of that variable against all other explanatory variables: A VIF is calculated for each explanatory variable and those with high values are removed. The definition of 'high' is somewhat arbitrary, but values in the range of 5-10 are commonly used for 'high'. If VIF value exceeding 4.0, or by tolerance less than 0.2 then there is a problem with multicollinearity [\@JHair2013]. However, it depends on the researcher's criteria. The lower the VIF the better, but you shouldn't be too concerned as long as your VIF is not greater than 10.
>

```{r}
vif(fit)
```

### ANOVA

**One-way:** one value

```{r}
res.aov <- aov(y ~ x, data = data)
summary(res.aov)
```

**Two-way:** more than two factors

```{r}
res.aov <- aov(y ~ x + d, data = data)
summary(res.aov)
```

With interaction

```{r}
res.aov <- aov(y ~ x * d, data = data)
summary(res.aov)
```

**Three-way**

1.  Three way
2.  With interaction

```{r}
summary(aov(y ~ x + d, data=data))
summary(aov(y ~ x + d, data=data))
```

### Linearizing variables

```{r}
logged <- log(iris$Sepal.Length) # non-linear
quad <- cars$speed ^ 2 # quadratic
```

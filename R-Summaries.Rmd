---
title: "R-coding Cheatsheets & Summary"
date: "`r format(Sys.Date(), '%d %B, %Y')`"
site: bookdown::bookdown_site
documentclass: book
bibliography:
- book.bib
- packages.bib
biblio-style: apalike
link-citations: yes
description: This is a summary of r code learned throughout several courses of my master in management.
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, class.source = "watch-out", options(scipen=999), out.width = "100%", comment = "", warning=FALSE, reticulate.repl.quiet = FALSE) 
source("www/init.R")
source("www/Table_Design.R")
```

# Preface {.unnumbered}

The following document has been prepared to have a prompt link to the code learned and used throughout several projects in the last year. As a beginner in programming, it is always useful to have input accessible to avoid having to look through numerous repositories.

![](MIT-Coding-Brain-01-press_0.jpeg){width="402"}

------------------------------------------------------------------------

The **bookdown** package that has been used for this format can be installed from CRAN or Github:

```{r eval=FALSE}
install.packages("bookdown")
# or the development version
# devtools::install_github("rstudio/bookdown")
```

To compile this example to PDF, you will need XeLaTeX. You are recommended to install TinyTeX (which includes XeLaTeX): <https://yihui.org/tinytex/>.

```{r include=FALSE}
# automatically create a bib database for R packages
knitr::write_bib(c(
  .packages(), 'bookdown', 'knitr', 'rmarkdown'
), 'packages.bib')
```

------------------------------------------------------------------------

***Prepared by: DaniÃ«lle Kotter***

<!--chapter:end:index.Rmd-->

# Basics R

```{r, include=FALSE}
variable <- 1:10
value <- 5.50
```

```{r include=FALSE}
x <- 5
y <- 10
n <- 30
```

**Mathematical values**

$\mu$ = `$\mu$` = Population mean\
$\sigma$ = `$\sigma$` = Population sd\
$\bar{x}$ = `$\bar{x}` = Sample mean\
${e}$ = `${e}$` = Standard error\
$\pi$ = `$\pi$` = Pie $\ge$ = `$\ge$` = Bigger than\
$\le$ = `$\le$` = Smaller than

**Sample mean, standard deviation**

```{r}
mean <- mean(variable)
sd <- sd(variable)
```

Removes values NA in a data set:

```{r}
mean <- mean(variable, na.rm = TRUE)
sd <- sd(variable, na.rm = TRUE)
```

**Weighted mean & standard deviation**

```{r message=FALSE, warning=FALSE}
library(Hmisc)
weightedmean <- wtd.mean(x,y)
weightedsd <- sqrt(wtd.var(x,y))/sqrt(n)
```

**Variance**

```{r}
var(variable)
```

### Tables, frames & Matrices

As matrix =

```{r message=FALSE, warning=FALSE}
library(data.table)
matrix(c(1:8), nrow = 4, byrow = TRUE) #organized by row
matrix(c(1:8), ncol = 4, byrow = FALSE) #organized by col
```

As data frame =

```{r}
data.frame(Column1 = c(1:5), Column2 = c(1:5))
data.table(Column1 = c(1:5), Column2 = c(1:5))
```

As data table =

```{r}
data.table(matrix(c(1:8), nrow = 4)) # or
data.table(Variablex = 1:5, Variabley = 1:5)
```

Transforming tables from to other formats =

```{r}
table1 <- data.table(matrix(c(1:8), nrow = 4))
as.data.frame(table1)

table2 <- data.frame(Column1 = c(1:5), Column2 = c(1:5))
as.data.table(table2)
```

Binding and setting names =

rbind(table, newvariable) cbind(data, newvariable)

```{r}
rownames(table1) <- c("One", "Two", "Three", "Four")
colnames(table2) <- c("One", "Two")
```

Changing the order of a frequency table and factor

```{r}
value <- c("one", "three", "five", "one", "two", "three", "four", "two", "three")
table <- as.data.table(table(value))

table

table[,`value`:= factor(
 `value`, 
  levels = c(
    "one",
    "two",
    "three",
    "four",
    "five"
  )
)]

setorder(table, `value`)

table
```

**Frequencies**

```{r}
values <- c(1:10)
table(values)
prop.table(table(values))
round(prop.table(table(values)) * 100, 2)
```


## Data sets

Displaying head or tail of a data set:

```{r}
head(cars) # first 6 rows of the data set
tail(cars) # last 6 rows of the data set
```

Reading excel

`library(readxl)`
`data <- read.xls("data.xlsx", stringsAsFactors = TRUE)`


```{r}
names(cars) # shows the column names of the data set
attach(cars) # saves the names to be used as variables
```



### Removing infinite + NA values

1.  Removing Infinite values
2.  Removing NA values
3.  Changing Infinite values to NA values

```{r}
variable[is.finite(variable)]
variable[is.na(variable)]
variable[is.infinite(variable)] <- NA
```

### Transforming variable types

```{r}
as.numeric(value)
as.character(value)
as.factor(value)
```


### Markdown

```{r}
# Putting words in bold: **Word**
```

**Result**

```{r}
# Putting words in italic: *Word*
```

*Result*

```{r}
# dashes like this `here`
```

Show up like this: `here`

```{r}
# dashes like this with the letter r:  `r 4+4`
```

Asks r to have inline code. We can see the results here: `r 4+4`.

```{r}
# \newpage
```

Will start a new page for example in a pdf document

```{r}
# > 
# In here we can put a quote 
# >
```

>
In here we can put a quote
>


### Setup rmarkdown & code chunks

+----------------------+----------------------------------------------------------+
| Call                 | Description                                              |
+======================+==========================================================+
| Warning = TRUE/FALSE | Include / exclude warnings                               |
+----------------------+----------------------------------------------------------+
| Echo = TRUE/FALSE    | Include / exclude r chunks but show output               |
+----------------------+----------------------------------------------------------+
| Include = TRUE/FALSE | Run code but do not include in the knitted document      |
+----------------------+----------------------------------------------------------+
| Comment = "" | Include / exclude \#\# in output code chunks             |
+----------------------+----------------------------------------------------------+
| Message = TRUE/FALSE | Includes / excludes message from code                    |
+----------------------+----------------------------------------------------------+
| out.width='100%'     | Adjusts size of figure / chart                           |
+----------------------+----------------------------------------------------------+
| fig.width =          | Set specific size of figure / chart width                |
+----------------------+----------------------------------------------------------+
| fig.height =         | Set specific size of figure / chart height               |
+----------------------+----------------------------------------------------------+
| fig.cap=             | Adds a title to a figure                                 |
+----------------------+----------------------------------------------------------+
| fig.align=           | 'center', 'left', 'right', adjust figure / chart at page |
+----------------------+----------------------------------------------------------+


### Miscellaneous

```{r}
round(0.50, 2) # rounds a value with two decimals
rep(5,5) #repeats the number 5, 5 times
describe(variable)
fivenum(variable)
summary(variable)
str(variable) # describing the variable
dim(cars) # amount of rows and amount of columns
```


### Subsetting

```{r}
cars[,1] # subsets by columns
cars[1,] # subsets by row
cars[cars$speed == 4,] # subsets by specific value 
cars[cars$speed > 4,] # subsets over specific value 
cars[cars$speed < 5,] # subsets under specific value 
```


<!--chapter:end:01-Basics.Rmd-->

# Charts templates - R

```{r message=FALSE, warning=FALSE}
library(gridExtra)
library(hrbrthemes)
library(ggplot2)

paletteDani <- c( "#ffa500", "#DAF7A6", "#5F7992", "#69b3a2", "#ffd561", "#ee5c42", "#C8A2C8", "#5c3170", "#990000", "#C70039", "#34495E", "#909497")
```

> **Several basic options:**

**Pie chart**

```{r}
data <- ToothGrowth

dani_theme <- 
    theme(
        axis.title.x = element_blank(),
        axis.title.y = element_blank(),
        panel.border = element_blank(),
        panel.grid = element_blank(),
        axis.ticks = element_blank(),
        axis.text.x=element_blank(),
        legend.title = element_text(face = "bold"),
        plot.title = element_text(hjust = 0.5, size = 12, face = "bold")
    )

ggplot(data, aes(x="",y = dose, fill = supp)) +
  geom_bar(width = 1, stat = "identity") +
  coord_polar("y", start = 0) +
  scale_fill_manual("Legendname:",
                    values = paletteDani) +
  dani_theme  +
  labs(title = "Title",
         x = "variablX",
         y = "variableY"
         )
```

**Bar chart**

```{r}
ggplot(data, aes(x = dose, y = supp)) + 
  geom_bar(stat = "identity", fill="#69b3a2", color="#e9ecef") +
  theme(legend.position="none")
```

**Histogram**

```{r}
ggplot(data = data, aes(len) ) + 
    geom_histogram(fill="#69b3a2", color="#e9ecef", alpha=0.9) +
    ggtitle("Title") +
      xlab("variablex") +
      ylab("variabley") +
    theme(plot.title = element_text(size = 11))
```

**Boxplot**

```{r}
ggplot(data=ToothGrowth, aes(x="", y=len, fill="")) +
    geom_boxplot(fill="#69b3a2", outlier.colour="red", outlier.shape=8, outlier.size=4) +
    theme_ipsum() +
    theme(
      legend.position="none",
      plot.title = element_text(size=12)
      ) +
    ggtitle("Title") +
    theme(plot.title = element_text(hjust = 0.5)) +
    	xlab("")+
    	ylab("")
```

**Scatter plot**

```{r}
 ggplot(data= ToothGrowth, aes(y = len, x = dose)) +
  geom_point(size=2) +
  geom_smooth(method="lm", color="#69b3a2", fullrange=TRUE, 
        formula = y ~ x) +
   theme(plot.title = element_text(hjust = 0.5)) +
    	labs(title = "Title",
         y = "yname",
         x = "xname"
         )
```

**Scatter plot with dummies**

```{r}
ggplot(data = ToothGrowth, aes(y = len, x = supp, colour=factor(supp))) + 
  geom_point(size=2) +
  geom_smooth(method="lm", fill = NA, fullrange=TRUE,  formula = y ~ x) +
    theme(plot.title = element_text(hjust = 0.5)) +
    scale_colour_manual(name="Legendtitle", 
    	labels=c("value1", "value2"),values = c("#69b3a2", "#F6726A"))+
    labs(title = "Title",
         y = "Yname",
         x = "Xname"
         )
```

**Arrange charts next to each other on a page**

```{r}
chart1 <- ggplot(data=ToothGrowth, aes(x="", y=len, fill="")) +
    geom_boxplot(fill="#69b3a2", outlier.colour="red", outlier.shape=8, outlier.size=4) +
    theme_ipsum() +
    theme(
      legend.position="none",
      plot.title = element_text(size=12)
      ) +
    ggtitle("Title") +
    theme(plot.title = element_text(hjust = 0.5)) +
    	xlab("")+
    	ylab("")


chart2 <- ggplot(data=ToothGrowth, aes(x="", y=len, fill="")) +
    geom_boxplot(fill="#69b3a2", outlier.colour="red", outlier.shape=8, outlier.size=4) +
    theme_ipsum() +
    theme(
      legend.position="none",
      plot.title = element_text(size=12)
      ) +
    ggtitle("Title") +
    theme(plot.title = element_text(hjust = 0.5)) +
    	xlab("")+
    	ylab("")


grid.arrange(chart1, chart2, nrow=1, widths=c(0.9,1))
```

**Shows the amounts of missing values (NA) in a data set**

```{r warning=FALSE, message=FALSE}
library(VIM)
aggr(cars, numbers = TRUE, prop = c(TRUE, FALSE), cex.axis = 0.5)
```

**Density plots with semi-transparent fill**

```{r}
ggplot(data = ToothGrowth, aes(x = len, fill = supp)) + 
  geom_density(alpha=.3) +
    theme(
      plot.title = element_text(size=15)
      ) +
    ggtitle("Density plot") +
    theme(plot.title = element_text(hjust = 0.5, face= "bold")) +
    xlab("")+
    ylab("Density")
```

**Correlation matrix - pairs panel**

```{r, warning = FALSE, message = FALSE}
library(psych)

pairs.panels(iris,
method = "pearson",
hist.col = "#00AFBB",
density = TRUE,
ellipses = TRUE
)
```

**Simple plots:**

```{r, warning = FALSE, message = FALSE}
model <- lm(dist ~ speed, data = cars)
plot(model)
plot(model, 4) # cook distance
library(car)
avPlots(model)
```

<!--chapter:end:02-Charts.Rmd-->

# Probability

```{r, message=FALSE, warning=FALSE}
library(prob)
library(LaplacesDemon) # Bayes Theorem
library(BSDA) #tsumtest
library(actuar) 
```

```{r, include=FALSE}
X = 15
x = 15
p = 0.5
n = 30
a = 10
b = 15
mu = 20
se = 15
Y = 10
y = 5
lambda = 5
sigma = 4
pi = 3
ts = 3
df = 4
sd = 2
```

```{r}
out <- c("Red", "White", "Black", "Blue", "Green")
freq <- c(1,2,3,4,5)
s <- probspace(out, probs = freq)
print(s)
```

1. If you toss two fair coins, what is the probability of two heads?

```{r}
space <- tosscoin(2, makespace = TRUE)
p <- Prob(space, toss1 == "H" & toss2 == "H")
```

*The probability is: `r p`*

2. When two dice are thrown, what is the probability of a 3 followed by a 5?

```{r}
space <- rolldie(2, makespace = TRUE)
p <- Prob(space, X1 == 3 & (X2 == 5) )
```

*The probability is: `r round(p, 2)`*

3. Sampling from an urn with or without replacement. 3 balls and sample size of 2:

```{r}
sample1 <- urnsamples(1:3, size = 2, replace = TRUE, ordered = TRUE)
sample2 <- urnsamples(1:3, size = 2, replace = FALSE, ordered = TRUE)
sample3 <- urnsamples(1:3, size = 2, replace = FALSE, ordered = FALSE)
sample4 <- urnsamples(1:3, size = 2, replace = TRUE, ordered = FALSE)
```

------------------------------------------------------------------------

## Bayes Theorem

**Unconditional probability:**

P(S) and P(NS)
Success or no success

```{r}
prS <- c(0.4, 0.6)
```

**Conditional probability:**

P(P | S ) and P( P | NS)
Predicted given it is successful  
Predicted given it is not successful

```{r}
prNS <- c(0.6, 0.2)
```

**Bayes prob, posterior probabilities**  
P(S | P) & P(NS | P)

```{r}
BayesTheorem(prS, prNS)
```

------------------------------------------------------------------------

## Discrete Probablity

### Uniform discrete probability distribution

1. Sample space with a set probability. Size = amount of tries
2. Density function: Individual probability. F.E. Getting a 4
3. Cumulative density: Uniform for a certain value distribution. F.E. 4 or less. 4 or more? 1-punif 3
4. Inverse cumulative density: Uniform for a certain probability ( up until a certain value). F.E. up to 25% of the tries

```{r, warning = FALSE, include = FALSE}
one <- sample(p, size = n, replace = TRUE)
two <- dunif(X, min = a, max = b)
three <- punif(X, min = 0, max = 6)
four <- qunif(X, min = 0, max = 6)
```

***Default = # or less. For # or more do: 1-probability of # or less*** 

### Binomial distribution

1. Binomial for a specific value for a certain sample. F.E. 2 from the sample are successful.
2. Binomial for a certain distribution of the sample. F.E. At most 2 in the sample are successful. Or 5 or more. 
3. Binomial for a certain percentage of the  sample. F.E. 25% of the sample has x value or less. 
4. Difference between two binomial values. F.E. Prob there are between 4 and 5 of the trials successful.

```{r, warning = FALSE}
one <- dbinom(x, size = n, prob = y)	
two <- pbinom(x, size = n, prob = y)	
three <- qbinom(p, size = n, prob = y)	
four <- diff(pbinom(c(X,Y), size = n, prob = y))
```

***Default = # or less (left area of the distribution). For # or more do: 1-probability of # or less***


### Poisson distribution

**Expected value =** $n * p = LAMDA$

1. Poisson for a certain value. Lambda = n*p. F.E. Prob of having a 5
2. Poisson for a certain value distribution. F.E. Prob of having less than 5. More than 5? = 1-	Ppois(4, lambda)
3. Poisson for a certain probability to capture a certain value. F.E. Poisson value for 25%.

```{r, warning=FALSE}
one <- dpois(x,lambda)
two <- ppois(x,lambda)	
three <- qpois(x,lambda)
```

***Default = # or less (left area of the distribution). For # or more do: 1-probability of # or less*** 


### The normal distribution

**Empirical rule**

For all normal distributions: 68-95-99.7 rule

99.7% of observations are located between: -3 mu and 3  
95% of observations are located between: -2 mu 2  
68% of observations are located between: -1 mu 1

**Normal distribution**

**Z-value**

```{r}
# z <- (x - mean) / sd. For example:
(11 - 10) / 2
```

1. Normal distribution for a certain proportion. Pi = population proportion mean%.
2. Normal distribution for a certain value distribution. F.E. Prob of value above 5. FALSE
	Prob less than 9. TRUE
3. Normal distribution for a certain probability to capture a certain value. F.E. Value that is given at 25% point. 
4. Difference between two values on the normal distribution. F.E. between 5 and 10. 

```{r}
one <- pnorm(X, pi, sd, lower.tail = FALSE)
two <- pnorm(X, mu, sd, lower.tail = FALSE)
three <- qnorm(p, mu, sd, lower.tail = FALSE) 
four <- diff(pnorm(c(X,Y), mu, sd, lower.tail = FALSE))
```

lower.tail = TRUE: The area of the left side of the slope  
lower.tail = FALSE: The area of the right side of the slope

**Confidence interval for normal distribution**

z.test(x, sd=sigma)
binconf(x = x, n = n) <- proportions
t.test(variable) <- t-distribution for conf.inv

#### Plotting the normal distribution

"With mean = 3 and standard deviation = 7  
Limits: mean +/- 3 * standard deviation  = 3*7 = 21 
Lower limit = 3 â€“ 21 = -18  
Upper limit = 3 + 21 = 24"  

Example:

```{r}
x <- seq(15, 45, length=50)
y <- dnorm(x, 30, 5)
plot(x,y,type="l",lwd=2,col="black")

x <- seq(15,35, length=100)
y <- dnorm(x, 30,5 )
polygon(c(15,x,35),c(0,y,0), density = c(15, 35), col = "black")

p <- pnorm(35, mean = 30, sd = 5,lower.tail = TRUE)
text(0,0.15,"68%")
```

#### Binomial

It will be possible to use the Normal distribution as an approximation to the Binomial if: n is large and p > 0.1 


1. Density function (individual probability). 
2. Cumulative density (between certain values). 
3. Difference between two binomial values
4. Inverse cumulative density. For a certain prob. 

```{r, warning = FALSE}
one <- dbinom(x, mu, sd)
two <- pbinom(x, mu, sd, lower.tail = FALSE)
three <- diff(pbinom(c(X,Y), mu, sd, lower.tail = FALSE))
four <- qbinom(p, mu, sd, lower.tail = FALSE)
```

------------------------------------------------------------------------

## Samples, estimation & confidence intervals

**The standard error of the sampling distribution of the mean**

```{r}
se <- sigma / sqrt(n)
```

**Probability sample**

1. To find the probability that X is larger than mu
2. To find the probability that X is smaller than mu

```{r}
p <- pnorm(X, mu, se, lower.tail = TRUE) 
p <- pnorm(X, mu, se, lower.tail = FALSE)
```

**Probability proportions sample**

```{r}
sd <- sqrt((pi*(n-pi))/n)
z <- (p - pi)/sd

p <- pnorm(X, pi, se, lower.tail =FALSE)
```


**Sample size**

Package = "samplingbook".

Provides the sample size needed to have a 95% confidence to estimate the population mean. 
Level = confidence level. Se is required standard error. 

**sample.size.mean(se, sigma, level=0.95)**

------------------------------------------------------------------------

## Significance level

### Critical values

**Critical value for normal distribution, sample > 30**  

1. Two-sided: Critical value, 5% significance level = 1.96
2. Two-sided: Critical value, 1% significance level = 2.58
3. Two-sided: Critical value, 10% significance level = 1.96
4. One-sided: Critical value, 5% significance level = 1.64
5. One-sided: Critical value, 1% significance level = 2.33
6. One-sided: Critical value, 10% significance level = 1.28

```{r}
qnorm(0.975)
qnorm(0.995)
qnorm(0.95)

qnorm(0.95)
qnorm(0.99)
qnorm(0.90)
```

**Critical values t-distribution**

1. One-sided: critical value at a 5% significance level
2. One-sided: critical value at a 10% significance level
3. One-sided: critical value at a 1% significance level
4. Two-sided: critical value at a 5% significance level
5. Two-sided: critical value at a 10% significance level
6. Two-sided: critical value at a 1% significance level

```{r}
cv <- qt(0.95, df) 
cv <- qt(0.90, df) 
cv <- qt(0.99, df)

cv <- qt(0.975, df)
cv <- qt(0.95, df) 
cv <- qt(0.995, df)
```

**Confidence interval**

```{r}
cv <- cv
mu <- mu
sd <- sd
se <- sd / (sqrt(n))
n <- n

conf_int95 <- cv * sd / (sqrt(n))
mu_plus <- mu + conf_int95
mu_min <- mu - conf_int95
```

**Large sample significance testing**

1. Two-sided
2. One-sided: X is greater than the population mean
3. One-sided: X is less than the population mean

```{r message=FALSE, warning=FALSE}
library(BSDA)

one <- tsum.test(mean.x = X, s.x = sd, n.x = n, mu = mu, alternative = "two.sided", var.equal = TRUE) 

two <- tsum.test(mean.x = X, s.x = sd, n.x = n, mu = mu, alternative = "greater", var.equal = TRUE) 

three <- tsum.test(mean.x = X, s.x = sd, n.x = n, mu = mu, alternative = "less", var.equal = TRUE) 
```

**For proportions:**

prop.test(x = X, n = n, p = p, correct = TRUE, alternative = "two.sided")

Same goes for above: two.sided, greater, less

### Test of equality - two samples

H_0 <- $\mu1 = \mu2$ or $(\mu1 - \mu2) = 0$  
H_a <- $\mu1 \neq \mu2$ or $\mu1 - \mu2 \neq 0$


Difference in two means with a certain confidence level confidence interval. Default = 95%

```{r}
tsum.test(mean.x = X, s.x = sd, n.x = n, mean.y = X, s.y = sd, n.y = n, var.equal=FALSE)
```

2-sample test for equality of proportions without continuity correction.

prop.test(data, correct=FALSE, alternative="greater")

------------------------------------------------------------------------

## Non-Parametric testing

### Contengency table / frequencies

Obtain contingency table

```{r}
table(ToothGrowth$dose)
```

### Chi-square

1. Chi-square test
2. Get the expected value
3. Probability for chi-square 

```{r}
data <- matrix(c(27,373,33,567),byrow=TRUE,nrow=2)
chisq.test(data,correct=FALSE)

chisq.test(data,correct=FALSE)$expected

prop.table(chisq.test(data,correct=FALSE)$expected,1)
prop.table(chisq.test(data,correct=FALSE)$expected,2)
```

Degree of freedom = # of row - 1 * # of columns = fixed 

***All expected frequencies must be above five! If not, categories must be combined!***


### Goodness of fit

**Uniform:**

Degree of freedom =  number of categories - number of parameters - 1.

```{r}
x <- c(1,2,3,4,5)
p <- rep(1/5, 5)
chisq.test(x, p = p)
```

***All expected frequencies must be above five! If not, categories must be combined!***


**Binomial:**

dbinom(x, size = n, prob = y)	

For example:

```{r message=FALSE, warning=FALSE}
library(actuar)
cj <- c(-0.5, 0.5, 1.5, 2.5, 3.5, 4.5, 5.5)

#or

cj <- seq(from = -0.5, to=5, by=1)

nj <- c(15,20,20,18,13)
data <- grouped.data(Group = cj, Frequency = nj)
p <- mean(data)/5
pr <-c(dbinom(0,5,p),dbinom(1,5,p),dbinom(2,5,p),dbinom(3,5,p),dbinom(4,5,p),dbinom(5,5,p))

nj2 <- c(35,20,18,23)
pr2 <- c(dbinom(0,5,p)+dbinom(1,5,p),dbinom(2,5,p),dbinom(3,5,p),dbinom(4,5,p)+dbinom(5,5,p))

chisq.test(nj2,p=pr2)
```

***All expected frequencies must be above five! If not, categories must be combined!***

**Poisson**

Degree of freedom = number of categories - number of parameters - 1.

NOTE! Distribution goes to infinity. Counter for one value that is X or more. 1 - until X. 

Example:

```{r}
cj <- c(-0.5, 0.5, 1.5, 2.5, 3.5, 4.5, 5.5)

#or

cj <- seq(from = -0.5, to=6, by=1)
nj <- c(16, 30, 37, 7, 10, 5)
data <- grouped.data(Group = cj, Frequency = nj)
m <- mean(data)

pr <- c(dpois(0, m),dpois(1,m),dpois(2, m), dpois(3, m), dpois(4, m), + (1-ppois(4,m)) )

chisq.test(nj, p = pr)
```

**Normal distribution**

Example:

```{r}
cv <- qchisq(0.90, 2)

cj <- c(0, 1, 3, 10, 15, 30)
nj <- c(16, 30, 37, 7, 10)
data <- grouped.data(Group = cj, Frequency = nj) 
m <- mean(data)
s <- sqrt(emm(data,2))

pr <- c(pnorm(1,m,s), diff(pnorm(c(1,3),m,s)), diff(pnorm(c(3,10),m,s)), diff(pnorm(c(10,15),m,s)), 1 - pnorm(c(15),m,s) )
         
chisq.test(nj,p=pr)
```

###Mann-whitney test

N = Number of pairs - number of draws

**For small tests**

c1 values sample 1  
c2 values sample 2  

*wilcox.text(x, c2)*

**Larger sample test > 10**

You can use a approximation based on the normal distribution. Therefore critical values will be 1.96 for this two sided test.


###Wilcoxon test

Two options  
	- Do not predict direction --> two sided  
	- Predict direction --> one sided  
	
*wilcox.test(w1, w2, paired=TRUE,correct=FALSE)*


###Run test

```{r message=FALSE, warning=FALSE}
library(randtests)
pers <- c(0,1,1,0,0,0,0,1,1,0,1)
pers.f <- factor(pers,labels=c("Male","Female"))
runs.test(pers)
```

### P-value

Find p value: Probability of getting this test statistic or more:

```{r}
pchisq(ts, df, lower.tail=FALSE)
```


<!--chapter:end:03-Probability.Rmd-->

# Simple regressions

```{r include=FALSE}
data <- cars
x <- cars$speed
y <- cars$dist
d <- rep(0,50)
```

## Basics regressions

Regressions, correlation and dummy's

Y = Dependent  
X = Explanatory

**Correlation**

```{r}
cor(x, y)
```

**Creating the regression:**

1. To plot the regression model
2. Evaluates the coefficient of the model
3. Only the first colum estimattion 

```{r}
model <- lm(y~x, data = data)
summary(model)$coef
est <- summary(model)$coef[,1]
```


### Summarizing regressions:

1. Using stargazer package

```{r message=FALSE, warning=FALSE}
library(stargazer)

stargazer(lm(y~x, data=data), type="text")

# Multiple models adjacent

model1 <- lm(y~x, data=data)
model2 <- lm(x~y, data=data)

stargazer(model1, model2, type="text")

```

2. Using summary function:

```{r}
summary(lm(y~x))
```

Regressions

**Plotting regression**

```{r}
plot(y~x,data=data, 
     main="Title",
     ylab="yname",
     xlab="xname"
     )
```

**Including regression line:**

```{r}
plot(y~x,data=data, 
     main="Title",
     ylab="yname",
     xlab="xname"
     )
abline(lm(y~x, data=data), col="blue")
```

**Confidence interval around slope**

```{r}
confint(lm(y~x), level=0.95)
```

**Sub-sampling regression**

Specify dimensions [,]. 
First is row. Column, second. 

1. Selects the rows where age is larger than 5.
2. Lower than 5.

```{r}
sub1 <- summary(lm(y~x, data=data["speed">=5,]))
sub2 <- summary(lm(y~x, data=data["speed"<=5,]))
```


### Dummy variables, diff in means


### Regression + dummy

Y = Constant0 + B0 * X - Diff in means + B1 * variable1*2


**Omitting the intercept:**

Shows the means separately and not the difference between means. Tests whether the expected counts are different from zero. 

```{r}
lm(y ~ x - 1, data = data)
```

Reorders group, to specific value to be first.
 
 variable2 <- relevel(variable, "C")

## Prediction  

``` {r}
model <- lm(y~x)

newdata <- data.frame(variablename = c(1:50))
pred <- predict(model, newdata = newdata)
```

**Prediction confidence interval:**

1. One value
2. Multiple values from a existing data frame

```{r}
pred1 <- predict(model, data.frame(valuename = x), interval = "confidence", level=0.95)
pred2 <- predict(model, newdata = newdata, interval = "confidence", level=0.95)
```

**Prediction interval**

1. One value
2. Multiple values from a existing data frame

```{r}
pred1 <- predict(model, data.frame(valuename = x), interval="predict",level=0.95)
pred2 <- predict(model, newdata, interval="predict",level=0.95)
```


### Confidence and prediction plotting

Adds: observed values, fitted line, conf interval, predicted interval

```{r message=FALSE, warning=FALSE}
library(HH)
fit <- lm(y~x, data = data)
ci.plot(fit)
```

### Prediction with dummy variables

Prediction = ð›¼1+ð›¼2Constant Dummy+ð›½1ð‘†ð‘–ð‘§ð‘’+ð›½2Slope Dummy

### Prediction intervals examples

**Prediction**

```{r}
fit <- lm(y ~ x + d + d, data = data)

pred <- predict(fit, data.frame(VariableX = c(10), Dummy = c(1), Slopedummy = c(10)) )
```
 
**Confidence interval prediction**  

```{r}
fit <- lm(y ~ x + d + d, data = data)

pred <- predict(fit, data.frame(VariableX = c(10), Dummy = c(1), Slopedummy = c(10), interval="confidence"))
```  

**Prediction interval**

```{r}
fit <- lm(y ~ x + d + d, data = data)

pred <- predict(fit, data.frame(VariableX = c(10), Dummy = c(1), Slopedummy = c(10), interval="predict"))
```


## Data problems

**Residual plot**

```{r}
# residual.plots(fitted(fit), resid(fit), sigma.hat(fit), main="Title")
```

**Influential measure test**

```{r}
im <- influence.measures(fit)
```


### Multicollinearity

F-test

```{r}
fit <- lm(y~x + d, data = data)

anova(fit)
```

### Variance inflation factors

The variance inflation factor (vif) is $1 / 1âˆ’R2$.
A simple approach to identify collinearity among explanatory variables is the use of variance inflation factors (VIF). It is the ratio of variance in a model with multiple terms, divided by the variance of a model with one term alone. The higher the value, the higher the collinearity. A VIF for a single explanatory variable is obtained using the r-squared value of the regression of that variable against all other explanatory variables: A VIF is calculated for each explanatory variable and those with high values are removed. The definition of â€˜highâ€™ is somewhat arbitrary, but values in the range of 5-10 are commonly used for â€˜highâ€™. If VIF value exceeding 4.0, or by tolerance less than 0.2 then there is a problem with multicollinearity (Hair et al., 2010). However, it depends on the researcherâ€™s criteria. The lower the vif the better, but you shouldnâ€™t be too concerned as long as your VIF is not greater than 10.

```{r}
vif(fit)
```


### ANOVA

**One-way:** one value

```{r}
res.aov <- aov(y ~ x, data = data)
summary(res.aov)
```

**Two-way:** more than two factors 

```{r}
res.aov <- aov(y ~ x + d, data = data)
summary(res.aov)
```

With interaction

```{r}
res.aov <- aov(y ~ x * d, data = data)
summary(res.aov)
```

**Three-way**

1. Three way
2. With interaction

```{r}
summary(aov(y ~ x + d, data=data))
summary(aov(y ~ x + d, data=data))
```

**MANOVA**: Multiple factors

1. Test in difference
2. Test separately


test_manova <- manova(cbind(y, d) ~ x, data = data)
summary(test_manova)
summary.aov(test_manova)


### Linearizing variables

```{r}
logged <- log(iris$Sepal.Length) # non-linear
quad <- cars$speed ^ 2 # quadratic
```



<!--chapter:end:04-Simpleregressions.Rmd-->

# Structure equation models

**Structural Equation Modeling (SEM):** is an extremely broad and flexible framework for data analysis, perhaps better thought of as a family of related methods rather than as a single technique. Measuring latent constructs is challenging and we must also incorporate estimates of measurement error into our models. SEM excels at both of these tasks. SEM is especially suited for causal analysis.[@kdnuggets2017]

VS

**Ordinary least-squares (OLS):** models assume that the analyst is fitting a model of a relationship between one or more explanatory variables and a continuous or at least interval outcome variable that minimizes the sum of square errors, where an error is the difference between the actual and the predicted value of the outcome variable. The most common analytic method that utilizes OLS models is linear regression (with a single or multiple predictor variables).

**Latent variables =** Unobserved variables (or unmeasured variables in SEM lingo). These are theoretical concepts which can be inferred but not directly measured.

Linear regression model = $Y = alpha + betaX + error$

The model has to account for randomization. If there is a factor that that cannot be explained, it is included in the error. When this unmeasured factor that is in the error is correlated to another independent variable, there is ***endogeneity***.

**Endogeneity variables =** correlated with the error terms. Arises when the marginal distribution of the independent variable is not independent of the conditional distribution of the dependent variable given the independent.

**Exogenous variables =** not driven by other factors (observable or observable)

*Sources of endogeneity =*

    1. Omitted variables: relevant variables left out of the model,  attributing to effect to those that were included. 
    2. Simultaneity: where x causes y and y causes x
    3. Selection bias: sampling bias

In the structural equation model we can effectively avoid endogeneity.

------------------------------------------------------------------------

## Path analysis (structural equations)

**Path diagrams =** Communicates the structure of our model. Useful for structural equation models. The objects in the model mean:

-   Rectangular = any variable that we can observe directly (observed variable), measured variables

-   Circle / Ovals = Cannot be observed (Latent variable)

-   Arrow = directed effect. One variable impacts the other. Hypothetical causal relationship.

-   Numbers by the arrows = regression coefficient.

![](images/paste-C79DE382.png){width="352"}

-   Triangle is the constant in the Linear Model.

![](images/paste-948ED17C.png){width="81"}

-   Double handed arrows = indicate co-variances or correlations without a causal interpretation.

-   Double handed arrow between two independent variables = they are correlated to each other.

![](images/paste-D2D686C6.png){width="350"}

-   Residual error term = measurement errors. We expect that the factor will not perfectly predict the observed variables.

-   The bi-directed arrows, the ones with two side arrows (in this representation, a connecting line with no arrows) represent co-variances among variables.

------------------------------------------------------------------------

## Coding the model

```{r, warning=FALSE, message=FALSE}
library(semPlot)
library(lavaan)
```

**Setting up the model and summarizing**

```{r comment= ""}
fit <- 'dist ~ speed'
model <- lavaan(fit, data = cars, estimator="MLM", auto.var = TRUE)
summary(model)
```

*Estimator = MLM in the model:* protects for non-linearity, non-normality and elasticity of the raw data. This needs the raw data, not only the covariance matrix.

**Setting up the path diagram**

```{r}
semPaths(model, "std", title = FALSE,
weighted = FALSE, sizeInt = 4, sizeMan = 5,
edge.label.cex = 1.3, asize = 2)
title("SEM path", line = 3)
```

*"std" =* standardizing the variables

*"est" =* true estimation

*style="ram" =* to get circles around the measurement arrows

**With latent variable & SEM**

```{r, warning = FALSE}
model <- "
# regression
Petal =~ Petal.Width + Petal.Length
Sepal.Length ~ Sepal.Width  + Petal
"

fit <- sem(model, data = iris, sample.cov = S, sample.nobs = 122)
semPaths(fit, "std", sizeInt = 4, sizeMan = 3, edge.label.cex = 1, asize=3, weighted=TRUE, exoCov = TRUE)
```

\
**Testing the model**

```{r}
fitMeasures(fit)[ c("chisq","df", "pvalue" ,"rmsea")]
```

1.  Chisq is a chi-squared test statistic for. H0: moment restrictions implied by the model hold true
2.  The degrees of freedom
3.  P-value of the chi-square test
4.  Rmsea is a fit index, the root mean square of approximation. It should be small for a good fit of the model. Threshold of .05 is often applied to declare good fit.

*See later chapter for further explanation*

**Modification indices**

This tells us how we could improve your model. If i liberate one parameter, then \_ would change.

```{r}
modi = modindices(fit)
modi[order(modi[,4], decreasing=T), ]
```

**Parameters fit**

```{r}
parameterestimates(fit, 
standardized = TRUE, rsquare=TRUE, ci=FALSE)[1:4,] # showing the top 4
```

\+

------------------------------------------------------------------------

### Covariance

Covariance is a measure of how much two random variables vary together. It's similar to variance, but where variance tells you how a single variable varies, co variance tells you how two variables vary together.

Cov(1, 2) =

$sum (1- mean(1) * (2-mean(2)  / n$

**Moment matrix:**

![](images/paste-0F03CE3C.png){width="261"}

**Covariance matrix:**

```{r}
cov(cars)
```

![](images/paste-BCC7A4B7.png){width="353"}

Creating the covariance matrix when only having the lower half:

```{r}
lower <- "
0.03300863
0.15894229 5.0185561
0.15670560 0.9841531 1.2142232
"
S <- getCov(lower,
names = c("variable1", "variable2", "variable3"))
print(S)
```

------------------------------------------------------------------------

### Reliability

In statistics ***reliability*** is the consistency of a set of measurements or measuring instrument, often used to describe a test. This can either be whether the measurements of the same instrument give or are likely to give the same measurement (test-retest), or in the case of more subjective instruments, such as personality or trait inventories, whether two independent assessors give similar scores (inter-rater-reliability). Reliability is inversely related to random error. In words, reliability is defined as a proportion of observed variance that is true variance. Reliability is interpreted as a proportion and therefore cannot be negative.

Various kinds of reliability coefficients, with values ranging between 0.00 (much error) and 1.00 (no error), are usually used to indicate the amount of error in the scores.

The reliability is expressed as [k]{.ul} and there are several options to calculate:

**Option 1: reliability of a latent variable**

1 - Measurement error variable 1 / observed variable 2 (latent variable). Dividing the true variance by the observed variance.

Here is an example:

![](images/paste-3355EB34.png)

**Option 2: based on the standardized path diagram**

The square of the standardized loading is the reliability of the variable. Example:

```{r echo=FALSE}
fit <- 'dist ~ speed'
model <- lavaan(fit, data = cars, estimator="MLM", auto.var = TRUE)
semPaths(model, "std", title = FALSE,
weighted = FALSE, sizeInt = 4, sizeMan = 5,
edge.label.cex = 1.3, asize = 2)
title("SEM path", line = 3)
```

Here the reliability of the variable distance (dst) = $0.35 ^ 2 = 0.12 = k$

\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_

**Testing the model**

`fitMeasures(fit) [ c("chisq", "df", pvalue", "rmsea")]`

**Model chi-square test =** We test whether the fitted model is correct. HO: moment restrictions implied by the model hold. The fit is correct If \> 0.05 we cannot reject the model. I accept the model.

Therefore, the chi-square test allows researchers to evaluate the fitness of a model by using the null hypothesis significance test approach.


**Degrees of freedom (df) =** Number of observations available for model estimation - Number of observations used to estimate parameters.

"Number of free parameters" refers to all of the things that this model estimated freely. Parameter is a regression coefficient when standardized is called a beta coefficient.

**The Root Mean Square Error of Approximation (RMSEA) =** fit index: how the covariance fit in the model. Difference between observed and the fitted.

The RMSEA is widely used in Structural Equation Modeling to provide a mechanism for adjusting for sample size where chi-square statistics are used. Measures the discrepancy due to the approximation per degree of freedom.

The objective is to have the RMSEA as low as possible.

------------------------------------------------------------------------

## Factor model

**Factor analysis:** a statistical method used to describe variability among observed, correlated variable in terms of a potentially lower number of unobserved variables called factors. For example, it is possible that variations in six observed variables mainly reflect the variations in two unobserved (underlying) variables. Factor analysis searches for such joint variations in response to unobserved latent variables. The observed variables are modeled as linear combinations of the potential factors, plus "error" terms.

It is including a common return that has a impact to multiple variables. Then, you can find how much variance is due to permanent and depended on these variables.

Below we can find the factor model including means based on the Spearman model from 1903.

![](images/paste-746E3DB0.png)

**Difference in regression equations:**

***Basic model - Y, X are centered :***

$Y = beta X + error$

***Y, X NOT centered - including constant:***

$Y = alpha*1 + beta X + error$

***Factor model:***

$Y = lambda*F + error1$

**Lamba =** the weights \
**F =** common factor \
**Error =** Specific to the factors

Here instead of setting the alpha to 1 which we do in the basic model, now we add a weight which is the lambda.

When the data is standardized, beta becomes the standardized beta coefficient. Because y, x are centered, you do not need to present the constant in the model.

Next step in factor models with simultaneous equations --\> ML estimation of a general model. Linear structural relations

You have to fix the variance of a variable that you do not observe. If you do not do this, the model is not identified. Meaning there is no minimum. Including latent variables two options:

    A) Var(F) = 1, F Standardized (F typically has mean zero)
    B) Lambda_1 = 1

They are equivalent regarding degrees of freedom or model fit. When you put 1\* variable1, you force the true beta to be 1.

**In r setting the model =**

```{r}
#regression equation option 1
Model <- "dist =~ 1 * speed
"

#regression equation option 2
Model <- "dist =~ NA * speed
"
```

When using: Default NA = not available. We are asking R to calculate the beta instead of setting it to 1. This gives the equation: $Y = Lambda*F + Error$. \

**Degrees of freedom**

When you bring variables into the model, the degrees of freedom increases. When there are 0 degrees of freedom, you cannot test but you can fit the model. Now you cannot reject the model, but you can check the reliability.

### Setting covariance & variances

Independent variables have variances and co-variances, unless the model specification puts them to zero. Variance is to be estimated in the model and if necessary we should be imposing restrictions. When you include all variances, you put a lot of tension on your model.

Y \~\~ 1\* Y = forces the variable to be standardized

X \~\~ X = Gives two explanatory variables the possibility to covariance

Equality with error variances (multiple independent variables) X\~\~X \*A = setting a restriction.

**Auto regressive =** one variables keeps impacting the following. For example: 2011 impacts 2012 which impacts 2013 etc. In this case, the variables need to be set to allow for correlation.


<!--chapter:end:05-SEM.Rmd-->

---
editor_options: 
  markdown: 
    wrap: 72
---

```{r, echo=FALSE, warning=FALSE, message=FALSE}
library(knitr)  
library(reticulate)
os <- import("os")
os$listdir(".")
options(reticulate.repl.quiet = FALSE)
use_python("/usr/local/bin/python")
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, cache.lazy = FALSE)
use_virtualenv("r-reticulate")
```

```{python}
# This Python file uses the following encoding: utf-8
import os, sys
```

```{python, warning=FALSE, message=FALSE, comment=FALSE}
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from matplotlib.pyplot import figure
import seaborn as sns
import os
```

# Basics Python

```{r}
y = 5 + 5
y
```

Printing characters

```{r}
print('Hello, readers!')
```

Printing numbers

```{r}
print(15)
```

Printing length of a value

```{python, warning=FALSE, message=FALSE}
Length = len('Danielle')
print(Length)
```

------------------------------------------------------------------------

## Data set

Loading the data set & viewing head + tails:

```{python}
sns.set_context('paper')

tips = sns.load_dataset('tips')

tips.head()
tips.tail()
```

1.  Length of the variable
2.  Shape: Number of rows and number of columns
3.  Type of variables + basic info
4.  Descriptive statistics variable

```{python}
len(tips)

tips.shape

tips.info()

tips.describe()
```

------------------------------------------------------------------------

## Matrixes

Series:

Panda series method:

```{python}
Serie1 = pd.Series(
  [4200, 8000, 6500],
  index=["Amsterdam", "Toronto", "Tokyo"]
  )
Serie1
```

Python dictionary method:

```{python}
Serie2 = pd.Series({"Amsterdam": 5, "Tokyo": 8})
Serie2
```

Data frame:

```{python}
Combined_serie = pd.DataFrame({
  "Revenue": Serie1,
  "Employee_count": Serie2
  })
  
Combined_serie
```

Sub-setting by row:

```{python}
Combined_serie["Tokyo":]
```

**Creating our own functions:**

Saying hello + name

```{python, warning=FALSE, message=FALSE}
def printing_name(name):
  print('Good morning,', name)
printing_name('Danielle')
```

Multiple arguments:

Saying hello + name + location

```{python, warning=FALSE, message=FALSE}
def welcome(name, location):
  print("Good morning", name, "Welcome to", location)
welcome("Danielle,", "class.")
```

**Bar plot**

```{python}
sns.set_context('paper')

tips = sns.load_dataset("tips")
tips.head()

sns.barplot(x = 'day', y = 'total_bill', hue = 'sex', data = tips, palette = 'Blues', edgecolor = 'w')

plt.show()

tips.groupby(['day','sex']).mean()
```

**1-hot-encoding:**

Transforming categorical features to values from 0 or 1. F.e. You can be
from origin: America or Europe. If the observation is from America it
receives a 1 for America and a 0 for Europe.

**Reading from a csv file with pandas:**

```{python}
import pandas as pd

d = pd.read_csv('~/Documents/Pompeu Fabra BSM/Practical data science/auto-mpg.csv')
```

```{python}
d.mpg.mean().round() # rounding by amount of decimals
```

```{python}
d.dtypes # describes the type of variables
pd.to_numeric(d.hp) # transforms to numerical
```

```{python}
len(d) # amount of rows
d.shape # amount of rows & columns
d.columns # gives the names of the columns
mpg = d.mpg # shows me only the values from one variable
mpg = d['mpg'] # alternative way to show only one value
d.head() # shows the first 5 observations of a data set
d.tail() # shows the last 5 observations of a data set
```

**Basic statistics:**

```{python}
d.mpg.mean()
d.mpg.median()
d.mpg.max()
d.mpg.min()
d.mean() # mean for all columns
d.describe().round(2).head() # statistics for the whole data set, rounded to two decimals
```

```{python}
d[['year']] # defines a list of one variable
d[['year', 'cylinders']] # defines a list of multiple variables
d.year.unique() # gives me the unique values of that column, not the repetitions
```

------------------------------------------------------------------------

## Filtering a data set

```{python}
d77 = d[d.year == 77] #filters the data set to all observations that are equal to a certain value of a variable 

d77 = d[d.year != 77] #filters the data set to all observations that are NOT equal to a certain value of a variable 

d77 = d[d.year <= 77] # filters those that are smaller than a value

d77 = d[d.year >= 77] # filters those that are large than a value

d77 = d[(d.year >= 77) & (d.year <= 90)] # in between certain values

d77 = d[(d.year == 80) | (d.year == 90)] # those from one value OR another

d77 = d[~(d.year == 70)] # excludes the values
```

**Aggregating**

```{python}
dm = d.groupby('year').mean()

dm = d.groupby(['year', 'cylinders']).mean()

dm = d.groupby(['year', 'cylinders']).median()

dm = d.groupby(['year', 'cylinders']).mean()['mpg'] # for selected variables only
```

**Pivot table**

```{python}
d.pivot_table(index='year', columns='cylinders', values='mpg').round()

d.pivot_table(index='year', columns='cylinders', values='mpg').round().fillna('')
```

**Creating new columns starting from existing columns**

```{python}
d['nam_of_new_column'] = d.mpg * 2
```

Through the package numpy:

```{python}
import numpy as np

d['sqrt_of_mpg_2'] = np.sqrt(d.mpg)

d['log(10)_of_mpg'] = np.log10(d.mpg)
```

**Dropping / deleting columns**

```{python}
d['double_mpg'] = d.mpg * 2

del(d['double_mpg']) # deleting columns

d.drop(columns=['sqrt_of_mpg_2', 'log(10)_of_mpg', 'nam_of_new_column']).head() # dropping columns
```

------------------------------------------------------------------------

## Data imputation

```{python}
pd.to_numeric(d.hp, errors='coerce').head()

d.hp = pd.to_numeric(d.hp, errors='coerce')

d[d.hp.isna()] # transform values to NA

d[-(d.hp.isna())] # delete missing values
```

------------------------------------------------------------------------

## Data visualization

Getting the correlation matrix

**Correlation matrix**

```{python}
d.corr().round(decimals=2)
```

Pair plot

```{python}
#sns.pairplot(d);

#sns.pairplot(d, hue='origin'); # with color
```

```{python}
tips = sns.load_dataset("tips")
sns.set_theme(style="whitegrid")

sns.boxplot(data=tips, x='sex', y='tip')

plt.show()
```

```{python}
sns.boxplot(data=tips, x='sex', y='tip', color='black', boxprops=dict(alpha=.6))

plt.show()

sns.boxplot(x="day", y="total_bill", hue="smoker",
  data=tips, palette="Set3")
  
plt.show()
```

```{python}
sns.displot(data=tips, x='tip');
plt.show()

sns.displot(data=tips, x='tip', kind='kde');
plt.show()
```

```{python}
sns.kdeplot(data=tips, x='tip', y='total_bill', shade=True, cbar=True);
plt.show()
```

```{python}
plt.rcParams['figure.figsize'] = (10,8)

plt.show()
```

```{python}
sns.histplot(d.mpg); # Histogram

plt.show()
```

```{python}
sns.histplot(d.mpg, bins=20); # with binwidth

plt.show()

sns.histplot(data=d, x='mpg', bins=20); 

sns.histplot(data=d, x='mpg', bins=20, cumulative=True); # cumulative values
```





<!--chapter:end:06-Python.Rmd-->

---
editor_options: 
  markdown: 
    wrap: 72
---

# Practical data science

## Machine Learning

There are multiple types of machine learning:

-   Supervised

-   Unsupervised

-   Reinforcement learning

#### **Reinforcement learning:**

Alpha go. ML for chess. Trained by trial and error. First they are
taught the simple rules and then asked to train by themselves and learn
from their mistakes. The algorithms are asked to do something and either

-   Get a rewards or

-   A penalty

As a result, they learn which moves are good and continue to try
something else. Machine learning still does not understand casual
relationships.

#### **Unsupervised learning:**

We do not have labels on the data. Can still observe patterns, it
understands there are commonalities but not with a reason.

You can combine labeled data, for example from the passed and look for
patterns with the unlabeled data.

![](images/paste-0F7C76CE.png){width="656"}

#### **Supervised learning**

Supervised learning: Extracting patterns from data and making
predictions based on passed behavior.

An example is a picture of an animal and the algorithms predicts which
animal it is.

Hereby we use training data to train the algorithm. However, the data
must be labeled: we already know the correct answer. This method does
not include trial or error.

For example, first showing examples of cats and then it can make
predictions. Meaning, we show a new picture and it can predict whether
it is a cat or not a cat.

-   Regression tasks: label is a continuous number. Hereby what we want
    to predict is continuous, not necessarily the data given to predict.
    F.E. House prices

-   Classification tasks: Label is one of discrete set of possible
    values. F.E. Is it a dog or a cat

### Theory

Supervised learning - Regression - Classification

+---------------+-----------------------------------------------------+
| Symbol        | Explanation                                         |
+===============+=====================================================+
| y             | Real value                                          |
+---------------+-----------------------------------------------------+
| **Å·**         | Prediction                                          |
+---------------+-----------------------------------------------------+
| **y-Å·**       | Absolute error                                      |
+---------------+-----------------------------------------------------+
| **( y-Å·)\^2** | Squared error \|                                    |
+---------------+-----------------------------------------------------+
| **x**         | Generic input / Features. The things I use to       |
|               | predict y.                                          |
|               |                                                     |
|               | The independent variable (statistics).              |
+---------------+-----------------------------------------------------+
| **y**         | Generic output. The label. (ML) The thing I want to |
|               | predict.                                            |
|               |                                                     |
|               | Dependent variable (statistics).                    |
+---------------+-----------------------------------------------------+
| **p**         | Number of features (machine learning).              |
|               |                                                     |
|               | Number of independent variables I have              |
|               | (statistics).                                       |
+---------------+-----------------------------------------------------+
| **n**         | Size of the data set                                |
+---------------+-----------------------------------------------------+

If I know the inputs, I could try to "predict" the output. This would
state that in the real world the outputs (y) are a function of the
inputs (x). In mathematical terms:

$y = f(x)$

For example, if we know multiple features of a house, we could try to
predict whether a person would like it. (based on f.e. Square meters,
number of bedrooms etc.) However, realistically and economically we
cannot always find all the possible features that would predict an
output. It is always affected by some uncertainty. Therefore, y is a
function of x but adding some noise. Which leads to:

Deterministic function: $y = f(x) + E$

E = (error) noise, a random variable which models some unpredictable
events that happens in the real world that we do not have a
corresponding input for to take into account.

We assume that E obeys at least a couple of properties:

1.  E is not correlated with any of the features
2.  Expected value of the random variable, E[E] = 0

Assumption 1 can be F.E. that someone really wants to buy a house
because there is a good place to put a dog bed. This cannot be predicted
based on the other inputs (features) that are in my data set. They
should not be correlated.

Assumption 2 says that the $e$ doesn't ALWAYS cause either a increase or
a decrease in the output. It has to be truly random.

------------------------------------------------------------------------

The real world: $y = f(x) + E$. We want to try to learn more about this
function f.

Estimator = $\hat f$.

If we do a good job, we are able to find a $\hat f$ that is similar to
the true value of f. If I am able to find a $\hat f$, I can plug in the
input into the estimator and make a prediction. The estimator is the
thing that I want to use to approximate as best as possible the real
relationship between the inputs and outputs in the real world. In
mathematical terms:

$\hat f (x) = \hat y$ = prediction

If the model is **accurate** the prediction is **accurate** =

$\hat f$ is similar to $f$ and therefore $\hat y$ is similar to $y$ and
therefore We have accurate predictions.

*What is key for the data scientist is: **Out-Of-Sample Accuracy.***

This means that your model is accurate with your sample but also with
out-of-sample data.

How can I measure how accurate $\hat y$ is compared to $y$? We look at
the error.

**Squared error:** Most classical error measure = $(y-\hat y)^2$

Multiple reasons on why squared errors are used:

\- Taking square means I forget about the sign of the error (negative vs
positive)\
- Taking square penalizes more 'extreme' errors

![](images/paste-B4EC7C48.png)

Alternative way of valuing the error is the **absolute error:**
$y-\hat y$

#### Data Visualized

Typically we will call our data: x & y

One data point looks like:

$(x1, x2, â€¦., xp, y)$

Here X1 can be independent variable 1 for example square meters. X2 can
be number of rooms and Xp the year it is build. Y is the price of the
house.

![](images/paste-0140ACD4.png){width="535"}

### Finding the expected value of the error

**Random variable:** is described informally as a variable whose values
depend on outcomes of a random phenomenon.

The error is a random variable. Therefore, y is also a random variable
because some of its expression is: $f(x) + Ïµ = y$.

$EÏµ[y-\hat y]^2$ = expected value of the squared error

Because we know that the formula for y is = $f(x) + Ïµ = y$, we can
re-write this expression as:

$EÏµ[(f(x)+ Ïµ-\hat y]^2$

We continue to solve the equation. We can re-write $\hat y$ as
$\hat f(x)$:

$EÏµ[(f(x)+ Ïµ-\hat f(x)]^2$

We rearrange these terms:

$EÏµ[(f(x)-\hat f(x)+ Ïµ]^2$

For ease of notation, $f(x)-\hat f(x)$ becomes alpha

$EÏµ[\alpha+ Ïµ]^2$

We expand:

$EÏµ[\alpha^2 + 2\alphaÏµ + Ïµ^2]$

Make use of it being linear:

$EÏµ[\alpha^2] + 2EÏµ[\alphaÏµ] + EÏµ[Ïµ^2]$

Now we can see that alpha does not have the random element Ïµ making it
not a random variable and is deterministic term (constant). As it is
constant and without error, it is already the expected value. We
re-write again:

$\alpha^2 + 2EÏµ[\alphaÏµ] + EÏµ[Ïµ^2]$

As previously explained in the theory, the expect value of the noise
should be 0. This is the assumption made.

$\alpha^2 + 0 + EÏµ[Ïµ^2]$

The variance of a random variable is, for example variable z =

$Var[z] = E[z^2] - (E[z])^2$

If we apply this definition to the above Ïµ:

$Var[Ïµ] = E[Ïµ^2] - (E[Ïµ])^2$

As we said before, the expected value of Ïµ is 0 and therefore the
variance is:

$Var[Ïµ] = E[Ïµ^2] - 0 = E[Ïµ^2]$

To combine this with the previous equation:

$\alpha^2 + EÏµ[Ïµ^2] = [f(x) - \hat f(x)]^2 + Var[Ïµ]$

This can be separated in two parts:

+---------------------------+---------------------------------+
| $[f(x) - \hat f(x)]^2$    | $Var[Ïµ]$                        |
+---------------------------+---------------------------------+
| Reducible error:          | Irreducible error               |
|                           |                                 |
| Real relation - estimator | Intrinsic property of the error |
+---------------------------+---------------------------------+

If the model is really good, the estimator is similar to the real
relation and I can "reduce" the error. If the model is extremely
precise, the estimator can even be exactly the real relation. It
therefore depends on the accuracy of the estimator.

$\hat f = f$

However, even when this happens, I still cannot affect the "irreducible"
error because it is noise from the real world. It is intrinsic property
/ characteristics of the data, not the estimator.

To conclude, we can only affect the reducible error.

------------------------------------------------------------------------

Data set = $(x1, y1)â€¦.,(xn, yn)$

Data point i = $Xi ÏµR^p$

The estimator is a function that takes a p dimensional and produces a
real values output.

$\hat f:R^p â€“> R$

$\hat y = \hat f (x)$= prediction or estimate

$\hat f$ = estimator

If I have a concrete set of observations, I can estimate the expected
value. For example, I can take the average height of a class to estimate
the expected value of the height of the class.

Mean squared error (MSE): The empirical average of the expected value of
the error term. In mathematical terms:

$MSE(\hat f) = \frac{1}{n} \sum_{i=1}^{n}(y_i-\hat y_i)^2$

Considering that $\hat y_i$ is nothing else than the prediction for the
i input = $\hat y_i = \hat f(x_i)$. Therefore, we can transform again:

$MSE(\hat f) = \frac{1}{n} \sum_{i=1}^{n}(y_i-\hat f(x_i))^2$

I can apply $\hat f$ to one row, calculate the p features and look at
the real label, to compute the MSE.

![](images/paste-DDB3C592.png){width="353"}

Mean absolute error (MAE): Hereby the only difference is that it is not
squared.

$MAE(\hat f) = \frac{1}{n} \sum_{i=1}^{n}[y_i-\hat f(x_i)]$

------------------------------------------------------------------------

### Loss function

We can use a loss function which takes as input two numbers: the real
and the predicted value and gives as output another real number. This is
the formula:

$L(y, \hat y): R^2 = R$

| Squared error                 | Absolute error              |
|-------------------------------|-----------------------------|
| $L(y, \hat y) = (y-\hat y)^2$ | $L(y, \hat y) = (y-\hat y)$ |

\
*I want the loss function to obey two properties:*

1)  If I make a correct prediction $(\hat y = y)$, then I have 0 loss
    $L(y, \hat y) = 0$ if $(\hat y = y)$.

2)  For most loss function, I want $L(y, \hat y)$ to be large than the\
    "wronger" my prediction \$\$\\hat y\$ is. The loss function should
    not become smaller when the prediction becomes "wronger". Wronger =
    the more different my prediction is than the true number.

Loss function should be small when my prediction is close to the true
value and it should be large when it is not.

Estimating the error on existing data on which I know the label:

$Error(\hat f) = \frac{1}{n} \sum_{i=1}^{n}L(y_i-\hat f(x_i))$

Now I am calculating how accurate my model is based on my data set.
However, we want our model to work well on new previously unseen data
that is out of my data set / sample. In other words: **Out of sample
accuracy.**

We therefore, separate our data in two parts: the training set and the
test set.

![](images/paste-ADE4509B.png){width="365"}

**Training set:** Data we show our model to have it learn a good
estimator. $\hat f = +/- f$. The training set will be used to derive to
a estimator.

**Test set:** Data which we hide from our model. After the model has
been trained, we will simulate it to the test data to evaluate the
model's performance. The test set will be sued to estimate the error fo
the $\hat f$.

The training set is called: N = $(x_1,y_1)....,(x_n,y_n)$

The test set is called: M = $(x_1,y_1)....,(x_m,y_m)$

Therefore, to calculate the estimate of the error of the model:

$Err(\hat f) = \frac{1}{n-m} \sum_{i=m+1}^{n}L(y_i-\hat f(x_i))$

\
To train a model = to find good values for its parameters. First I have
to fix the shape of the model.

-   **Linear:** $\hat f(x_1-,x_p) = \beta_0+\beta_1,.....+\beta_p X_p$\
    Beta's are the linear coefficients and the X1, Xp are the variables.
    The $\beta$ 's are parameters. I can train a model to find a good
    estimator by finding good parameters

-   **Quadratic:**
    $\hat f(x_1-,x_p) = \beta_0+\beta_1,.....+\beta_p X_p + \beta_1X_1^2+....\beta_1pX_1p + etc.$

I do not know what model is the good model. So I train each model and
then I pick the model that has the lowest error.

![](images/paste-90F835B4.png)

$x$ = an input (p)

$\beta$ = a vector of parameters (k)

$\beta^*$ = optimal solution of the betas

This is a optimization problem where I try to minimize the empirical
error of the model on the training set. Once we solve the following
model, I find the optimal values for the $\beta$'s and this will find me
the estimator. Once I have my estimator, I can take the test data and
estimate an error for the model.

**Training error
=**$\frac{1}{n} \sum_{i=1}^{n}L(y_i-\hat f(\beta_ix_i))$

**Test error =**
$Err(\hat f) = \frac{1}{n-m} \sum_{i=m+1}^{n}L(y_i-\hat f(\beta^*_i,x_i))$

------------------------------------------------------------------------

<!--chapter:end:07-PracticalDataScience.Rmd-->

`r if (knitr::is_html_output()) '
# References {-}
'`

<!--chapter:end:08-references.Rmd-->


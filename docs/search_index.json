[["advanced-statistical-methods.html", "Chapter - 8 Advanced Statistical Methods 8.1 Clustering 8.2 Multi-Dimensional Scaling 8.3 Principal component analysis 8.4 Correspondence analysis 8.5 Text analytics", " Chapter - 8 Advanced Statistical Methods library(lavaan) library(formattable) library(kableExtra) library(knitr) 8.1 Clustering Clustering has the aim to classify similar objects (cases, people, companies, products, texts, advertising campaigns…) into groups so that within each group the objects are relatively similar, while the groups are different between one another. This is also called segmentation or partitioning. The basic concept in clustering, as well as in most areas of statistics where there are many variables being measured simultaneously on the objects, is that of distance. If we can define a distance between objects then we can devise algorithms to put those “close” to one another in the same group, and those “far” from one another in different groups. The same underlying concept is used in several methods of big data analysis, for example, finding people in a large database that are “close” to you as a client in terms of your purchasing choices. Three types of hierarchical clustering: Complete linkage: maximum method: take the maximum level of the two options Average linkage: average method Ward linkage Significance clustering. Visualized by a dendogram. 8.1.1 Hierachical clustering Argentina Brazil Canada China Germany Spain Italy Netherlands Poland Turkey USA Argentina 0 2 7 6 8 4 6 7 8 4 6 Brazil 2 0 6 6 7 4 5 7 8 4 6 Canada 7 6 0 5 3 4 4 2 4 5 2 China 6 6 5 0 7 7 7 6 6 7 5 Germany 8 7 3 7 0 3 3 1 2 5 4 Spain 4 4 4 7 3 0 1 3 4 3 5 Italy 6 5 4 7 3 1 0 3 4 3 5 Netherlands 7 7 2 6 1 3 3 0 3 5 4 Poland 8 8 4 6 2 4 4 3 0 5 4 Turkey 4 4 5 7 5 3 3 5 5 0 6 USA 6 6 2 5 4 5 5 4 4 6 0 8.1.1.1 Complete Linkage DK.clust &lt;- hclust(as.dist(table)) plot(DK.clust) + abline(h = 6, col=&quot;red&quot;) integer(0) 8.1.1.2 Average linkage plot(hclust(as.dist(table), method=&quot;average&quot;), main=&quot;Average linkage&quot;) + abline(h = 5, col=&quot;red&quot;) integer(0) Significance clustering Total sum of squares (total variance): Each subset has a different mean. We calculate the within-group SS. ANOVA: We look at the distance between groups and compare it with the total sum of square. Then we check if this is significant or if it is random. 8.1.1.3 Ward linkage Ward clustering: related to variance criteria &amp; ANOVA. Of particular interest for all Euclidean and weighted Euclidean distance functions. It decomposed the total variance of the points in space into parts: “Within-clusters” and “between-clusters.” The idea can be illustrated by analysis-of-variance (ANOVA) for one variable (SS= sum[or average] of squared deviations from the mean), when observations are grouped. plot(hclust(as.dist(table), method=&quot;ward.D2&quot;), main=&quot;Ward linkage&quot;, xlab=&quot;Countries&quot;, hang=-1) + abline(h = 8, col=&quot;red&quot;) integer(0) Ward clustering: Ward clustering in multivariate space: For any set of n points in (weighted) Euclidean space: Centroid = overall mean For successful clustering, we want the between-group variance to be as high as possible, which is equivalent to the within-group variance being as low as possible. This is what the hierarchical form of ward clustering does at each merging step of the dendogram. 8.1.2 Non-Hierachical clustering K-means: K = the number of groups. To achieve a simple partitioning of a data set (usually very large) into pre-chosen number of homogeneous groups. Suppose G groups and select 3 points as seeds: the initial point (team leader). Seeds can be chosen at random or by intelligent selection. Next these new groups will calculate its mean. The centroids are now the seeds of the next step. The groups are recalculated and a new centroid is found. This continues until there is no further improvement to make, this is the solution. In these steps, the between-group variance becomes large and the within-group variance decreases. Because the seed is randomly chosen, you should repeat it multiple times to have different starts, and therefore solutions and get the optimal result. You can choose to use the Euclidean distance or the Manhattan distance. Because of the bigger differences in the variables with wide ranges compared to those with narrower ranges, one way to get rid of this effect is to simply divide each variable by its maximum value. Now, there is a range between 0-1. This is classed range-standardized data. 8.2 Multi-Dimensional Scaling Scaling Factorial methods Principal components analysis (PCA) Correspondence analysis (CA) Multidimensional scaling (MDS) Metric MDS Non-metric MDS [1] 0.7181458 0.7909089 [1] 0.09199945 8.3 Principal component analysis satis &lt;- read.csv(&quot;www/satisfaction.csv&quot;, header = TRUE, sep = &quot;,&quot;, dec = &quot;.&quot;) attach(satis) data &lt;- satis[,c(4:12)] row.names(data) &lt;- satis[,2] DK.diss &lt;- dist(data) DK.data &lt;- cmdscale(DK.diss, eig=TRUE) 8.3.1 Biplots Loading required package: ca Loading required package: vegan Loading required package: permute Attaching package: &#39;permute&#39; The following object is masked from &#39;package:LaplacesDemon&#39;: Blocks This is vegan 2.5-7 Attaching package: &#39;vegan&#39; The following object is masked from &#39;package:survey&#39;: calibrate Loading required package: ellipse Attaching package: &#39;ellipse&#39; The following object is masked from &#39;package:graphics&#39;: pairs DK.PCA &lt;- PCA(as.matrix(data), weight=FALSE) PLOT.PCA(DK.PCA, map=&quot;asymmetric&quot;, rescale=0.5, axes.inv=c(-1,1), dim=c(2,3)) 8.4 Correspondence analysis Cross-Tabulation my_table(age.V18.US) SA A NN D SD 15-24 20 59 15 24 7 25-34 31 84 19 88 18 35-44 29 104 22 71 16 45-54 13 91 20 70 25 55-64 16 75 23 66 17 65-74 4 54 10 56 16 75+ 3 35 11 42 14 my.ca &lt;- ca(age.V18.US) plot(my.ca, main=&quot;One parent can bring up a child as well as two parents together&quot;, mass=c(TRUE,TRUE)) summary(my.ca) Principal inertias (eigenvalues): dim value % cum% scree plot 1 0.033590 76.7 76.7 ******************* 2 0.007499 17.1 93.8 **** 3 0.002111 4.8 98.7 * 4 0.000589 1.3 100.0 -------- ----- Total: 0.043789 100.0 Rows: name mass qlt inr k=1 cor ctr k=2 cor ctr 1 | 1524 | 99 996 320 | -360 914 381 | 107 81 152 | 2 | 2534 | 189 990 129 | -61 122 21 | -161 868 656 | 3 | 3544 | 191 922 94 | -141 922 113 | 2 0 0 | 4 | 4554 | 173 920 78 | 100 513 52 | 89 407 184 | 5 | 5564 | 155 128 25 | 25 85 3 | 18 43 6 | 6 | 6574 | 110 931 181 | 258 931 220 | 4 0 0 | 7 | 75 | 83 936 173 | 292 934 211 | 11 1 1 | Columns: name mass qlt inr k=1 cor ctr k=2 cor ctr 1 | SA | 91 995 460 | -450 919 551 | -130 76 205 | 2 | A | 396 868 94 | -64 389 48 | 70 479 262 | 3 | NN | 95 408 62 | -58 119 10 | 91 289 105 | 4 | D | 329 996 233 | 149 719 218 | -93 277 377 | 5 | SD | 89 938 151 | 255 880 173 | 65 58 51 | 8.5 Text analytics "]]

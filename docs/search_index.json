[["index.html", "R-coding Cheatsheets &amp; Summary Preface", " R-coding Cheatsheets &amp; Summary 19 March, 2021 Preface The following document has been prepared to have a prompt link to the code learned and used throughout several projects in the last year. As a beginner in programming, it is always useful to have input accessible to avoid having to look through numerous repositories. The bookdown package that has been used for this format can be installed from CRAN or Github: install.packages(&quot;bookdown&quot;) # or the development version # devtools::install_github(&quot;rstudio/bookdown&quot;) To compile this example to PDF, you will need XeLaTeX. You are recommended to install TinyTeX (which includes XeLaTeX): https://yihui.org/tinytex/. Prepared by: Dani√´lle Kotter "],["basics.html", "Chapter - 1 Basics 1.1 Data sets", " Chapter - 1 Basics Mathematical values \\(\\mu\\) = $\\mu$ = Population mean \\(\\sigma\\) = $\\sigma$ = Population sd \\(\\bar{x}\\) = $\\bar{x} = Sample mean \\({e}\\) = ${e}$ = Standard error \\(\\pi\\) = $\\pi$ = Pie \\(\\ge\\) = $\\ge$ = Bigger than \\(\\le\\) = $\\le$ = Smaller than Sample mean, standard deviation mean &lt;- mean(variable) sd &lt;- sd(variable) Removes values NA in a data set: mean &lt;- mean(variable, na.rm = TRUE) sd &lt;- sd(variable, na.rm = TRUE) Weighted mean &amp; standard deviation library(Hmisc) weightedmean &lt;- wtd.mean(x,y) weightedsd &lt;- sqrt(wtd.var(x,y))/sqrt(n) Variance var(variable) [1] 9.166667 1.0.1 Tables, frames &amp; Matrices As matrix = library(data.table) matrix(c(1:8), nrow = 4, byrow = TRUE) #organized by row [,1] [,2] [1,] 1 2 [2,] 3 4 [3,] 5 6 [4,] 7 8 matrix(c(1:8), ncol = 4, byrow = FALSE) #organized by col [,1] [,2] [,3] [,4] [1,] 1 3 5 7 [2,] 2 4 6 8 As data frame = data.frame(Column1 = c(1:5), Column2 = c(1:5)) Column1 Column2 1 1 1 2 2 2 3 3 3 4 4 4 5 5 5 data.table(Column1 = c(1:5), Column2 = c(1:5)) Column1 Column2 1: 1 1 2: 2 2 3: 3 3 4: 4 4 5: 5 5 As data table = data.table(matrix(c(1:8), nrow = 4)) # or V1 V2 1: 1 5 2: 2 6 3: 3 7 4: 4 8 data.table(Variablex = 1:5, Variabley = 1:5) Variablex Variabley 1: 1 1 2: 2 2 3: 3 3 4: 4 4 5: 5 5 Transforming tables from to other formats = table1 &lt;- data.table(matrix(c(1:8), nrow = 4)) as.data.frame(table1) V1 V2 1 1 5 2 2 6 3 3 7 4 4 8 table2 &lt;- data.frame(Column1 = c(1:5), Column2 = c(1:5)) as.data.table(table2) Column1 Column2 1: 1 1 2: 2 2 3: 3 3 4: 4 4 5: 5 5 Binding and setting names = rbind(table, newvariable) cbind(data, newvariable) rownames(table1) &lt;- c(&quot;One&quot;, &quot;Two&quot;, &quot;Three&quot;, &quot;Four&quot;) colnames(table2) &lt;- c(&quot;One&quot;, &quot;Two&quot;) Changing the order of a frequency table and factor value &lt;- c(&quot;one&quot;, &quot;three&quot;, &quot;five&quot;, &quot;one&quot;, &quot;two&quot;, &quot;three&quot;, &quot;four&quot;, &quot;two&quot;, &quot;three&quot;) table &lt;- as.data.table(table(value)) table value N 1: five 1 2: four 1 3: one 2 4: three 3 5: two 2 table[,`value`:= factor( `value`, levels = c( &quot;one&quot;, &quot;two&quot;, &quot;three&quot;, &quot;four&quot;, &quot;five&quot; ) )] setorder(table, `value`) table value N 1: one 2 2: two 2 3: three 3 4: four 1 5: five 1 Frequencies values &lt;- c(1:10) table(values) values 1 2 3 4 5 6 7 8 9 10 1 1 1 1 1 1 1 1 1 1 prop.table(table(values)) values 1 2 3 4 5 6 7 8 9 10 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 round(prop.table(table(values)) * 100, 2) values 1 2 3 4 5 6 7 8 9 10 10 10 10 10 10 10 10 10 10 10 1.1 Data sets Displaying head or tail of a data set: head(cars) # first 6 rows of the data set speed dist 1 4 2 2 4 10 3 7 4 4 7 22 5 8 16 6 9 10 tail(cars) # last 6 rows of the data set speed dist 45 23 54 46 24 70 47 24 92 48 24 93 49 24 120 50 25 85 Reading excel library(readxl) data &lt;- read.xls(\"data.xlsx\", stringsAsFactors = TRUE) names(cars) # shows the column names of the data set [1] &quot;speed&quot; &quot;dist&quot; attach(cars) # saves the names to be used as variables 1.1.1 Removing infinite + NA values Removing Infinite values Removing NA values Changing Infinite values to NA values variable[is.finite(variable)] [1] 1 2 3 4 5 6 7 8 9 10 variable[is.na(variable)] integer(0) variable[is.infinite(variable)] &lt;- NA 1.1.2 Transforming variable types as.numeric(value) [1] NA NA NA NA NA NA NA NA NA as.character(value) [1] &quot;one&quot; &quot;three&quot; &quot;five&quot; &quot;one&quot; &quot;two&quot; &quot;three&quot; &quot;four&quot; &quot;two&quot; &quot;three&quot; as.factor(value) [1] one three five one two three four two three Levels: five four one three two 1.1.3 Markdown # Putting words in bold: **Word** Result # Putting words in italic: *Word* Result # dashes like this `here` Show up like this: here # dashes like this with the letter r: `r 4+4` Asks r to have inline code. We can see the results here: 8. # \\newpage Will start a new page for example in a pdf document # &gt; # In here we can put a quote # &gt; In here we can put a quote 1.1.4 Setup rmarkdown &amp; code chunks Call Description Warning = TRUE/FALSE Include / exclude warnings Echo = TRUE/FALSE Include / exclude r chunks but show output Include = TRUE/FALSE Run code but do not include in the knitted document Comment = \"\" | Include / exclude ## in output code chunks Message = TRUE/FALSE Includes / excludes message from code out.width=‚Äò100%‚Äô Adjusts size of figure / chart fig.width = Set specific size of figure / chart width fig.height = Set specific size of figure / chart height fig.cap= Adds a title to a figure fig.align= ‚Äòcenter,‚Äô ‚Äòleft,‚Äô ‚Äòright,‚Äô adjust figure / chart at page 1.1.5 Miscellaneous round(0.50, 2) # rounds a value with two decimals [1] 0.5 rep(5,5) #repeats the number 5, 5 times [1] 5 5 5 5 5 describe(variable) vars n mean sd median trimmed mad min max range skew kurtosis se X1 1 10 5.5 3.03 5.5 5.5 3.71 1 10 9 0 -1.56 0.96 fivenum(variable) [1] 1.0 3.0 5.5 8.0 10.0 summary(variable) Min. 1st Qu. Median Mean 3rd Qu. Max. 1.00 3.25 5.50 5.50 7.75 10.00 str(variable) # describing the variable int [1:10] 1 2 3 4 5 6 7 8 9 10 dim(cars) # amount of rows and amount of columns [1] 50 2 1.1.6 Subsetting cars[,1] # subsets by columns [1] 4 4 7 7 8 9 10 10 10 11 11 12 12 12 12 13 13 13 13 14 14 14 14 15 15 [26] 15 16 16 17 17 17 18 18 18 18 19 19 19 20 20 20 20 20 22 23 24 24 24 24 25 cars[1,] # subsets by row speed dist 1 4 2 cars[cars$speed == 4,] # subsets by specific value speed dist 1 4 2 2 4 10 cars[cars$speed &gt; 4,] # subsets over specific value speed dist 3 7 4 4 7 22 5 8 16 6 9 10 7 10 18 8 10 26 9 10 34 10 11 17 11 11 28 12 12 14 13 12 20 14 12 24 15 12 28 16 13 26 17 13 34 18 13 34 19 13 46 20 14 26 21 14 36 22 14 60 23 14 80 24 15 20 25 15 26 26 15 54 27 16 32 28 16 40 29 17 32 30 17 40 31 17 50 32 18 42 33 18 56 34 18 76 35 18 84 36 19 36 37 19 46 38 19 68 39 20 32 40 20 48 41 20 52 42 20 56 43 20 64 44 22 66 45 23 54 46 24 70 47 24 92 48 24 93 49 24 120 50 25 85 cars[cars$speed &lt; 5,] # subsets under specific value speed dist 1 4 2 2 4 10 "],["charts-templates.html", "Chapter - 2 Charts templates", " Chapter - 2 Charts templates library(gridExtra) library(hrbrthemes) library(ggplot2) paletteDani &lt;- c( &quot;#ffa500&quot;, &quot;#DAF7A6&quot;, &quot;#5F7992&quot;, &quot;#69b3a2&quot;, &quot;#ffd561&quot;, &quot;#ee5c42&quot;, &quot;#C8A2C8&quot;, &quot;#5c3170&quot;, &quot;#990000&quot;, &quot;#C70039&quot;, &quot;#34495E&quot;, &quot;#909497&quot;) Several basic options: Pie chart data &lt;- ToothGrowth dani_theme &lt;- theme( axis.title.x = element_blank(), axis.title.y = element_blank(), panel.border = element_blank(), panel.grid = element_blank(), axis.ticks = element_blank(), axis.text.x=element_blank(), legend.title = element_text(face = &quot;bold&quot;), plot.title = element_text(hjust = 0.5, size = 12, face = &quot;bold&quot;) ) ggplot(data, aes(x=&quot;&quot;,y = dose, fill = supp)) + geom_bar(width = 1, stat = &quot;identity&quot;) + coord_polar(&quot;y&quot;, start = 0) + scale_fill_manual(&quot;Legendname:&quot;, values = paletteDani) + dani_theme + labs(title = &quot;Title&quot;, x = &quot;variablX&quot;, y = &quot;variableY&quot; ) Bar chart ggplot(data, aes(x = dose, y = supp)) + geom_bar(stat = &quot;identity&quot;, fill=&quot;#69b3a2&quot;, color=&quot;#e9ecef&quot;) + theme(legend.position=&quot;none&quot;) Histogram ggplot(data = data, aes(len) ) + geom_histogram(fill=&quot;#69b3a2&quot;, color=&quot;#e9ecef&quot;, alpha=0.9) + ggtitle(&quot;Title&quot;) + xlab(&quot;variablex&quot;) + ylab(&quot;variabley&quot;) + theme(plot.title = element_text(size = 11)) `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Boxplot ggplot(data=ToothGrowth, aes(x=&quot;&quot;, y=len, fill=&quot;&quot;)) + geom_boxplot(fill=&quot;#69b3a2&quot;, outlier.colour=&quot;red&quot;, outlier.shape=8, outlier.size=4) + theme_ipsum() + theme( legend.position=&quot;none&quot;, plot.title = element_text(size=12) ) + ggtitle(&quot;Title&quot;) + theme(plot.title = element_text(hjust = 0.5)) + xlab(&quot;&quot;)+ ylab(&quot;&quot;) Scatter plot ggplot(data= ToothGrowth, aes(y = len, x = dose)) + geom_point(size=2) + geom_smooth(method=&quot;lm&quot;, color=&quot;#69b3a2&quot;, fullrange=TRUE, formula = y ~ x) + theme(plot.title = element_text(hjust = 0.5)) + labs(title = &quot;Title&quot;, y = &quot;yname&quot;, x = &quot;xname&quot; ) Scatter plot with dummies ggplot(data = ToothGrowth, aes(y = len, x = supp, colour=factor(supp))) + geom_point(size=2) + geom_smooth(method=&quot;lm&quot;, fill = NA, fullrange=TRUE, formula = y ~ x) + theme(plot.title = element_text(hjust = 0.5)) + scale_colour_manual(name=&quot;Legendtitle&quot;, labels=c(&quot;value1&quot;, &quot;value2&quot;),values = c(&quot;#69b3a2&quot;, &quot;#F6726A&quot;))+ labs(title = &quot;Title&quot;, y = &quot;Yname&quot;, x = &quot;Xname&quot; ) Arrange charts next to each other on a page chart1 &lt;- ggplot(data=ToothGrowth, aes(x=&quot;&quot;, y=len, fill=&quot;&quot;)) + geom_boxplot(fill=&quot;#69b3a2&quot;, outlier.colour=&quot;red&quot;, outlier.shape=8, outlier.size=4) + theme_ipsum() + theme( legend.position=&quot;none&quot;, plot.title = element_text(size=12) ) + ggtitle(&quot;Title&quot;) + theme(plot.title = element_text(hjust = 0.5)) + xlab(&quot;&quot;)+ ylab(&quot;&quot;) chart2 &lt;- ggplot(data=ToothGrowth, aes(x=&quot;&quot;, y=len, fill=&quot;&quot;)) + geom_boxplot(fill=&quot;#69b3a2&quot;, outlier.colour=&quot;red&quot;, outlier.shape=8, outlier.size=4) + theme_ipsum() + theme( legend.position=&quot;none&quot;, plot.title = element_text(size=12) ) + ggtitle(&quot;Title&quot;) + theme(plot.title = element_text(hjust = 0.5)) + xlab(&quot;&quot;)+ ylab(&quot;&quot;) grid.arrange(chart1, chart2, nrow=1, widths=c(0.9,1)) Shows the amounts of missing values (NA) in a data set library(VIM) aggr(cars, numbers = TRUE, prop = c(TRUE, FALSE), cex.axis = 0.5) Density plots with semi-transparent fill ggplot(data = ToothGrowth, aes(x = len, fill = supp)) + geom_density(alpha=.3) + theme( plot.title = element_text(size=15) ) + ggtitle(&quot;Density plot&quot;) + theme(plot.title = element_text(hjust = 0.5, face= &quot;bold&quot;)) + xlab(&quot;&quot;)+ ylab(&quot;Density&quot;) Correlation matrix - pairs panel library(psych) pairs.panels(iris, method = &quot;pearson&quot;, hist.col = &quot;#00AFBB&quot;, density = TRUE, ellipses = TRUE ) Simple plots: model &lt;- lm(dist ~ speed, data = cars) plot(model) plot(model, 4) # cook distance library(car) avPlots(model) "],["probability.html", "Chapter - 3 Probability 3.1 Bayes Theorem 3.2 Discrete Probablity 3.3 Samples, estimation &amp; confidence intervals 3.4 Significance level 3.5 Non-Parametric testing", " Chapter - 3 Probability library(prob) library(LaplacesDemon) # Bayes Theorem library(BSDA) #tsumtest library(actuar) out &lt;- c(&quot;Red&quot;, &quot;White&quot;, &quot;Black&quot;, &quot;Blue&quot;, &quot;Green&quot;) freq &lt;- c(1,2,3,4,5) s &lt;- probspace(out, probs = freq) print(s) x probs 1 Red 0.06666667 2 White 0.13333333 3 Black 0.20000000 4 Blue 0.26666667 5 Green 0.33333333 If you toss two fair coins, what is the probability of two heads? space &lt;- tosscoin(2, makespace = TRUE) p &lt;- Prob(space, toss1 == &quot;H&quot; &amp; toss2 == &quot;H&quot;) The probability is: 0.25 When two dice are thrown, what is the probability of a 3 followed by a 5? space &lt;- rolldie(2, makespace = TRUE) p &lt;- Prob(space, X1 == 3 &amp; (X2 == 5) ) The probability is: 0.03 Sampling from an urn with or without replacement. 3 balls and sample size of 2: sample1 &lt;- urnsamples(1:3, size = 2, replace = TRUE, ordered = TRUE) sample2 &lt;- urnsamples(1:3, size = 2, replace = FALSE, ordered = TRUE) sample3 &lt;- urnsamples(1:3, size = 2, replace = FALSE, ordered = FALSE) sample4 &lt;- urnsamples(1:3, size = 2, replace = TRUE, ordered = FALSE) 3.1 Bayes Theorem Unconditional probability: P(S) and P(NS) Success or no success prS &lt;- c(0.4, 0.6) Conditional probability: P(P | S ) and P( P | NS) Predicted given it is successful Predicted given it is not successful prNS &lt;- c(0.6, 0.2) Bayes prob, posterior probabilities P(S | P) &amp; P(NS | P) BayesTheorem(prS, prNS) [1] 0.6666667 0.3333333 attr(,&quot;class&quot;) [1] &quot;bayestheorem&quot; 3.2 Discrete Probablity 3.2.1 Uniform discrete probability distribution Sample space with a set probability. Size = amount of tries Density function: Individual probability. F.E. Getting a 4 Cumulative density: Uniform for a certain value distribution. F.E. 4 or less. 4 or more? 1-punif 3 Inverse cumulative density: Uniform for a certain probability ( up until a certain value). F.E. up to 25% of the tries Default = # or less. For # or more do: 1-probability of # or less 3.2.2 Binomial distribution Binomial for a specific value for a certain sample. F.E. 2 from the sample are successful. Binomial for a certain distribution of the sample. F.E. At most 2 in the sample are successful. Or 5 or more. Binomial for a certain percentage of the sample. F.E. 25% of the sample has x value or less. Difference between two binomial values. F.E. Prob there are between 4 and 5 of the trials successful. one &lt;- dbinom(x, size = n, prob = y) two &lt;- pbinom(x, size = n, prob = y) three &lt;- qbinom(p, size = n, prob = y) four &lt;- diff(pbinom(c(X,Y), size = n, prob = y)) Default = # or less (left area of the distribution). For # or more do: 1-probability of # or less 3.2.3 Poisson distribution Expected value = \\(n * p = LAMDA\\) Poisson for a certain value. Lambda = n*p. F.E. Prob of having a 5 Poisson for a certain value distribution. F.E. Prob of having less than 5. More than 5? = 1- Ppois(4, lambda) Poisson for a certain probability to capture a certain value. F.E. Poisson value for 25%. one &lt;- dpois(x,lambda) two &lt;- ppois(x,lambda) three &lt;- qpois(x,lambda) Default = # or less (left area of the distribution). For # or more do: 1-probability of # or less 3.2.4 The normal distribution Empirical rule For all normal distributions: 68-95-99.7 rule 99.7% of observations are located between: -3 mu and 3 95% of observations are located between: -2 mu 2 68% of observations are located between: -1 mu 1 Normal distribution Z-value # z &lt;- (x - mean) / sd. For example: (11 - 10) / 2 [1] 0.5 Normal distribution for a certain proportion. Pi = population proportion mean%. Normal distribution for a certain value distribution. F.E. Prob of value above 5. FALSE Prob less than 9. TRUE Normal distribution for a certain probability to capture a certain value. F.E. Value that is given at 25% point. Difference between two values on the normal distribution. F.E. between 5 and 10. one &lt;- pnorm(X, pi, sd, lower.tail = FALSE) two &lt;- pnorm(X, mu, sd, lower.tail = FALSE) three &lt;- qnorm(p, mu, sd, lower.tail = FALSE) four &lt;- diff(pnorm(c(X,Y), mu, sd, lower.tail = FALSE)) lower.tail = TRUE: The area of the left side of the slope lower.tail = FALSE: The area of the right side of the slope Confidence interval for normal distribution z.test(x, sd=sigma) binconf(x = x, n = n) &lt;- proportions t.test(variable) &lt;- t-distribution for conf.inv 3.2.4.1 Plotting the normal distribution \"With mean = 3 and standard deviation = 7 Limits: mean +/- 3 * standard deviation = 3*7 = 21 Lower limit = 3 ‚Äì 21 = -18 Upper limit = 3 + 21 = 24\" Example: x &lt;- seq(15, 45, length=50) y &lt;- dnorm(x, 30, 5) plot(x,y,type=&quot;l&quot;,lwd=2,col=&quot;black&quot;) x &lt;- seq(15,35, length=100) y &lt;- dnorm(x, 30,5 ) polygon(c(15,x,35),c(0,y,0), density = c(15, 35), col = &quot;black&quot;) p &lt;- pnorm(35, mean = 30, sd = 5,lower.tail = TRUE) text(0,0.15,&quot;68%&quot;) 3.2.4.2 Binomial It will be possible to use the Normal distribution as an approximation to the Binomial if: n is large and p &gt; 0.1 Density function (individual probability). Cumulative density (between certain values). Difference between two binomial values Inverse cumulative density. For a certain prob. one &lt;- dbinom(x, mu, sd) two &lt;- pbinom(x, mu, sd, lower.tail = FALSE) three &lt;- diff(pbinom(c(X,Y), mu, sd, lower.tail = FALSE)) four &lt;- qbinom(p, mu, sd, lower.tail = FALSE) 3.3 Samples, estimation &amp; confidence intervals The standard error of the sampling distribution of the mean se &lt;- sigma / sqrt(n) Probability sample To find the probability that X is larger than mu To find the probability that X is smaller than mu p &lt;- pnorm(X, mu, se, lower.tail = TRUE) p &lt;- pnorm(X, mu, se, lower.tail = FALSE) Probability proportions sample sd &lt;- sqrt((pi*(n-pi))/n) z &lt;- (p - pi)/sd p &lt;- pnorm(X, pi, se, lower.tail =FALSE) Sample size Package = ‚Äúsamplingbook.‚Äù Provides the sample size needed to have a 95% confidence to estimate the population mean. Level = confidence level. Se is required standard error. sample.size.mean(se, sigma, level=0.95) 3.4 Significance level 3.4.1 Critical values Critical value for normal distribution, sample &gt; 30 Two-sided: Critical value, 5% significance level = 1.96 Two-sided: Critical value, 1% significance level = 2.58 Two-sided: Critical value, 10% significance level = 1.96 One-sided: Critical value, 5% significance level = 1.64 One-sided: Critical value, 1% significance level = 2.33 One-sided: Critical value, 10% significance level = 1.28 qnorm(0.975) [1] 1.959964 qnorm(0.995) [1] 2.575829 qnorm(0.95) [1] 1.644854 qnorm(0.95) [1] 1.644854 qnorm(0.99) [1] 2.326348 qnorm(0.90) [1] 1.281552 Critical values t-distribution One-sided: critical value at a 5% significance level One-sided: critical value at a 10% significance level One-sided: critical value at a 1% significance level Two-sided: critical value at a 5% significance level Two-sided: critical value at a 10% significance level Two-sided: critical value at a 1% significance level cv &lt;- qt(0.95, df) cv &lt;- qt(0.90, df) cv &lt;- qt(0.99, df) cv &lt;- qt(0.975, df) cv &lt;- qt(0.95, df) cv &lt;- qt(0.995, df) Confidence interval cv &lt;- cv mu &lt;- mu sd &lt;- sd se &lt;- sd / (sqrt(n)) n &lt;- n conf_int95 &lt;- cv * sd / (sqrt(n)) mu_plus &lt;- mu + conf_int95 mu_min &lt;- mu - conf_int95 Large sample significance testing Two-sided One-sided: X is greater than the population mean One-sided: X is less than the population mean library(BSDA) one &lt;- tsum.test(mean.x = X, s.x = sd, n.x = n, mu = mu, alternative = &quot;two.sided&quot;, var.equal = TRUE) two &lt;- tsum.test(mean.x = X, s.x = sd, n.x = n, mu = mu, alternative = &quot;greater&quot;, var.equal = TRUE) three &lt;- tsum.test(mean.x = X, s.x = sd, n.x = n, mu = mu, alternative = &quot;less&quot;, var.equal = TRUE) For proportions: prop.test(x = X, n = n, p = p, correct = TRUE, alternative = ‚Äútwo.sided‚Äù) Same goes for above: two.sided, greater, less 3.4.2 Test of equality - two samples H_0 &lt;- \\(\\mu1 = \\mu2\\) or \\((\\mu1 - \\mu2) = 0\\) H_a &lt;- \\(\\mu1 \\neq \\mu2\\) or \\(\\mu1 - \\mu2 \\neq 0\\) Difference in two means with a certain confidence level confidence interval. Default = 95% tsum.test(mean.x = X, s.x = sd, n.x = n, mean.y = X, s.y = sd, n.y = n, var.equal=FALSE) Welch Modified Two-Sample t-Test data: Summarized x and y t = 0, df = 58, p-value = 1 alternative hypothesis: true difference in means is not equal to 0 95 percent confidence interval: -0.8492568 0.8492568 sample estimates: mean of x mean of y 15 15 2-sample test for equality of proportions without continuity correction. prop.test(data, correct=FALSE, alternative=‚Äúgreater‚Äù) 3.5 Non-Parametric testing 3.5.1 Contengency table / frequencies Obtain contingency table table(ToothGrowth$dose) 0.5 1 2 20 20 20 3.5.2 Chi-square Chi-square test Get the expected value Probability for chi-square data &lt;- matrix(c(27,373,33,567),byrow=TRUE,nrow=2) chisq.test(data,correct=FALSE) Pearson&#39;s Chi-squared test data: data X-squared = 0.66489, df = 1, p-value = 0.4148 chisq.test(data,correct=FALSE)$expected [,1] [,2] [1,] 24 376 [2,] 36 564 prop.table(chisq.test(data,correct=FALSE)$expected,1) [,1] [,2] [1,] 0.06 0.94 [2,] 0.06 0.94 prop.table(chisq.test(data,correct=FALSE)$expected,2) [,1] [,2] [1,] 0.4 0.4 [2,] 0.6 0.6 Degree of freedom = # of row - 1 * # of columns = fixed All expected frequencies must be above five! If not, categories must be combined! 3.5.3 Goodness of fit Uniform: Degree of freedom = number of categories - number of parameters - 1. x &lt;- c(1,2,3,4,5) p &lt;- rep(1/5, 5) chisq.test(x, p = p) Chi-squared test for given probabilities data: x X-squared = 3.3333, df = 4, p-value = 0.5037 All expected frequencies must be above five! If not, categories must be combined! Binomial: dbinom(x, size = n, prob = y) For example: library(actuar) cj &lt;- c(-0.5, 0.5, 1.5, 2.5, 3.5, 4.5, 5.5) #or cj &lt;- seq(from = -0.5, to=5, by=1) nj &lt;- c(15,20,20,18,13) data &lt;- grouped.data(Group = cj, Frequency = nj) p &lt;- mean(data)/5 pr &lt;-c(dbinom(0,5,p),dbinom(1,5,p),dbinom(2,5,p),dbinom(3,5,p),dbinom(4,5,p),dbinom(5,5,p)) nj2 &lt;- c(35,20,18,23) pr2 &lt;- c(dbinom(0,5,p)+dbinom(1,5,p),dbinom(2,5,p),dbinom(3,5,p),dbinom(4,5,p)+dbinom(5,5,p)) chisq.test(nj2,p=pr2) Chi-squared test for given probabilities data: nj2 X-squared = 38.736, df = 3, p-value = 0.00000001975 All expected frequencies must be above five! If not, categories must be combined! Poisson Degree of freedom = number of categories - number of parameters - 1. NOTE! Distribution goes to infinity. Counter for one value that is X or more. 1 - until X. Example: cj &lt;- c(-0.5, 0.5, 1.5, 2.5, 3.5, 4.5, 5.5) #or cj &lt;- seq(from = -0.5, to=6, by=1) nj &lt;- c(16, 30, 37, 7, 10, 5) data &lt;- grouped.data(Group = cj, Frequency = nj) m &lt;- mean(data) pr &lt;- c(dpois(0, m),dpois(1,m),dpois(2, m), dpois(3, m), dpois(4, m), + (1-ppois(4,m)) ) chisq.test(nj, p = pr) Chi-squared test for given probabilities data: nj X-squared = 9.7845, df = 5, p-value = 0.08158 Normal distribution Example: cv &lt;- qchisq(0.90, 2) cj &lt;- c(0, 1, 3, 10, 15, 30) nj &lt;- c(16, 30, 37, 7, 10) data &lt;- grouped.data(Group = cj, Frequency = nj) m &lt;- mean(data) s &lt;- sqrt(emm(data,2)) pr &lt;- c(pnorm(1,m,s), diff(pnorm(c(1,3),m,s)), diff(pnorm(c(3,10),m,s)), diff(pnorm(c(10,15),m,s)), 1 - pnorm(c(15),m,s) ) chisq.test(nj,p=pr) Chi-squared test for given probabilities data: nj X-squared = 77.503, df = 4, p-value = 0.0000000000000005887 ###Mann-whitney test N = Number of pairs - number of draws For small tests c1 values sample 1 c2 values sample 2 wilcox.text(x, c2) Larger sample test &gt; 10 You can use a approximation based on the normal distribution. Therefore critical values will be 1.96 for this two sided test. ###Wilcoxon test Two options - Do not predict direction ‚Äì&gt; two sided - Predict direction ‚Äì&gt; one sided wilcox.test(w1, w2, paired=TRUE,correct=FALSE) ###Run test library(randtests) pers &lt;- c(0,1,1,0,0,0,0,1,1,0,1) pers.f &lt;- factor(pers,labels=c(&quot;Male&quot;,&quot;Female&quot;)) runs.test(pers) Runs Test data: pers statistic = NaN, runs = 1, n1 = 5, n2 = 0, n = 5, p-value = NA alternative hypothesis: nonrandomness 3.5.4 P-value Find p value: Probability of getting this test statistic or more: pchisq(ts, df, lower.tail=FALSE) [1] 0.5578254 "],["simple-regressions.html", "Chapter - 4 Simple regressions 4.1 Basics regressions 4.2 Prediction 4.3 Data problems", " Chapter - 4 Simple regressions 4.1 Basics regressions Regressions, correlation and dummy‚Äôs Y = Dependent X = Explanatory Correlation cor(x, y) [1] 0.8068949 Creating the regression: To plot the regression model Evaluates the coefficient of the model Only the first colum estimattion model &lt;- lm(y~x, data = data) summary(model)$coef Estimate Std. Error t value Pr(&gt;|t|) (Intercept) -17.579095 6.7584402 -2.601058 0.012318816153809090 x 3.932409 0.4155128 9.463990 0.000000000001489836 est &lt;- summary(model)$coef[,1] 4.1.1 Summarizing regressions: Using stargazer package library(stargazer) stargazer(lm(y~x, data=data), type=&quot;text&quot;) =============================================== Dependent variable: --------------------------- y ----------------------------------------------- x 3.932*** (0.416) Constant -17.579** (6.758) ----------------------------------------------- Observations 50 R2 0.651 Adjusted R2 0.644 Residual Std. Error 15.380 (df = 48) F Statistic 89.567*** (df = 1; 48) =============================================== Note: *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01 # Multiple models adjacent model1 &lt;- lm(y~x, data=data) model2 &lt;- lm(x~y, data=data) stargazer(model1, model2, type=&quot;text&quot;) ========================================================== Dependent variable: ---------------------------- y x (1) (2) ---------------------------------------------------------- x 3.932*** (0.416) y 0.166*** (0.017) Constant -17.579** 8.284*** (6.758) (0.874) ---------------------------------------------------------- Observations 50 50 R2 0.651 0.651 Adjusted R2 0.644 0.644 Residual Std. Error (df = 48) 15.380 3.156 F Statistic (df = 1; 48) 89.567*** 89.567*** ========================================================== Note: *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01 Using summary function: summary(lm(y~x)) Call: lm(formula = y ~ x) Residuals: Min 1Q Median 3Q Max -29.069 -9.525 -2.272 9.215 43.201 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) -17.5791 6.7584 -2.601 0.0123 * x 3.9324 0.4155 9.464 0.00000000000149 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 15.38 on 48 degrees of freedom Multiple R-squared: 0.6511, Adjusted R-squared: 0.6438 F-statistic: 89.57 on 1 and 48 DF, p-value: 0.00000000000149 Regressions Plotting regression plot(y~x,data=data, main=&quot;Title&quot;, ylab=&quot;yname&quot;, xlab=&quot;xname&quot; ) Including regression line: plot(y~x,data=data, main=&quot;Title&quot;, ylab=&quot;yname&quot;, xlab=&quot;xname&quot; ) abline(lm(y~x, data=data), col=&quot;blue&quot;) Confidence interval around slope confint(lm(y~x), level=0.95) 2.5 % 97.5 % (Intercept) -31.167850 -3.990340 x 3.096964 4.767853 Sub-sampling regression Specify dimensions [,]. First is row. Column, second. Selects the rows where age is larger than 5. Lower than 5. sub1 &lt;- summary(lm(y~x, data=data[&quot;speed&quot;&gt;=5,])) sub2 &lt;- summary(lm(y~x, data=data[&quot;speed&quot;&lt;=5,])) 4.1.2 Dummy variables, diff in means 4.1.3 Regression + dummy Y = Constant0 + B0 * X - Diff in means + B1 * variable1*2 Omitting the intercept: Shows the means separately and not the difference between means. Tests whether the expected counts are different from zero. lm(y ~ x - 1, data = data) Call: lm(formula = y ~ x - 1, data = data) Coefficients: x 2.909 Reorders group, to specific value to be first. variable2 &lt;- relevel(variable, ‚ÄúC‚Äù) 4.2 Prediction model &lt;- lm(y~x) newdata &lt;- data.frame(variablename = c(1:50)) pred &lt;- predict(model, newdata = newdata) Prediction confidence interval: One value Multiple values from a existing data frame pred1 &lt;- predict(model, data.frame(valuename = x), interval = &quot;confidence&quot;, level=0.95) pred2 &lt;- predict(model, newdata = newdata, interval = &quot;confidence&quot;, level=0.95) Prediction interval One value Multiple values from a existing data frame pred1 &lt;- predict(model, data.frame(valuename = x), interval=&quot;predict&quot;,level=0.95) pred2 &lt;- predict(model, newdata, interval=&quot;predict&quot;,level=0.95) 4.2.1 Confidence and prediction plotting Adds: observed values, fitted line, conf interval, predicted interval library(HH) fit &lt;- lm(y~x, data = data) ci.plot(fit) 4.2.2 Prediction with dummy variables Prediction = ùõº1+ùõº2Constant Dummy+ùõΩ1ùëÜùëñùëßùëí+ùõΩ2Slope Dummy 4.2.3 Prediction intervals examples Prediction fit &lt;- lm(y ~ x + d + d, data = data) pred &lt;- predict(fit, data.frame(VariableX = c(10), Dummy = c(1), Slopedummy = c(10)) ) Confidence interval prediction fit &lt;- lm(y ~ x + d + d, data = data) pred &lt;- predict(fit, data.frame(VariableX = c(10), Dummy = c(1), Slopedummy = c(10), interval=&quot;confidence&quot;)) Prediction interval fit &lt;- lm(y ~ x + d + d, data = data) pred &lt;- predict(fit, data.frame(VariableX = c(10), Dummy = c(1), Slopedummy = c(10), interval=&quot;predict&quot;)) 4.3 Data problems Residual plot # residual.plots(fitted(fit), resid(fit), sigma.hat(fit), main=&quot;Title&quot;) Influential measure test im &lt;- influence.measures(fit) 4.3.1 Multicollinearity F-test fit &lt;- lm(y~x + d, data = data) anova(fit) Analysis of Variance Table Response: y Df Sum Sq Mean Sq F value Pr(&gt;F) x 1 21186 21185.5 89.567 0.00000000000149 *** Residuals 48 11354 236.5 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 4.3.2 Variance inflation factors The variance inflation factor (vif) is \\(1 / 1‚àíR2\\). A simple approach to identify collinearity among explanatory variables is the use of variance inflation factors (VIF). It is the ratio of variance in a model with multiple terms, divided by the variance of a model with one term alone. The higher the value, the higher the collinearity. A VIF for a single explanatory variable is obtained using the r-squared value of the regression of that variable against all other explanatory variables: A VIF is calculated for each explanatory variable and those with high values are removed. The definition of ‚Äòhigh‚Äô is somewhat arbitrary, but values in the range of 5-10 are commonly used for ‚Äòhigh.‚Äô If VIF value exceeding 4.0, or by tolerance less than 0.2 then there is a problem with multicollinearity (Hair et al., 2010). However, it depends on the researcher‚Äôs criteria. The lower the vif the better, but you shouldn‚Äôt be too concerned as long as your VIF is not greater than 10. vif(fit) x d 1 NaN 4.3.3 ANOVA One-way: one value res.aov &lt;- aov(y ~ x, data = data) summary(res.aov) Df Sum Sq Mean Sq F value Pr(&gt;F) x 1 21185 21185 89.57 0.00000000000149 *** Residuals 48 11354 237 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Two-way: more than two factors res.aov &lt;- aov(y ~ x + d, data = data) summary(res.aov) Df Sum Sq Mean Sq F value Pr(&gt;F) x 1 21185 21185 89.57 0.00000000000149 *** Residuals 48 11354 237 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 With interaction res.aov &lt;- aov(y ~ x * d, data = data) summary(res.aov) Df Sum Sq Mean Sq F value Pr(&gt;F) x 1 21185 21185 89.57 0.00000000000149 *** Residuals 48 11354 237 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Three-way Three way With interaction summary(aov(y ~ x + d, data=data)) Df Sum Sq Mean Sq F value Pr(&gt;F) x 1 21185 21185 89.57 0.00000000000149 *** Residuals 48 11354 237 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 summary(aov(y ~ x + d, data=data)) Df Sum Sq Mean Sq F value Pr(&gt;F) x 1 21185 21185 89.57 0.00000000000149 *** Residuals 48 11354 237 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 MANOVA: Multiple factors Test in difference Test separately test_manova &lt;- manova(cbind(y, d) ~ x, data = data) summary(test_manova) summary.aov(test_manova) 4.3.4 Linearizing variables logged &lt;- log(iris$Sepal.Length) # non-linear quad &lt;- cars$speed ^ 2 # quadratic "],["structure-equation-models.html", "Chapter - 5 Structure equation models 5.1 Coding the model 5.2 Factor model", " Chapter - 5 Structure equation models Structural Equation Modeling (SEM): is an extremely broad and flexible framework for data analysis, perhaps better thought of as a family of related methods rather than as a single technique. Measuring latent constructs is challenging and we must also incorporate estimates of measurement error into our models. SEM excels at both of these tasks. SEM is especially suited for causal analysis.(Gray 2017) Path analysis (structural equations) Path diagrams = Communicates the structure of our model. Useful for structural equation models. Rectangular = any variable that we can observe directly (observed variable), measured variables Circle / Ovals = Cannot be observed (Latent variable) Arrow = directed effect. One variable impacts the other. Hypothetical causal relationship. Numbers by the arrows = regression coefficient. Correlations coefficients. Triangle is the constant in the Linear Model. Double handed arrows = indicate co-variances or correlations without a causal interpretation. SEM -&gt; style=‚Äúram‚Äù to get circles around the measurement arrow Double handed arrow between two independent variables = they are correlated to each other. Covariance Residual error term = measurement errors. We expect that the factor will not perfectly predict the observed variables. The bidirected arrows, the ones with two side arrows (in this representation, a connecting line with no arrows) represent covariances among variables. Ordinary least-squares (OLS): models assume that the analyst is fitting a model of a relationship between one or more explanatory variables and a continuous or at least interval outcome variable that minimizes the sum of square errors, where an error is the difference between the actual and the predicted value of the outcome variable. The most common analytic method that utilizes OLS models is linear regression (with a single or multiple predictor variables). OLS: $$b = beta * k$ Latent variables = unobserved variables or unmeasured variables in SEM lingo. These are theoretical concepts which can be inferred but not directly measured. Linear regression model = \\(Y = alpha + betaX + error\\) The model has to account for randomization. If there is a factor that that cannot be explained, it is included in the error. When this unmeasured factor that is in the error is correlated to another independent variable, there is endogeneity. Endogeneity variables = correlated with the error terms. Arises when the marginal distribution of the independent variable is not independent of the conditional distribution of the dependent variable given the independent. Exogenous variables = not driven by other factors (observable or observable) Sources of endogeneity = 1. Omitted variables: relevant variables left out of the model, attributing to effect to those that were included. 2. Simultaneity: where x causes y and y causes x 3. Selection bias: sampling bias 5.1 Coding the model library(semPlot) library(lavaan) Setting up the model and summarizing fit &lt;- &#39;dist ~ speed&#39; model &lt;- lavaan(fit, data = cars, estimator=&quot;MLM&quot;, auto.var = TRUE) summary(model) lavaan 0.6-8 ended normally after 15 iterations Estimator ML Optimization method NLMINB Number of model parameters 2 Number of observations 50 Model Test User Model: Standard Robust Test Statistic 0.000 0.000 Degrees of freedom 0 0 Parameter Estimates: Standard errors Robust.sem Information Expected Information saturated (h1) model Structured Regressions: Estimate Std.Err z-value P(&gt;|z|) dist ~ speed 3.932 0.399 9.864 0.000 Variances: Estimate Std.Err z-value P(&gt;|z|) .dist 227.070 54.619 4.157 0.000 Setting up the path diagram semPaths(model, &quot;std&quot;, title = FALSE, weighted = FALSE, sizeInt = 4, sizeMan = 5, edge.label.cex = 1.3, asize = 2) title(&quot;SEM path&quot;, line = 3) With latent variable &amp; sem model &lt;- &quot; # regression Petal =~ Petal.Width + Petal.Length Sepal.Length ~ Sepal.Width + Petal &quot; fit &lt;- sem(model, data = iris, sample.cov = S, sample.nobs = 122) semPaths(fit, &quot;std&quot;, sizeInt = 4, sizeMan = 3, edge.label.cex = 1, asize=3, weighted=TRUE, exoCov = TRUE) Testing the model fitMeasures(fit)[ c(&quot;chisq&quot;,&quot;df&quot;, &quot;pvalue&quot; ,&quot;rmsea&quot;)] chisq df pvalue rmsea 35.94875570001079 2.00000000000000 0.00000001562525 0.33639637185920 Chisq is a chi-squared test statistic for. H0: moment restrictions implied by the model hold true The degrees of freedom P-value of the chi-square test Rmsea is a fit index, the root mean square of approximation. It should be small for a good fit of the model. Threshold of .05 is often applied to declare good fit. Modification indices modi = modindices(fit) modi[order(modi[,4], decreasing=T), ] lhs op rhs mi epc sepc.lv sepc.all sepc.nox 13 Sepal.Width ~ Sepal.Length 30.061 -0.229 -0.229 -0.499 -0.499 14 Sepal.Width ~ Petal 30.061 -0.263 -0.189 -0.435 -0.435 16 Petal ~ Sepal.Width 30.061 -0.719 -1.002 -0.435 -1.002 15 Petal ~ Sepal.Length 30.061 -1.104 -1.539 -1.456 -1.456 Parameters fit parameterestimates(fit, standardized = TRUE, rsquare=TRUE, ci=FALSE)[1:4,] # showing the top 4 lhs op rhs est se z pvalue std.lv std.all std.nox 1 Petal =~ Petal.Width 1.000 0.000 NA NA 0.717 0.944 0.944 2 Petal =~ Petal.Length 2.500 0.065 38.745 0 1.794 1.020 1.020 3 Sepal.Length ~ Sepal.Width 0.651 0.058 11.158 0 0.651 0.299 0.688 4 Sepal.Length ~ Petal 1.149 0.052 22.015 0 0.824 0.871 0.871 Multiple regression We can assume that the independent variables are correlated. The residual error in multiple regression analysis is actually an unobserved, latent variable. The residual error is 1, to achieve identification. Estimator = MLM in the model, protects for non-linearity, non-normality and elasticity of the raw data. Need the raw data, not only the covariance. Structural equation = \\(y = beta*x + e; var(y)=cov(y,y)\\) \\(Beta^2* var(x) + var(e-) = y\\) 5.1.1 Covariance Covariance is a measure of how much two random variables vary together. It‚Äôs similar to variance, but where variance tells you how a single variable varies, co variance tells you how two variables vary together. Cov(1,2) = \\(sum (1- mean(1) * (2-mean(2) / n\\) Moment matrix: Covariance matrix: cov(cars) speed dist speed 27.95918 109.9469 dist 109.94694 664.0608 Difference between covariance and correlation. Creating the covariance matrix when only having the lower half: lower &lt;- &quot; 0.03300863 0.15894229 5.0185561 0.15670560 0.9841531 1.2142232 &quot; S &lt;- getCov(lower, names = c(&quot;variable1&quot;, &quot;variable2&quot;, &quot;variable3&quot;)) print(S) variable1 variable2 variable3 variable1 0.03300863 0.1589423 0.1567056 variable2 0.15894229 5.0185561 0.9841531 variable3 0.15670560 0.9841531 1.2142232 5.1.2 Reliability In statistics reliability is the consistency of a set of measurements or measuring instrument, often used to describe a test. This can either be whether the measurements of the same instrument give or are likely to give the same measurement (test-retest), or in the case of more subjective instruments, such as personality or trait inventories, whether two independent assessors give similar scores (inter-rater-reliability). Reliability is inversely related to random error. Divide true variance of the true / observed variance = reliability \\(X = T + e\\) \\(Reliability of X = var(T) / var(T) + var(e)\\) \\(var(Tintake) / var(Ointake) = 1 - var(error) / var(Ointake)\\) In words, reliability is defined as a proportion of observed variance that is true variance. Reliability is interpreted as a proportion‚Äîreliability cannot be negative. Reliability = 1- Latent variable variance / variance variable 1 K = variance(true)/variance(Observed(true+error)) 1- Measurement error variable 1 / measurement error variable 2 (latent varible) https://www.uwo.ca/fhs/tc/labs/07.Reliability.pdf Various kinds of reliability coefficients, with values ranging between 0.00 (much error) and 1.00 (no error), are usually used to indicate the amount of error in the scores.\" [2] For example, measurements of people‚Äôs height and weight are often extremely reliable. The square of the standardized loading is the reliability of the variable. Example: Here the reliability of distance (dst) = \\(0.35 ^ 2 = 0.12 = k\\) ____________________________________________________ Testing the model fitMeasures(fit) [ c(\"chisq\", \"df\", pvalue\", \"rmsea\")] MI = if i liberate one parameter, then __ there would Model chi-square test. We test whether the fitted model is correct. HO: moment restrictions implied by the model hold. The fit is correct If &gt; 0.05 we cannot reject the model. I accept the model. Therefore, the chi-square test allows researchers to evaluate the fitness of a model by using the null hypothesis significance test approach. The Root Mean Square Error of Approximation (RMSEA) = fit index: how the covariance fit in the model. Difference between observed and the fitted. The RMSEA is widely used in Structural Equation Modeling to provide a mechanism for adjusting for sample size where chi-square statistics are used. Measures the discrepancy due to the approximation per degree of freedom. The objective is to have the RMSEA as low as possible. MLM estimation Raw data is needed. Independent variables have variances and co-variances, unless the model specification puts them to zero. The square of the standardized loading is the reliability of the variable. For example, the reliability k of AM1 is .59**2 = 0.35 . Measurement equation = TSE =~ 1SE1 + 1SE2 Regression equation PERF ~ TS + TSE + VERB Degrees of freedom (df) = Number of observations available for model estimation - Number of observations used to estimate parameters df for empty model = ( k(k-1) ) / 2 ‚ÄúNumber of free parameters‚Äù refers to all of the things that this model estimated freely Parameter is a regression coefficient when standardized is called a beta coefficient. https://fredclavel.org/2014/05/03/disentangling-degrees-of-freedom-for-sem/#:~:text=The%20degrees%20of%20freedom%20for,%3D%2015%20%E2%80%93%205%20%3D%2010 5.2 Factor model Factor analysis: is a statistical method used to describe variability among observed, correlated variable in terms of a potentially lower number of unobserved variables called factors. For example, it is possible that variations in six observed variables mainly reflect the variations in two unobserved (underlying) variables. Factor analysis searches for such joint variations in response to unobserved latent variables. The observed variables are modelled as linear combinations of the potential factors, plus ‚Äúerror‚Äù terms. Common return that has a impact to multiple variables. You can find how much variance is due to permanent and depended on variables. Factor model include means: Spearman model in 1903. Basic model - Y, X are centered Y = beta X + error Y, X NOT centered - including constant Y = alpha*1 + beta X + error Factor model Y = lambda*F + error1 Lamba = the weights F = common factor Error = Specific factors Instead of setting the alpha to 1, we add a weight for lambda When the data is standardized, beta becomes the standardized beta coefficient. Because y, x are centered, you do not need to constant. Next step is factor models with simultaneous equations ‚Äì&gt; ML estimation of a general model. Linear structural relations You have to fix the variance of a variable that you do not observe. If you do not do this, the model is not identified. Meaning there is no minimum. Including latent variables two options: A) Var(F) = 1, F Standardized (F typically has mean zero) B) Lambda_1 = 1 They are equivalent regarding degrees of freedom or model fit. When you put 1* variable1, you force the true beta to be 1. In r setting the model = Regression equation F = ~ NAPh Y ~~ 1Y Default NA = not available. Asking R to calculate the beta instead of setting it to 1. Gives = Y = Lambda*F + Error When you bring more equation, the degrees of freedom increases. Y ~~ 1* Y = force that it is standardized 0 degrees of freedom = you cannot test but you can fit. You cannot reject the model, but you can check the reliability. Reliability = variance of the factor / variance of the factor + variance of the variable How much variation is due to the permanent of the component. Modindices(fit). It tells you how you could improve your model Two explanatory variables giving possibility for correlation = X~~X Equality with error variances (multiple independent variables) X~~X A = setting a restriction. Variance is to be estimated. Imposing restrictions. When you include variances, you put a lot of tension on your model. Autoregressive, one variables keeps impacting the following. For example: 2011 impacts 2012 which impacts 2013 etc. Simplex model "],["references.html", "References", " References Gray, kevin. 2017. Structural Equation Model. kdnuggets. https://www.kdnuggets.com/2017/03/structural-equation-modeling.html. "]]

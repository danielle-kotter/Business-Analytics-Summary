[["index.html", "Business Analytics - Cheatsheets &amp; Summary Preface", " Business Analytics - Cheatsheets &amp; Summary 21 June, 2021 Preface The following document has been prepared to have a prompt link to the code learned and used throughout several projects in the last year. As a beginner in programming, it is always useful to have input accessible to avoid having to look through numerous repositories. The bookdown package that has been used for this format can be installed from CRAN or Github: install.packages(&quot;bookdown&quot;) # or the development version # devtools::install_github(&quot;rstudio/bookdown&quot;) To compile this example to PDF, you will need XeLaTeX. You are recommended to install TinyTeX (which includes XeLaTeX): https://yihui.org/tinytex/. Prepared by: Daniëlle Kotter "],["basics-r.html", "Chapter - 1 Basics R 1.1 Data sets", " Chapter - 1 Basics R Sample mean, standard deviation mean &lt;- mean(variable) sd &lt;- sd(variable) Removes values NA in a data set: mean &lt;- mean(variable, na.rm = TRUE) sd &lt;- sd(variable, na.rm = TRUE) Weighted mean &amp; standard deviation library(Hmisc) weightedmean &lt;- wtd.mean(x,y) weightedsd &lt;- sqrt(wtd.var(x,y))/sqrt(n) Variance var(variable) [1] 9.166667 1.0.1 Tables, frames &amp; Matrices As matrix = library(data.table) matrix(c(1:8), nrow = 4, byrow = TRUE) #organized by row [,1] [,2] [1,] 1 2 [2,] 3 4 [3,] 5 6 [4,] 7 8 matrix(c(1:8), ncol = 4, byrow = FALSE) #organized by col [,1] [,2] [,3] [,4] [1,] 1 3 5 7 [2,] 2 4 6 8 As data frame = data.frame(Column1 = c(1:5), Column2 = c(1:5)) Column1 Column2 1 1 1 2 2 2 3 3 3 4 4 4 5 5 5 data.table(Column1 = c(1:5), Column2 = c(1:5)) Column1 Column2 1: 1 1 2: 2 2 3: 3 3 4: 4 4 5: 5 5 As data table = data.table(matrix(c(1:8), nrow = 4)) # or V1 V2 1: 1 5 2: 2 6 3: 3 7 4: 4 8 data.table(Variablex = 1:5, Variabley = 1:5) Variablex Variabley 1: 1 1 2: 2 2 3: 3 3 4: 4 4 5: 5 5 Transforming tables from to other formats = table1 &lt;- data.table(matrix(c(1:8), nrow = 4)) as.data.frame(table1) V1 V2 1 1 5 2 2 6 3 3 7 4 4 8 table2 &lt;- data.frame(Column1 = c(1:5), Column2 = c(1:5)) as.data.table(table2) Column1 Column2 1: 1 1 2: 2 2 3: 3 3 4: 4 4 5: 5 5 Binding and setting names = rbind(table, newvariable) cbind(data, newvariable) rownames(table1) &lt;- c(&quot;One&quot;, &quot;Two&quot;, &quot;Three&quot;, &quot;Four&quot;) colnames(table2) &lt;- c(&quot;One&quot;, &quot;Two&quot;) Changing the order of a frequency table and factor value &lt;- c(&quot;one&quot;, &quot;three&quot;, &quot;five&quot;, &quot;one&quot;, &quot;two&quot;, &quot;three&quot;, &quot;four&quot;, &quot;two&quot;, &quot;three&quot;) table &lt;- as.data.table(table(value)) table value N 1: five 1 2: four 1 3: one 2 4: three 3 5: two 2 table[,`value`:= factor( `value`, levels = c( &quot;one&quot;, &quot;two&quot;, &quot;three&quot;, &quot;four&quot;, &quot;five&quot; ) )] setorder(table, `value`) table value N 1: one 2 2: two 2 3: three 3 4: four 1 5: five 1 Frequencies values &lt;- c(1:10) table(values) values 1 2 3 4 5 6 7 8 9 10 1 1 1 1 1 1 1 1 1 1 prop.table(table(values)) values 1 2 3 4 5 6 7 8 9 10 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 round(prop.table(table(values)) * 100, 2) values 1 2 3 4 5 6 7 8 9 10 10 10 10 10 10 10 10 10 10 10 1.1 Data sets Displaying head or tail of a data set: head(cars) # first 6 rows of the data set speed dist 1 4 2 2 4 10 3 7 4 4 7 22 5 8 16 6 9 10 tail(cars) # last 6 rows of the data set speed dist 45 23 54 46 24 70 47 24 92 48 24 93 49 24 120 50 25 85 Reading excel library(readxl) data &lt;- read.xls(\"data.xlsx\", stringsAsFactors = TRUE) names(cars) # shows the column names of the data set [1] &quot;speed&quot; &quot;dist&quot; attach(cars) # saves the names to be used as variables 1.1.1 Removing infinite + NA values Removing Infinite values Removing NA values Changing Infinite values to NA values variable[is.finite(variable)] [1] 1 2 3 4 5 6 7 8 9 10 variable[is.na(variable)] integer(0) variable[is.infinite(variable)] &lt;- NA 1.1.2 Transforming variable types as.numeric(value) [1] NA NA NA NA NA NA NA NA NA as.character(value) [1] &quot;one&quot; &quot;three&quot; &quot;five&quot; &quot;one&quot; &quot;two&quot; &quot;three&quot; &quot;four&quot; &quot;two&quot; &quot;three&quot; as.factor(value) [1] one three five one two three four two three Levels: five four one three two 1.1.3 Markdown # Putting words in bold: **Word** Result # Putting words in italic: *Word* Result # dashes like this `here` Show up like this: here # dashes like this with the letter r: `r 4+4` Asks r to have inline code. We can see the results here: 8. # \\newpage Will start a new page for example in a pdf document # &gt; # In here we can put a quote # &gt; In here we can put a quote 1.1.4 Setup rmarkdown &amp; code chunks Call Description Warning = TRUE/FALSE Include / exclude warnings Echo = TRUE/FALSE Include / exclude r chunks but show output Include = TRUE/FALSE Run code but do not include in the knitted document Comment = \"\" | Include / exclude ## in output code chunks Message = TRUE/FALSE Includes / excludes message from code out.width=‘100%’ Adjusts size of figure / chart fig.width = Set specific size of figure / chart width fig.height = Set specific size of figure / chart height fig.cap= Adds a title to a figure fig.align= ‘center,’ ‘left,’ ‘right,’ adjust figure / chart at page 1.1.5 Latex Symbol Writing Description \\(\\mu\\) $\\mu$ Population mean \\(\\sigma\\) $\\sigma$ Population standard deviation \\(\\bar{x}\\) $\\bar{x} Sample mean \\({e}\\) ${e}$ Standard error \\(\\ge\\) $\\ge$ Bigger than \\(\\le\\) $\\le$ Smaller than \\(\\pi\\) $\\pi$ Pie \\(\\hat f\\) $\\hat f$ Estimator \\(\\frac{1}{2}\\) $\\frac{1}{2}$ Fraction \\(\\sum\\) $\\sum$ Sum \\(\\sum_{i=1}^{n}\\) $\\sum_{i=1}^{n}$ Sequence \\(\\nabla\\) | $\\nabla$ | Gradient, Nabla \\(\\partial\\) $\\partial$ Partial \\(\\beta\\) | $\\beta$ | Beta \\(\\beta^1\\) $\\beta^1$ Beta 1 \\(\\alpha\\) $\\alpha$ Alpha \\(\\approx\\) $\\approx$ Similar to 1.1.6 Miscellaneous round(0.50, 2) # rounds a value with two decimals [1] 0.5 rep(5,5) #repeats the number 5, 5 times [1] 5 5 5 5 5 describe(variable) vars n mean sd median trimmed mad min max range skew kurtosis se X1 1 10 5.5 3.03 5.5 5.5 3.71 1 10 9 0 -1.56 0.96 fivenum(variable) [1] 1.0 3.0 5.5 8.0 10.0 summary(variable) Min. 1st Qu. Median Mean 3rd Qu. Max. 1.00 3.25 5.50 5.50 7.75 10.00 str(variable) # describing the variable int [1:10] 1 2 3 4 5 6 7 8 9 10 dim(cars) # amount of rows and amount of columns [1] 50 2 1.1.7 Subsetting cars[,1] # subsets by columns [1] 4 4 7 7 8 9 10 10 10 11 11 12 12 12 12 13 13 13 13 14 14 14 14 15 15 [26] 15 16 16 17 17 17 18 18 18 18 19 19 19 20 20 20 20 20 22 23 24 24 24 24 25 cars[1,] # subsets by row speed dist 1 4 2 cars[cars$speed == 4,] # subsets by specific value speed dist 1 4 2 2 4 10 cars[cars$speed &gt; 4,] # subsets over specific value speed dist 3 7 4 4 7 22 5 8 16 6 9 10 7 10 18 8 10 26 9 10 34 10 11 17 11 11 28 12 12 14 13 12 20 14 12 24 15 12 28 16 13 26 17 13 34 18 13 34 19 13 46 20 14 26 21 14 36 22 14 60 23 14 80 24 15 20 25 15 26 26 15 54 27 16 32 28 16 40 29 17 32 30 17 40 31 17 50 32 18 42 33 18 56 34 18 76 35 18 84 36 19 36 37 19 46 38 19 68 39 20 32 40 20 48 41 20 52 42 20 56 43 20 64 44 22 66 45 23 54 46 24 70 47 24 92 48 24 93 49 24 120 50 25 85 cars[cars$speed &lt; 5,] # subsets under specific value speed dist 1 4 2 2 4 10 "],["charts-templates-r.html", "Chapter - 2 Charts templates - R", " Chapter - 2 Charts templates - R library(gridExtra) library(hrbrthemes) library(ggplot2) paletteDani &lt;- c( &quot;#ffa500&quot;, &quot;#DAF7A6&quot;, &quot;#5F7992&quot;, &quot;#69b3a2&quot;, &quot;#ffd561&quot;, &quot;#ee5c42&quot;, &quot;#C8A2C8&quot;, &quot;#5c3170&quot;, &quot;#990000&quot;, &quot;#C70039&quot;, &quot;#34495E&quot;, &quot;#909497&quot;) Several basic options: Pie chart data &lt;- ToothGrowth dani_theme &lt;- theme( axis.title.x = element_blank(), axis.title.y = element_blank(), panel.border = element_blank(), panel.grid = element_blank(), axis.ticks = element_blank(), axis.text.x=element_blank(), legend.title = element_text(face = &quot;bold&quot;), plot.title = element_text(hjust = 0.5, size = 12, face = &quot;bold&quot;) ) ggplot(data, aes(x=&quot;&quot;,y = dose, fill = supp)) + geom_bar(width = 1, stat = &quot;identity&quot;) + coord_polar(&quot;y&quot;, start = 0) + scale_fill_manual(&quot;Legendname:&quot;, values = paletteDani) + dani_theme + labs(title = &quot;Title&quot;, x = &quot;variablX&quot;, y = &quot;variableY&quot; ) Bar chart ggplot(data, aes(x = dose, y = supp)) + geom_bar(stat = &quot;identity&quot;, fill=&quot;#69b3a2&quot;, color=&quot;#e9ecef&quot;) + theme(legend.position=&quot;none&quot;) Histogram ggplot(data = data, aes(len) ) + geom_histogram(fill=&quot;#69b3a2&quot;, color=&quot;#e9ecef&quot;, alpha=0.9) + ggtitle(&quot;Title&quot;) + xlab(&quot;variablex&quot;) + ylab(&quot;variabley&quot;) + theme(plot.title = element_text(size = 11)) `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Boxplot ggplot(data=ToothGrowth, aes(x=&quot;&quot;, y=len, fill=&quot;&quot;)) + geom_boxplot(fill=&quot;#69b3a2&quot;, outlier.colour=&quot;red&quot;, outlier.shape=8, outlier.size=4) + theme_ipsum() + theme( legend.position=&quot;none&quot;, plot.title = element_text(size=12) ) + ggtitle(&quot;Title&quot;) + theme(plot.title = element_text(hjust = 0.5)) + xlab(&quot;&quot;)+ ylab(&quot;&quot;) Scatter plot ggplot(data= ToothGrowth, aes(y = len, x = dose)) + geom_point(size=2) + geom_smooth(method=&quot;lm&quot;, color=&quot;#69b3a2&quot;, fullrange=TRUE, formula = y ~ x) + theme(plot.title = element_text(hjust = 0.5)) + labs(title = &quot;Title&quot;, y = &quot;yname&quot;, x = &quot;xname&quot; ) Scatter plot with dummies ggplot(data = ToothGrowth, aes(y = len, x = supp, colour=factor(supp))) + geom_point(size=2) + geom_smooth(method=&quot;lm&quot;, fill = NA, fullrange=TRUE, formula = y ~ x) + theme(plot.title = element_text(hjust = 0.5)) + scale_colour_manual(name=&quot;Legendtitle&quot;, labels=c(&quot;value1&quot;, &quot;value2&quot;),values = c(&quot;#69b3a2&quot;, &quot;#F6726A&quot;))+ labs(title = &quot;Title&quot;, y = &quot;Yname&quot;, x = &quot;Xname&quot; ) Arrange charts next to each other on a page chart1 &lt;- ggplot(data=ToothGrowth, aes(x=&quot;&quot;, y=len, fill=&quot;&quot;)) + geom_boxplot(fill=&quot;#69b3a2&quot;, outlier.colour=&quot;red&quot;, outlier.shape=8, outlier.size=4) + theme_ipsum() + theme( legend.position=&quot;none&quot;, plot.title = element_text(size=12) ) + ggtitle(&quot;Title&quot;) + theme(plot.title = element_text(hjust = 0.5)) + xlab(&quot;&quot;)+ ylab(&quot;&quot;) chart2 &lt;- ggplot(data=ToothGrowth, aes(x=&quot;&quot;, y=len, fill=&quot;&quot;)) + geom_boxplot(fill=&quot;#69b3a2&quot;, outlier.colour=&quot;red&quot;, outlier.shape=8, outlier.size=4) + theme_ipsum() + theme( legend.position=&quot;none&quot;, plot.title = element_text(size=12) ) + ggtitle(&quot;Title&quot;) + theme(plot.title = element_text(hjust = 0.5)) + xlab(&quot;&quot;)+ ylab(&quot;&quot;) grid.arrange(chart1, chart2, nrow=1, widths=c(0.9,1)) Shows the amounts of missing values (NA) in a data set library(VIM) aggr(cars, numbers = TRUE, prop = c(TRUE, FALSE), cex.axis = 0.5) Density plots with semi-transparent fill ggplot(data = ToothGrowth, aes(x = len, fill = supp)) + geom_density(alpha=.3) + theme( plot.title = element_text(size=15) ) + ggtitle(&quot;Density plot&quot;) + theme(plot.title = element_text(hjust = 0.5, face= &quot;bold&quot;)) + xlab(&quot;&quot;)+ ylab(&quot;Density&quot;) Correlation matrix - pairs panel library(psych) pairs.panels(iris, method = &quot;pearson&quot;, hist.col = &quot;#00AFBB&quot;, density = TRUE, ellipses = TRUE ) Simple plots: model &lt;- lm(dist ~ speed, data = cars) plot(model) plot(model, 4) # cook distance library(car) avPlots(model) "],["probability.html", "Chapter - 3 Probability 3.1 Bayes Theorem 3.2 Discrete Probablity 3.3 Samples, estimation &amp; confidence intervals 3.4 Significance level 3.5 Non-Parametric testing", " Chapter - 3 Probability Class given by: Walter Garcia-Fontes library(prob) library(LaplacesDemon) # Bayes Theorem library(BSDA) #tsumtest library(actuar) out &lt;- c(&quot;Red&quot;, &quot;White&quot;, &quot;Black&quot;, &quot;Blue&quot;, &quot;Green&quot;) freq &lt;- c(1,2,3,4,5) s &lt;- probspace(out, probs = freq) print(s) x probs 1 Red 0.06666667 2 White 0.13333333 3 Black 0.20000000 4 Blue 0.26666667 5 Green 0.33333333 If you toss two fair coins, what is the probability of two heads? space &lt;- tosscoin(2, makespace = TRUE) p &lt;- Prob(space, toss1 == &quot;H&quot; &amp; toss2 == &quot;H&quot;) The probability is: 0.25 When two dice are thrown, what is the probability of a 3 followed by a 5? space &lt;- rolldie(2, makespace = TRUE) p &lt;- Prob(space, X1 == 3 &amp; (X2 == 5) ) The probability is: 0.03 Sampling from an urn with or without replacement. 3 balls and sample size of 2: sample1 &lt;- urnsamples(1:3, size = 2, replace = TRUE, ordered = TRUE) sample2 &lt;- urnsamples(1:3, size = 2, replace = FALSE, ordered = TRUE) sample3 &lt;- urnsamples(1:3, size = 2, replace = FALSE, ordered = FALSE) sample4 &lt;- urnsamples(1:3, size = 2, replace = TRUE, ordered = FALSE) 3.1 Bayes Theorem Unconditional probability: P(S) and P(NS) Success or no success prS &lt;- c(0.4, 0.6) Conditional probability: P(P | S ) and P( P | NS) Predicted given it is successful Predicted given it is not successful prNS &lt;- c(0.6, 0.2) Bayes prob, posterior probabilities P(S | P) &amp; P(NS | P) BayesTheorem(prS, prNS) [1] 0.6666667 0.3333333 attr(,&quot;class&quot;) [1] &quot;bayestheorem&quot; 3.2 Discrete Probablity 3.2.1 Uniform discrete probability distribution Sample space with a set probability. Size = amount of tries Density function: Individual probability. F.E. Getting a 4 Cumulative density: Uniform for a certain value distribution. F.E. 4 or less. 4 or more? 1-punif 3 Inverse cumulative density: Uniform for a certain probability ( up until a certain value). F.E. up to 25% of the tries Default = # or less. For # or more do: 1-probability of # or less 3.2.2 Binomial distribution Binomial for a specific value for a certain sample. F.E. 2 from the sample are successful. Binomial for a certain distribution of the sample. F.E. At most 2 in the sample are successful. Or 5 or more. Binomial for a certain percentage of the sample. F.E. 25% of the sample has x value or less. Difference between two binomial values. F.E. Prob there are between 4 and 5 of the trials successful. one &lt;- dbinom(x, size = n, prob = y) two &lt;- pbinom(x, size = n, prob = y) three &lt;- qbinom(p, size = n, prob = y) four &lt;- diff(pbinom(c(X,Y), size = n, prob = y)) Default = # or less (left area of the distribution). For # or more do: 1-probability of # or less 3.2.3 Poisson distribution Expected value = \\(n * p = LAMDA\\) Poisson for a certain value. Lambda = n*p. F.E. Prob of having a 5 Poisson for a certain value distribution. F.E. Prob of having less than 5. More than 5? = 1- Ppois(4, lambda) Poisson for a certain probability to capture a certain value. F.E. Poisson value for 25%. one &lt;- dpois(x,lambda) two &lt;- ppois(x,lambda) three &lt;- qpois(x,lambda) Default = # or less (left area of the distribution). For # or more do: 1-probability of # or less 3.2.4 The normal distribution Empirical rule For all normal distributions: 68-95-99.7 rule 99.7% of observations are located between: -3 mu and 3 95% of observations are located between: -2 mu 2 68% of observations are located between: -1 mu 1 Normal distribution Z-value # z &lt;- (x - mean) / sd. For example: (11 - 10) / 2 [1] 0.5 Normal distribution for a certain proportion. Pi = population proportion mean%. Normal distribution for a certain value distribution. F.E. Prob of value above 5. FALSE Prob less than 9. TRUE Normal distribution for a certain probability to capture a certain value. F.E. Value that is given at 25% point. Difference between two values on the normal distribution. F.E. between 5 and 10. one &lt;- pnorm(X, pi, sd, lower.tail = FALSE) two &lt;- pnorm(X, mu, sd, lower.tail = FALSE) three &lt;- qnorm(p, mu, sd, lower.tail = FALSE) four &lt;- diff(pnorm(c(X,Y), mu, sd, lower.tail = FALSE)) lower.tail = TRUE: The area of the left side of the slope lower.tail = FALSE: The area of the right side of the slope Confidence interval for normal distribution z.test(x, sd=sigma) binconf(x = x, n = n) &lt;- proportions t.test(variable) &lt;- t-distribution for conf.inv 3.2.4.1 Plotting the normal distribution \"With mean = 3 and standard deviation = 7 Limits: mean +/- 3 * standard deviation = 3*7 = 21 Lower limit = 3 – 21 = -18 Upper limit = 3 + 21 = 24\" Example: x &lt;- seq(15, 45, length=50) y &lt;- dnorm(x, 30, 5) plot(x,y,type=&quot;l&quot;,lwd=2,col=&quot;black&quot;) x &lt;- seq(15,35, length=100) y &lt;- dnorm(x, 30,5 ) polygon(c(15,x,35),c(0,y,0), density = c(15, 35), col = &quot;black&quot;) p &lt;- pnorm(35, mean = 30, sd = 5,lower.tail = TRUE) text(0,0.15,&quot;68%&quot;) 3.2.4.2 Binomial It will be possible to use the Normal distribution as an approximation to the Binomial if: n is large and p &gt; 0.1 Density function (individual probability). Cumulative density (between certain values). Difference between two binomial values Inverse cumulative density. For a certain prob. one &lt;- dbinom(x, mu, sd) two &lt;- pbinom(x, mu, sd, lower.tail = FALSE) three &lt;- diff(pbinom(c(X,Y), mu, sd, lower.tail = FALSE)) four &lt;- qbinom(p, mu, sd, lower.tail = FALSE) 3.3 Samples, estimation &amp; confidence intervals The standard error of the sampling distribution of the mean se &lt;- sigma / sqrt(n) Probability sample To find the probability that X is larger than mu To find the probability that X is smaller than mu p &lt;- pnorm(X, mu, se, lower.tail = TRUE) p &lt;- pnorm(X, mu, se, lower.tail = FALSE) Probability proportions sample sd &lt;- sqrt((pi*(n-pi))/n) z &lt;- (p - pi)/sd p &lt;- pnorm(X, pi, se, lower.tail =FALSE) Sample size Package = “samplingbook.” Provides the sample size needed to have a 95% confidence to estimate the population mean. Level = confidence level. Se is required standard error. sample.size.mean(se, sigma, level=0.95) 3.4 Significance level 3.4.1 Critical values Critical value for normal distribution, sample &gt; 30 Two-sided: Critical value, 5% significance level = 1.96 Two-sided: Critical value, 1% significance level = 2.58 Two-sided: Critical value, 10% significance level = 1.96 One-sided: Critical value, 5% significance level = 1.64 One-sided: Critical value, 1% significance level = 2.33 One-sided: Critical value, 10% significance level = 1.28 qnorm(0.975) [1] 1.959964 qnorm(0.995) [1] 2.575829 qnorm(0.95) [1] 1.644854 qnorm(0.95) [1] 1.644854 qnorm(0.99) [1] 2.326348 qnorm(0.90) [1] 1.281552 Critical values t-distribution One-sided: critical value at a 5% significance level One-sided: critical value at a 10% significance level One-sided: critical value at a 1% significance level Two-sided: critical value at a 5% significance level Two-sided: critical value at a 10% significance level Two-sided: critical value at a 1% significance level cv &lt;- qt(0.95, df) cv &lt;- qt(0.90, df) cv &lt;- qt(0.99, df) cv &lt;- qt(0.975, df) cv &lt;- qt(0.95, df) cv &lt;- qt(0.995, df) Confidence interval cv &lt;- cv mu &lt;- mu sd &lt;- sd se &lt;- sd / (sqrt(n)) n &lt;- n conf_int95 &lt;- cv * sd / (sqrt(n)) mu_plus &lt;- mu + conf_int95 mu_min &lt;- mu - conf_int95 Large sample significance testing Two-sided One-sided: X is greater than the population mean One-sided: X is less than the population mean library(BSDA) one &lt;- tsum.test(mean.x = X, s.x = sd, n.x = n, mu = mu, alternative = &quot;two.sided&quot;, var.equal = TRUE) two &lt;- tsum.test(mean.x = X, s.x = sd, n.x = n, mu = mu, alternative = &quot;greater&quot;, var.equal = TRUE) three &lt;- tsum.test(mean.x = X, s.x = sd, n.x = n, mu = mu, alternative = &quot;less&quot;, var.equal = TRUE) For proportions: prop.test(x = X, n = n, p = p, correct = TRUE, alternative = “two.sided”) Same goes for above: two.sided, greater, less 3.4.2 Test of equality - two samples H_0 &lt;- \\(\\mu1 = \\mu2\\) or \\((\\mu1 - \\mu2) = 0\\) H_a &lt;- \\(\\mu1 \\neq \\mu2\\) or \\(\\mu1 - \\mu2 \\neq 0\\) Difference in two means with a certain confidence level confidence interval. Default = 95% tsum.test(mean.x = X, s.x = sd, n.x = n, mean.y = X, s.y = sd, n.y = n, var.equal=FALSE) Welch Modified Two-Sample t-Test data: Summarized x and y t = 0, df = 58, p-value = 1 alternative hypothesis: true difference in means is not equal to 0 95 percent confidence interval: -0.8492568 0.8492568 sample estimates: mean of x mean of y 15 15 2-sample test for equality of proportions without continuity correction. prop.test(data, correct=FALSE, alternative=“greater”) 3.5 Non-Parametric testing 3.5.1 Contengency table / frequencies Obtain contingency table table(ToothGrowth$dose) 0.5 1 2 20 20 20 3.5.2 Chi-square Chi-square test Get the expected value Probability for chi-square data &lt;- matrix(c(27,373,33,567),byrow=TRUE,nrow=2) chisq.test(data,correct=FALSE) Pearson&#39;s Chi-squared test data: data X-squared = 0.66489, df = 1, p-value = 0.4148 chisq.test(data,correct=FALSE)$expected [,1] [,2] [1,] 24 376 [2,] 36 564 prop.table(chisq.test(data,correct=FALSE)$expected,1) [,1] [,2] [1,] 0.06 0.94 [2,] 0.06 0.94 prop.table(chisq.test(data,correct=FALSE)$expected,2) [,1] [,2] [1,] 0.4 0.4 [2,] 0.6 0.6 Degree of freedom = # of row - 1 * # of columns = fixed All expected frequencies must be above five! If not, categories must be combined! 3.5.3 Goodness of fit Uniform: Degree of freedom = number of categories - number of parameters - 1. x &lt;- c(1,2,3,4,5) p &lt;- rep(1/5, 5) chisq.test(x, p = p) Chi-squared test for given probabilities data: x X-squared = 3.3333, df = 4, p-value = 0.5037 All expected frequencies must be above five! If not, categories must be combined! Binomial: dbinom(x, size = n, prob = y) For example: library(actuar) cj &lt;- c(-0.5, 0.5, 1.5, 2.5, 3.5, 4.5, 5.5) #or cj &lt;- seq(from = -0.5, to=5, by=1) nj &lt;- c(15,20,20,18,13) data &lt;- grouped.data(Group = cj, Frequency = nj) p &lt;- mean(data)/5 pr &lt;-c(dbinom(0,5,p),dbinom(1,5,p),dbinom(2,5,p),dbinom(3,5,p),dbinom(4,5,p),dbinom(5,5,p)) nj2 &lt;- c(35,20,18,23) pr2 &lt;- c(dbinom(0,5,p)+dbinom(1,5,p),dbinom(2,5,p),dbinom(3,5,p),dbinom(4,5,p)+dbinom(5,5,p)) chisq.test(nj2,p=pr2) Chi-squared test for given probabilities data: nj2 X-squared = 38.736, df = 3, p-value = 0.00000001975 All expected frequencies must be above five! If not, categories must be combined! Poisson Degree of freedom = number of categories - number of parameters - 1. NOTE! Distribution goes to infinity. Counter for one value that is X or more. 1 - until X. Example: cj &lt;- c(-0.5, 0.5, 1.5, 2.5, 3.5, 4.5, 5.5) #or cj &lt;- seq(from = -0.5, to=6, by=1) nj &lt;- c(16, 30, 37, 7, 10, 5) data &lt;- grouped.data(Group = cj, Frequency = nj) m &lt;- mean(data) pr &lt;- c(dpois(0, m),dpois(1,m),dpois(2, m), dpois(3, m), dpois(4, m), + (1-ppois(4,m)) ) chisq.test(nj, p = pr) Chi-squared test for given probabilities data: nj X-squared = 9.7845, df = 5, p-value = 0.08158 Normal distribution Example: cv &lt;- qchisq(0.90, 2) cj &lt;- c(0, 1, 3, 10, 15, 30) nj &lt;- c(16, 30, 37, 7, 10) data &lt;- grouped.data(Group = cj, Frequency = nj) m &lt;- mean(data) s &lt;- sqrt(emm(data,2)) pr &lt;- c(pnorm(1,m,s), diff(pnorm(c(1,3),m,s)), diff(pnorm(c(3,10),m,s)), diff(pnorm(c(10,15),m,s)), 1 - pnorm(c(15),m,s) ) chisq.test(nj,p=pr) Chi-squared test for given probabilities data: nj X-squared = 77.503, df = 4, p-value = 0.0000000000000005887 ###Mann-whitney test N = Number of pairs - number of draws For small tests c1 values sample 1 c2 values sample 2 wilcox.text(x, c2) Larger sample test &gt; 10 You can use a approximation based on the normal distribution. Therefore critical values will be 1.96 for this two sided test. ###Wilcoxon test Two options - Do not predict direction –&gt; two sided - Predict direction –&gt; one sided wilcox.test(w1, w2, paired=TRUE,correct=FALSE) ###Run test library(randtests) pers &lt;- c(0,1,1,0,0,0,0,1,1,0,1) pers.f &lt;- factor(pers,labels=c(&quot;Male&quot;,&quot;Female&quot;)) runs.test(pers) Runs Test data: pers statistic = NaN, runs = 1, n1 = 5, n2 = 0, n = 5, p-value = NA alternative hypothesis: nonrandomness 3.5.4 P-value Find p value: Probability of getting this test statistic or more: pchisq(ts, df, lower.tail=FALSE) [1] 0.5578254 "],["simple-regressions.html", "Chapter - 4 Simple regressions 4.1 Basics regressions 4.2 Prediction 4.3 Data problems", " Chapter - 4 Simple regressions Class given by: Walter Garcia-Fontes 4.1 Basics regressions Regressions, correlation and dummy’s Y = Dependent X = Explanatory Correlation cor(x, y) [1] 0.8068949 Creating the regression: To plot the regression model Evaluates the coefficient of the model Only the first colum estimattion model &lt;- lm(y~x, data = data) summary(model)$coef Estimate Std. Error t value Pr(&gt;|t|) (Intercept) -17.579095 6.7584402 -2.601058 0.012318816153809090 x 3.932409 0.4155128 9.463990 0.000000000001489836 est &lt;- summary(model)$coef[,1] 4.1.1 Summarizing regressions: Using stargazer package library(stargazer) stargazer(lm(y~x, data=data), type=&quot;text&quot;) =============================================== Dependent variable: --------------------------- y ----------------------------------------------- x 3.932*** (0.416) Constant -17.579** (6.758) ----------------------------------------------- Observations 50 R2 0.651 Adjusted R2 0.644 Residual Std. Error 15.380 (df = 48) F Statistic 89.567*** (df = 1; 48) =============================================== Note: *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01 # Multiple models adjacent model1 &lt;- lm(y~x, data=data) model2 &lt;- lm(x~y, data=data) stargazer(model1, model2, type=&quot;text&quot;) ========================================================== Dependent variable: ---------------------------- y x (1) (2) ---------------------------------------------------------- x 3.932*** (0.416) y 0.166*** (0.017) Constant -17.579** 8.284*** (6.758) (0.874) ---------------------------------------------------------- Observations 50 50 R2 0.651 0.651 Adjusted R2 0.644 0.644 Residual Std. Error (df = 48) 15.380 3.156 F Statistic (df = 1; 48) 89.567*** 89.567*** ========================================================== Note: *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01 Using summary function: summary(lm(y~x)) Call: lm(formula = y ~ x) Residuals: Min 1Q Median 3Q Max -29.069 -9.525 -2.272 9.215 43.201 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) -17.5791 6.7584 -2.601 0.0123 * x 3.9324 0.4155 9.464 0.00000000000149 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 15.38 on 48 degrees of freedom Multiple R-squared: 0.6511, Adjusted R-squared: 0.6438 F-statistic: 89.57 on 1 and 48 DF, p-value: 0.00000000000149 Regressions Plotting regression plot(y~x,data=data, main=&quot;Title&quot;, ylab=&quot;yname&quot;, xlab=&quot;xname&quot; ) Including regression line: plot(y~x,data=data, main=&quot;Title&quot;, ylab=&quot;yname&quot;, xlab=&quot;xname&quot; ) abline(lm(y~x, data=data), col=&quot;blue&quot;) Confidence interval around slope confint(lm(y~x), level=0.95) 2.5 % 97.5 % (Intercept) -31.167850 -3.990340 x 3.096964 4.767853 Sub-sampling regression Specify dimensions [,]. First is row. Column, second. Selects the rows where age is larger than 5. Lower than 5. sub1 &lt;- summary(lm(y~x, data=data[&quot;speed&quot;&gt;=5,])) sub2 &lt;- summary(lm(y~x, data=data[&quot;speed&quot;&lt;=5,])) 4.1.2 Dummy variables, diff in means 4.1.3 Regression + dummy Y = Constant0 + B0 * X - Diff in means + B1 * variable1*2 Omitting the intercept: Shows the means separately and not the difference between means. Tests whether the expected counts are different from zero. lm(y ~ x - 1, data = data) Call: lm(formula = y ~ x - 1, data = data) Coefficients: x 2.909 Reorders group, to specific value to be first. variable2 &lt;- relevel(variable, “C”) 4.2 Prediction model &lt;- lm(y~x) newdata &lt;- data.frame(variablename = c(1:50)) pred &lt;- predict(model, newdata = newdata) Prediction confidence interval: One value Multiple values from a existing data frame pred1 &lt;- predict(model, data.frame(valuename = x), interval = &quot;confidence&quot;, level=0.95) pred2 &lt;- predict(model, newdata = newdata, interval = &quot;confidence&quot;, level=0.95) Prediction interval One value Multiple values from a existing data frame pred1 &lt;- predict(model, data.frame(valuename = x), interval=&quot;predict&quot;,level=0.95) pred2 &lt;- predict(model, newdata, interval=&quot;predict&quot;,level=0.95) 4.2.1 Confidence and prediction plotting Adds: observed values, fitted line, conf interval, predicted interval library(HH) fit &lt;- lm(y~x, data = data) ci.plot(fit) 4.2.2 Prediction with dummy variables Prediction = 𝛼1+𝛼2Constant Dummy+𝛽1𝑆𝑖𝑧𝑒+𝛽2Slope Dummy 4.2.3 Prediction intervals examples Prediction fit &lt;- lm(y ~ x + d + d, data = data) pred &lt;- predict(fit, data.frame(VariableX = c(10), Dummy = c(1), Slopedummy = c(10)) ) Confidence interval prediction fit &lt;- lm(y ~ x + d + d, data = data) pred &lt;- predict(fit, data.frame(VariableX = c(10), Dummy = c(1), Slopedummy = c(10), interval=&quot;confidence&quot;)) Prediction interval fit &lt;- lm(y ~ x + d + d, data = data) pred &lt;- predict(fit, data.frame(VariableX = c(10), Dummy = c(1), Slopedummy = c(10), interval=&quot;predict&quot;)) 4.3 Data problems Residual plot # residual.plots(fitted(fit), resid(fit), sigma.hat(fit), main=&quot;Title&quot;) Influential measure test im &lt;- influence.measures(fit) 4.3.1 Multicollinearity F-test fit &lt;- lm(y~x + d, data = data) anova(fit) Analysis of Variance Table Response: y Df Sum Sq Mean Sq F value Pr(&gt;F) x 1 21186 21185.5 89.567 0.00000000000149 *** Residuals 48 11354 236.5 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 4.3.2 Variance inflation factors The variance inflation factor (vif) is \\(1 / 1−R2\\). A simple approach to identify collinearity among explanatory variables is the use of variance inflation factors (VIF). It is the ratio of variance in a model with multiple terms, divided by the variance of a model with one term alone. The higher the value, the higher the collinearity. A VIF for a single explanatory variable is obtained using the r-squared value of the regression of that variable against all other explanatory variables: A VIF is calculated for each explanatory variable and those with high values are removed. The definition of ‘high’ is somewhat arbitrary, but values in the range of 5-10 are commonly used for ‘high.’ If VIF value exceeding 4.0, or by tolerance less than 0.2 then there is a problem with multicollinearity (Hair et al., 2010). However, it depends on the researcher’s criteria. The lower the vif the better, but you shouldn’t be too concerned as long as your VIF is not greater than 10. vif(fit) x d 1 NaN 4.3.3 ANOVA One-way: one value res.aov &lt;- aov(y ~ x, data = data) summary(res.aov) Df Sum Sq Mean Sq F value Pr(&gt;F) x 1 21185 21185 89.57 0.00000000000149 *** Residuals 48 11354 237 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Two-way: more than two factors res.aov &lt;- aov(y ~ x + d, data = data) summary(res.aov) Df Sum Sq Mean Sq F value Pr(&gt;F) x 1 21185 21185 89.57 0.00000000000149 *** Residuals 48 11354 237 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 With interaction res.aov &lt;- aov(y ~ x * d, data = data) summary(res.aov) Df Sum Sq Mean Sq F value Pr(&gt;F) x 1 21185 21185 89.57 0.00000000000149 *** Residuals 48 11354 237 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Three-way Three way With interaction summary(aov(y ~ x + d, data=data)) Df Sum Sq Mean Sq F value Pr(&gt;F) x 1 21185 21185 89.57 0.00000000000149 *** Residuals 48 11354 237 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 summary(aov(y ~ x + d, data=data)) Df Sum Sq Mean Sq F value Pr(&gt;F) x 1 21185 21185 89.57 0.00000000000149 *** Residuals 48 11354 237 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 MANOVA: Multiple factors Test in difference Test separately test_manova &lt;- manova(cbind(y, d) ~ x, data = data) summary(test_manova) summary.aov(test_manova) 4.3.4 Linearizing variables logged &lt;- log(iris$Sepal.Length) # non-linear quad &lt;- cars$speed ^ 2 # quadratic "],["structure-equation-models.html", "Chapter - 5 Structure equation models 5.1 Path analysis (structural equations) 5.2 Coding the model 5.3 Factor model", " Chapter - 5 Structure equation models Class given by: Albert Satorra Structural Equation Modeling (SEM): is an extremely broad and flexible framework for data analysis, perhaps better thought of as a family of related methods rather than as a single technique. Measuring latent constructs is challenging and we must also incorporate estimates of measurement error into our models. SEM excels at both of these tasks. SEM is especially suited for causal analysis.(Gray 2017) VS Ordinary least-squares (OLS): models assume that the analyst is fitting a model of a relationship between one or more explanatory variables and a continuous or at least interval outcome variable that minimizes the sum of square errors, where an error is the difference between the actual and the predicted value of the outcome variable. The most common analytic method that utilizes OLS models is linear regression (with a single or multiple predictor variables). Latent variables = Unobserved variables (or unmeasured variables in SEM lingo). These are theoretical concepts which can be inferred but not directly measured. Linear regression model = \\(Y = alpha + betaX + error\\) The model has to account for randomization. If there is a factor that that cannot be explained, it is included in the error. When this unmeasured factor that is in the error is correlated to another independent variable, there is endogeneity. Endogeneity variables = correlated with the error terms. Arises when the marginal distribution of the independent variable is not independent of the conditional distribution of the dependent variable given the independent. Exogenous variables = not driven by other factors (observable or observable) Sources of endogeneity = 1. Omitted variables: relevant variables left out of the model, attributing to effect to those that were included. 2. Simultaneity: where x causes y and y causes x 3. Selection bias: sampling bias In the structural equation model we can effectively avoid endogeneity. 5.1 Path analysis (structural equations) Path diagrams = Communicates the structure of our model. Useful for structural equation models. The objects in the model mean: Rectangular = any variable that we can observe directly (observed variable), measured variables Circle / Ovals = Cannot be observed (Latent variable) Arrow = directed effect. One variable impacts the other. Hypothetical causal relationship. Numbers by the arrows = regression coefficient. Triangle is the constant in the Linear Model. Double handed arrows = indicate co-variances or correlations without a causal interpretation. Double handed arrow between two independent variables = they are correlated to each other. Residual error term = measurement errors. We expect that the factor will not perfectly predict the observed variables. The bi-directed arrows, the ones with two side arrows (in this representation, a connecting line with no arrows) represent co-variances among variables. 5.2 Coding the model library(semPlot) library(lavaan) Setting up the model and summarizing fit &lt;- &#39;dist ~ speed&#39; model &lt;- lavaan(fit, data = cars, estimator=&quot;MLM&quot;, auto.var = TRUE) summary(model) lavaan 0.6-8 ended normally after 15 iterations Estimator ML Optimization method NLMINB Number of model parameters 2 Number of observations 50 Model Test User Model: Standard Robust Test Statistic 0.000 0.000 Degrees of freedom 0 0 Parameter Estimates: Standard errors Robust.sem Information Expected Information saturated (h1) model Structured Regressions: Estimate Std.Err z-value P(&gt;|z|) dist ~ speed 3.932 0.399 9.864 0.000 Variances: Estimate Std.Err z-value P(&gt;|z|) .dist 227.070 54.619 4.157 0.000 Estimator = MLM in the model: protects for non-linearity, non-normality and elasticity of the raw data. This needs the raw data, not only the covariance matrix. Setting up the path diagram semPaths(model, &quot;std&quot;, title = FALSE, weighted = FALSE, sizeInt = 4, sizeMan = 5, edge.label.cex = 1.3, asize = 2) title(&quot;SEM path&quot;, line = 3) “std” = standardizing the variables “est” = true estimation style=“ram” = to get circles around the measurement arrows With latent variable &amp; SEM model &lt;- &quot; # regression Petal =~ Petal.Width + Petal.Length Sepal.Length ~ Sepal.Width + Petal &quot; fit &lt;- sem(model, data = iris, sample.cov = S, sample.nobs = 122) semPaths(fit, &quot;std&quot;, sizeInt = 4, sizeMan = 3, edge.label.cex = 1, asize=3, weighted=TRUE, exoCov = TRUE) Testing the model fitMeasures(fit)[ c(&quot;chisq&quot;,&quot;df&quot;, &quot;pvalue&quot; ,&quot;rmsea&quot;)] chisq df pvalue rmsea 35.94875570001079 2.00000000000000 0.00000001562525 0.33639637185920 Chisq is a chi-squared test statistic for. H0: moment restrictions implied by the model hold true The degrees of freedom P-value of the chi-square test Rmsea is a fit index, the root mean square of approximation. It should be small for a good fit of the model. Threshold of .05 is often applied to declare good fit. See later chapter for further explanation Modification indices This tells us how we could improve your model. If i liberate one parameter, then _ would change. modi = modindices(fit) modi[order(modi[,4], decreasing=T), ] lhs op rhs mi epc sepc.lv sepc.all sepc.nox 13 Sepal.Width ~ Sepal.Length 30.061 -0.229 -0.229 -0.499 -0.499 14 Sepal.Width ~ Petal 30.061 -0.263 -0.189 -0.435 -0.435 16 Petal ~ Sepal.Width 30.061 -0.719 -1.002 -0.435 -1.002 15 Petal ~ Sepal.Length 30.061 -1.104 -1.539 -1.456 -1.456 Parameters fit parameterestimates(fit, standardized = TRUE, rsquare=TRUE, ci=FALSE)[1:4,] # showing the top 4 lhs op rhs est se z pvalue std.lv std.all std.nox 1 Petal =~ Petal.Width 1.000 0.000 NA NA 0.717 0.944 0.944 2 Petal =~ Petal.Length 2.500 0.065 38.745 0 1.794 1.020 1.020 3 Sepal.Length ~ Sepal.Width 0.651 0.058 11.158 0 0.651 0.299 0.688 4 Sepal.Length ~ Petal 1.149 0.052 22.015 0 0.824 0.871 0.871 + 5.2.1 Covariance Covariance is a measure of how much two random variables vary together. It’s similar to variance, but where variance tells you how a single variable varies, co variance tells you how two variables vary together. Cov(1, 2) = \\(sum (1- mean(1) * (2-mean(2) / n\\) Moment matrix: Covariance matrix: cov(cars) speed dist speed 27.95918 109.9469 dist 109.94694 664.0608 Creating the covariance matrix when only having the lower half: lower &lt;- &quot; 0.03300863 0.15894229 5.0185561 0.15670560 0.9841531 1.2142232 &quot; S &lt;- getCov(lower, names = c(&quot;variable1&quot;, &quot;variable2&quot;, &quot;variable3&quot;)) print(S) variable1 variable2 variable3 variable1 0.03300863 0.1589423 0.1567056 variable2 0.15894229 5.0185561 0.9841531 variable3 0.15670560 0.9841531 1.2142232 5.2.2 Reliability In statistics reliability is the consistency of a set of measurements or measuring instrument, often used to describe a test. This can either be whether the measurements of the same instrument give or are likely to give the same measurement (test-retest), or in the case of more subjective instruments, such as personality or trait inventories, whether two independent assessors give similar scores (inter-rater-reliability). Reliability is inversely related to random error. In words, reliability is defined as a proportion of observed variance that is true variance. Reliability is interpreted as a proportion and therefore cannot be negative. Various kinds of reliability coefficients, with values ranging between 0.00 (much error) and 1.00 (no error), are usually used to indicate the amount of error in the scores. The reliability is expressed as k and there are several options to calculate: Option 1: reliability of a latent variable 1 - Measurement error variable 1 / observed variable 2 (latent variable). Dividing the true variance by the observed variance. Here is an example: Option 2: based on the standardized path diagram The square of the standardized loading is the reliability of the variable. Example: Here the reliability of the variable distance (dst) = \\(0.35 ^ 2 = 0.12 = k\\) ____________________________________________________ Testing the model fitMeasures(fit) [ c(\"chisq\", \"df\", pvalue\", \"rmsea\")] Model chi-square test = We test whether the fitted model is correct. HO: moment restrictions implied by the model hold. The fit is correct If &gt; 0.05 we cannot reject the model. I accept the model. Therefore, the chi-square test allows researchers to evaluate the fitness of a model by using the null hypothesis significance test approach. Degrees of freedom (df) = Number of observations available for model estimation - Number of observations used to estimate parameters. “Number of free parameters” refers to all of the things that this model estimated freely. Parameter is a regression coefficient when standardized is called a beta coefficient. The Root Mean Square Error of Approximation (RMSEA) = fit index: how the covariance fit in the model. Difference between observed and the fitted. The RMSEA is widely used in Structural Equation Modeling to provide a mechanism for adjusting for sample size where chi-square statistics are used. Measures the discrepancy due to the approximation per degree of freedom. The objective is to have the RMSEA as low as possible. 5.3 Factor model Factor analysis: a statistical method used to describe variability among observed, correlated variable in terms of a potentially lower number of unobserved variables called factors. For example, it is possible that variations in six observed variables mainly reflect the variations in two unobserved (underlying) variables. Factor analysis searches for such joint variations in response to unobserved latent variables. The observed variables are modeled as linear combinations of the potential factors, plus “error” terms. It is including a common return that has a impact to multiple variables. Then, you can find how much variance is due to permanent and depended on these variables. Below we can find the factor model including means based on the Spearman model from 1903. Difference in regression equations: Basic model - Y, X are centered : \\(Y = beta X + error\\) Y, X NOT centered - including constant: \\(Y = alpha*1 + beta X + error\\) Factor model: \\(Y = lambda*F + error1\\) Lamba = the weights F = common factor Error = Specific to the factors Here instead of setting the alpha to 1 which we do in the basic model, now we add a weight which is the lambda. When the data is standardized, beta becomes the standardized beta coefficient. Because y, x are centered, you do not need to present the constant in the model. Next step in factor models with simultaneous equations –&gt; ML estimation of a general model. Linear structural relations You have to fix the variance of a variable that you do not observe. If you do not do this, the model is not identified. Meaning there is no minimum. Including latent variables two options: A) Var(F) = 1, F Standardized (F typically has mean zero) B) Lambda_1 = 1 They are equivalent regarding degrees of freedom or model fit. When you put 1* variable1, you force the true beta to be 1. In r setting the model = #regression equation option 1 Model &lt;- &quot;dist =~ 1 * speed &quot; #regression equation option 2 Model &lt;- &quot;dist =~ NA * speed &quot; When using: Default NA = not available. We are asking R to calculate the beta instead of setting it to 1. This gives the equation: \\(Y = Lambda*F + Error\\). Degrees of freedom When you bring variables into the model, the degrees of freedom increases. When there are 0 degrees of freedom, you cannot test but you can fit the model. Now you cannot reject the model, but you can check the reliability. 5.3.1 Setting covariance &amp; variances Independent variables have variances and co-variances, unless the model specification puts them to zero. Variance is to be estimated in the model and if necessary we should be imposing restrictions. When you include all variances, you put a lot of tension on your model. Y ~~ 1* Y = forces the variable to be standardized X ~~ X = Gives two explanatory variables the possibility to covariance Equality with error variances (multiple independent variables) X~~X *A = setting a restriction. Auto regressive = one variables keeps impacting the following. For example: 2011 impacts 2012 which impacts 2013 etc. In this case, the variables need to be set to allow for correlation. [1] &quot;11-references.Rmd&quot; &quot;.Rhistory&quot; [3] &quot;rsconnect&quot; &quot;.DS_Store&quot; [5] &quot;R-Summaries.Rproj&quot; &quot;01-Basics.Rmd&quot; [7] &quot;preamble.tex&quot; &quot;index.Rmd&quot; [9] &quot;images&quot; &quot;02-Charts.Rmd&quot; [11] &quot;Summary_files&quot; &quot;09-MarketingAnalytics.Rmd&quot; [13] &quot;MIT-Coding-Brain-01-press_0.jpeg&quot; &quot;packages.bib&quot; [15] &quot;03-Probability.Rmd&quot; &quot;_output.yml&quot; [17] &quot;docs&quot; &quot;R-Summaries.Rmd&quot; [19] &quot;07-PracticalDataScience.Rmd&quot; &quot;www&quot; [21] &quot;04-Simpleregressions.Rmd&quot; &quot;_bookdown_files&quot; [23] &quot;README.md&quot; &quot;R-Summaries_files&quot; [25] &quot;05-SEM.Rmd&quot; &quot;_bookdown.yml&quot; [27] &quot;06-Python.Rmd&quot; &quot;.gitignore&quot; [29] &quot;.RData&quot; &quot;style.css&quot; [31] &quot;_book&quot; &quot;book.bib&quot; [33] &quot;view3D.png&quot; &quot;.git&quot; [35] &quot;08-AdvancedStats.Rmd&quot; &quot;.Rproj.user&quot; # This Python file uses the following encoding: utf-8 import os, sys import numpy as np import pandas as pd import matplotlib.pyplot as plt from matplotlib.pyplot import figure import seaborn as sns import os "],["basics-python.html", "Chapter - 6 Basics Python 6.1 Data set 6.2 Matrixes 6.3 Filtering a data set 6.4 Data imputation 6.5 Data visualization 6.6 Model selection", " Chapter - 6 Basics Python Class given by: Alberto Santini y = 5 + 5 y [1] 10 Printing characters print(&#39;Hello, readers!&#39;) [1] &quot;Hello, readers!&quot; Printing numbers print(15) [1] 15 Printing length of a value Length = len(&#39;Danielle&#39;) print(Length) 8 6.1 Data set Loading the data set &amp; viewing head + tails: sns.set_context(&#39;paper&#39;) tips = sns.load_dataset(&#39;tips&#39;) tips.head() total_bill tip sex smoker day time size 0 16.99 1.01 Female No Sun Dinner 2 1 10.34 1.66 Male No Sun Dinner 3 2 21.01 3.50 Male No Sun Dinner 3 3 23.68 3.31 Male No Sun Dinner 2 4 24.59 3.61 Female No Sun Dinner 4 tips.tail() total_bill tip sex smoker day time size 239 29.03 5.92 Male No Sat Dinner 3 240 27.18 2.00 Female Yes Sat Dinner 2 241 22.67 2.00 Male Yes Sat Dinner 2 242 17.82 1.75 Male No Sat Dinner 2 243 18.78 3.00 Female No Thur Dinner 2 Length of the variable Shape: Number of rows and number of columns Type of variables + basic info Descriptive statistics variable len(tips) 244 tips.shape (244, 7) tips.info() &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 244 entries, 0 to 243 Data columns (total 7 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 total_bill 244 non-null float64 1 tip 244 non-null float64 2 sex 244 non-null category 3 smoker 244 non-null category 4 day 244 non-null category 5 time 244 non-null category 6 size 244 non-null int64 dtypes: category(4), float64(2), int64(1) memory usage: 7.3 KB tips.describe() total_bill tip size count 244.000000 244.000000 244.000000 mean 19.785943 2.998279 2.569672 std 8.902412 1.383638 0.951100 min 3.070000 1.000000 1.000000 25% 13.347500 2.000000 2.000000 50% 17.795000 2.900000 2.000000 75% 24.127500 3.562500 3.000000 max 50.810000 10.000000 6.000000 6.2 Matrixes Series: Panda series method: Serie1 = pd.Series( [4200, 8000, 6500], index=[&quot;Amsterdam&quot;, &quot;Toronto&quot;, &quot;Tokyo&quot;] ) Serie1 Amsterdam 4200 Toronto 8000 Tokyo 6500 dtype: int64 Python dictionary method: Serie2 = pd.Series({&quot;Amsterdam&quot;: 5, &quot;Tokyo&quot;: 8}) Serie2 Amsterdam 5 Tokyo 8 dtype: int64 Data frame: Combined_serie = pd.DataFrame({ &quot;Revenue&quot;: Serie1, &quot;Employee_count&quot;: Serie2 }) Combined_serie Revenue Employee_count Amsterdam 4200 5.0 Tokyo 6500 8.0 Toronto 8000 NaN Sub-setting by row: Combined_serie[&quot;Tokyo&quot;:] Revenue Employee_count Tokyo 6500 8.0 Toronto 8000 NaN Creating our own functions: Saying hello + name def printing_name(name): print(&#39;Good morning,&#39;, name) printing_name(&#39;Danielle&#39;) Good morning, Danielle Multiple arguments: Saying hello + name + location def welcome(name, location): print(&quot;Good morning&quot;, name, &quot;Welcome to&quot;, location) welcome(&quot;Danielle,&quot;, &quot;class.&quot;) Good morning Danielle, Welcome to class. Bar plot sns.set_context(&#39;paper&#39;) tips = sns.load_dataset(&quot;tips&quot;) tips.head() total_bill tip sex smoker day time size 0 16.99 1.01 Female No Sun Dinner 2 1 10.34 1.66 Male No Sun Dinner 3 2 21.01 3.50 Male No Sun Dinner 3 3 23.68 3.31 Male No Sun Dinner 2 4 24.59 3.61 Female No Sun Dinner 4 sns.barplot(x = &#39;day&#39;, y = &#39;total_bill&#39;, hue = &#39;sex&#39;, data = tips, palette = &#39;Blues&#39;, edgecolor = &#39;w&#39;) plt.show() tips.groupby([&#39;day&#39;,&#39;sex&#39;]).mean() total_bill tip size day sex Thur Male 18.714667 2.980333 2.433333 Female 16.715312 2.575625 2.468750 Fri Male 19.857000 2.693000 2.100000 Female 14.145556 2.781111 2.111111 Sat Male 20.802542 3.083898 2.644068 Female 19.680357 2.801786 2.250000 Sun Male 21.887241 3.220345 2.810345 Female 19.872222 3.367222 2.944444 1-hot-encoding: Transforming categorical features to values from 0 or 1. F.e. You can be from origin: America or Europe. If the observation is from America it receives a 1 for America and a 0 for Europe. Reading from a csv file with pandas: import pandas as pd d = pd.read_csv(&#39;~/Documents/Pompeu Fabra BSM/Practical data science/auto-mpg.csv&#39;) d.mpg.mean().round() # rounding by amount of decimals 23.0 d.dtypes # describes the type of variables mpg float64 cylinders int64 displacement float64 hp float64 weight float64 acceleration float64 year int64 origin int64 dtype: object pd.to_numeric(d.hp) # transforms to numerical 0 130.0 1 165.0 2 150.0 3 150.0 4 140.0 ... 387 86.0 388 52.0 389 84.0 390 79.0 391 82.0 Name: hp, Length: 392, dtype: float64 len(d) # amount of rows 392 d.shape # amount of rows &amp; columns (392, 8) d.columns # gives the names of the columns Index([&#39;mpg&#39;, &#39;cylinders&#39;, &#39;displacement&#39;, &#39;hp&#39;, &#39;weight&#39;, &#39;acceleration&#39;, &#39;year&#39;, &#39;origin&#39;], dtype=&#39;object&#39;) mpg = d.mpg # shows me only the values from one variable mpg = d[&#39;mpg&#39;] # alternative way to show only one value d.head() # shows the first 5 observations of a data set mpg cylinders displacement hp weight acceleration year origin 0 18.0 8 307.0 130.0 3504.0 12.0 70 1 1 15.0 8 350.0 165.0 3693.0 11.5 70 1 2 18.0 8 318.0 150.0 3436.0 11.0 70 1 3 16.0 8 304.0 150.0 3433.0 12.0 70 1 4 17.0 8 302.0 140.0 3449.0 10.5 70 1 d.tail() # shows the last 5 observations of a data set mpg cylinders displacement hp weight acceleration year origin 387 27.0 4 140.0 86.0 2790.0 15.6 82 1 388 44.0 4 97.0 52.0 2130.0 24.6 82 2 389 32.0 4 135.0 84.0 2295.0 11.6 82 1 390 28.0 4 120.0 79.0 2625.0 18.6 82 1 391 31.0 4 119.0 82.0 2720.0 19.4 82 1 Basic statistics: d.mpg.mean() 23.445918367346938 d.mpg.median() 22.75 d.mpg.max() 46.6 d.mpg.min() 9.0 d.mean() # mean for all columns mpg 23.445918 cylinders 5.471939 displacement 194.411990 hp 104.469388 weight 2977.584184 acceleration 15.541327 year 75.979592 origin 1.576531 dtype: float64 d.describe().round(2).head() # statistics for the whole data set, rounded to two decimals mpg cylinders displacement ... acceleration year origin count 392.00 392.00 392.00 ... 392.00 392.00 392.00 mean 23.45 5.47 194.41 ... 15.54 75.98 1.58 std 7.81 1.71 104.64 ... 2.76 3.68 0.81 min 9.00 3.00 68.00 ... 8.00 70.00 1.00 25% 17.00 4.00 105.00 ... 13.78 73.00 1.00 [5 rows x 8 columns] d[[&#39;year&#39;]] # defines a list of one variable year 0 70 1 70 2 70 3 70 4 70 .. ... 387 82 388 82 389 82 390 82 391 82 [392 rows x 1 columns] d[[&#39;year&#39;, &#39;cylinders&#39;]] # defines a list of multiple variables year cylinders 0 70 8 1 70 8 2 70 8 3 70 8 4 70 8 .. ... ... 387 82 4 388 82 4 389 82 4 390 82 4 391 82 4 [392 rows x 2 columns] d.year.unique() # gives me the unique values of that column, not the repetitions array([70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82]) 6.3 Filtering a data set d77 = d[d.year == 77] #filters the data set to all observations that are equal to a certain value of a variable d77 = d[d.year != 77] #filters the data set to all observations that are NOT equal to a certain value of a variable d77 = d[d.year &lt;= 77] # filters those that are smaller than a value d77 = d[d.year &gt;= 77] # filters those that are large than a value d77 = d[(d.year &gt;= 77) &amp; (d.year &lt;= 90)] # in between certain values d77 = d[(d.year == 80) | (d.year == 90)] # those from one value OR another d77 = d[~(d.year == 70)] # excludes the values Aggregating dm = d.groupby(&#39;year&#39;).mean() dm = d.groupby([&#39;year&#39;, &#39;cylinders&#39;]).mean() dm = d.groupby([&#39;year&#39;, &#39;cylinders&#39;]).median() dm = d.groupby([&#39;year&#39;, &#39;cylinders&#39;]).mean()[&#39;mpg&#39;] # for selected variables only Pivot table d.pivot_table(index=&#39;year&#39;, columns=&#39;cylinders&#39;, values=&#39;mpg&#39;).round() cylinders 3 4 5 6 8 year 70 NaN 25.0 NaN 20.0 14.0 71 NaN 28.0 NaN 18.0 13.0 72 19.0 23.0 NaN NaN 14.0 73 18.0 23.0 NaN 19.0 13.0 74 NaN 28.0 NaN 17.0 14.0 75 NaN 25.0 NaN 18.0 16.0 76 NaN 27.0 NaN 20.0 15.0 77 22.0 29.0 NaN 20.0 16.0 78 NaN 30.0 20.0 19.0 19.0 79 NaN 32.0 25.0 23.0 19.0 80 24.0 35.0 36.0 26.0 NaN 81 NaN 33.0 NaN 23.0 27.0 82 NaN 32.0 NaN 28.0 NaN d.pivot_table(index=&#39;year&#39;, columns=&#39;cylinders&#39;, values=&#39;mpg&#39;).round().fillna(&#39;&#39;) cylinders 3 4 5 6 8 year 70 25.0 20 14 71 28.0 18 13 72 19 23.0 14 73 18 23.0 19 13 74 28.0 17 14 75 25.0 18 16 76 27.0 20 15 77 22 29.0 20 16 78 30.0 20 19 19 79 32.0 25 23 19 80 24 35.0 36 26 81 33.0 23 27 82 32.0 28 Creating new columns starting from existing columns d[&#39;nam_of_new_column&#39;] = d.mpg * 2 Through the package numpy: import numpy as np d[&#39;sqrt_of_mpg_2&#39;] = np.sqrt(d.mpg) d[&#39;log(10)_of_mpg&#39;] = np.log10(d.mpg) Dropping / deleting columns d[&#39;double_mpg&#39;] = d.mpg * 2 del(d[&#39;double_mpg&#39;]) # deleting columns d.drop(columns=[&#39;sqrt_of_mpg_2&#39;, &#39;log(10)_of_mpg&#39;, &#39;nam_of_new_column&#39;]).head() # dropping columns mpg cylinders displacement hp weight acceleration year origin 0 18.0 8 307.0 130.0 3504.0 12.0 70 1 1 15.0 8 350.0 165.0 3693.0 11.5 70 1 2 18.0 8 318.0 150.0 3436.0 11.0 70 1 3 16.0 8 304.0 150.0 3433.0 12.0 70 1 4 17.0 8 302.0 140.0 3449.0 10.5 70 1 6.4 Data imputation pd.to_numeric(d.hp, errors=&#39;coerce&#39;).head() 0 130.0 1 165.0 2 150.0 3 150.0 4 140.0 Name: hp, dtype: float64 d.hp = pd.to_numeric(d.hp, errors=&#39;coerce&#39;) d[d.hp.isna()] # transform values to NA Empty DataFrame Columns: [mpg, cylinders, displacement, hp, weight, acceleration, year, origin, nam_of_new_column, sqrt_of_mpg_2, log(10)_of_mpg] Index: [] d[-(d.hp.isna())] # delete missing values mpg cylinders ... sqrt_of_mpg_2 log(10)_of_mpg 0 18.0 8 ... 4.242641 1.255273 1 15.0 8 ... 3.872983 1.176091 2 18.0 8 ... 4.242641 1.255273 3 16.0 8 ... 4.000000 1.204120 4 17.0 8 ... 4.123106 1.230449 .. ... ... ... ... ... 387 27.0 4 ... 5.196152 1.431364 388 44.0 4 ... 6.633250 1.643453 389 32.0 4 ... 5.656854 1.505150 390 28.0 4 ... 5.291503 1.447158 391 31.0 4 ... 5.567764 1.491362 [392 rows x 11 columns] 6.5 Data visualization Getting the correlation matrix Correlation matrix d.corr().round(decimals=2) mpg cylinders ... sqrt_of_mpg_2 log(10)_of_mpg mpg 1.00 -0.78 ... 1.00 0.98 cylinders -0.78 1.00 ... -0.81 -0.83 displacement -0.81 0.95 ... -0.83 -0.85 hp -0.78 0.84 ... -0.81 -0.83 weight -0.83 0.90 ... -0.86 -0.88 acceleration 0.42 -0.50 ... 0.44 0.45 year 0.58 -0.35 ... 0.58 0.58 origin 0.57 -0.57 ... 0.57 0.56 nam_of_new_column 1.00 -0.78 ... 1.00 0.98 sqrt_of_mpg_2 1.00 -0.81 ... 1.00 1.00 log(10)_of_mpg 0.98 -0.83 ... 1.00 1.00 [11 rows x 11 columns] Pair plot #sns.pairplot(d); #sns.pairplot(d, hue=&#39;origin&#39;); # with color tips = sns.load_dataset(&quot;tips&quot;) sns.set_theme(style=&quot;whitegrid&quot;) sns.boxplot(data=tips, x=&#39;sex&#39;, y=&#39;tip&#39;) plt.show() sns.boxplot(data=tips, x=&#39;sex&#39;, y=&#39;tip&#39;, color=&#39;black&#39;, boxprops=dict(alpha=.6)) plt.show() sns.boxplot(x=&quot;day&quot;, y=&quot;total_bill&quot;, hue=&quot;smoker&quot;, data=tips, palette=&quot;Set3&quot;) plt.show() tips[&#39;weekend&#39;] = tips.day.isin([&#39;Sat&#39;, &#39;Sun&#39;]) sns.boxplot(data=tips, x=&#39;day&#39;, y=&#39;tip&#39;, hue=&#39;weekend&#39;, dodge=False) plt.show() sns.catplot(data=tips, x=&#39;smoker&#39;, y=&#39;tip&#39;, hue=&#39;sex&#39;, col=&#39;time&#39;, kind=&#39;box&#39;); plt.show() 6.5.1 Other categorical plots sns.stripplot(data=tips, x=&#39;sex&#39;, y=&#39;tip&#39;); plt.show() sns.swarmplot(data=tips, x=&#39;sex&#39;, y=&#39;tip&#39;); plt.show() sns.swarmplot(data=tips, x=&#39;sex&#39;, y=&#39;tip&#39;, hue=&#39;smoker&#39;); plt.show() sns.swarmplot(data=tips, x=&#39;sex&#39;, y=&#39;tip&#39;, hue=&#39;smoker&#39;, dodge=True); plt.show() sns.boxplot(data=tips, x=&#39;sex&#39;, y=&#39;tip&#39;, fliersize=8) sns.swarmplot(data=tips, x=&#39;sex&#39;, y=&#39;tip&#39;, color=&#39;black&#39;, alpha=0.6); plt.show() sns.boxenplot(data=tips, x=&#39;sex&#39;, y=&#39;tip&#39;); plt.show() sns.boxenplot(data=tips, x=&#39;sex&#39;, y=&#39;tip&#39;, hue=&#39;smoker&#39;); plt.show() sns.displot(data=tips, x=&#39;tip&#39;); plt.show() sns.displot(data=tips, x=&#39;tip&#39;, kind=&#39;kde&#39;); plt.show() Multiple plots fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12,6)) sns.boxenplot(data=tips, x=&#39;sex&#39;, y=&#39;tip&#39;, ax=axes.flat[0]); sns.violinplot(data=tips, x=&#39;sex&#39;, y=&#39;tip&#39;, ax=axes.flat[1]); plt.show() Multidimensional KDE for numeric variables plt.rcParams[&#39;figure.figsize&#39;] = (10,8) sns.kdeplot(data=tips, x=&#39;tip&#39;, y=&#39;total_bill&#39;, shade=True, cbar=True); plt.show() sns.histplot(d.mpg); # Histogram plt.show() sns.histplot(d.mpg, bins=20); # with binwidth plt.show() sns.histplot(data=d, x=&#39;mpg&#39;, bins=20); sns.histplot(data=d, x=&#39;mpg&#39;, bins=20, cumulative=True); # cumulative values #d.origin = pd.Categorical(d.origin.replace({1: &#39;america&#39;, 2:&#39;europe&#39; 3: &#39;japan&#39;})) sns.relplot(data=d, x=&#39;hp&#39;, y=&#39;mpg&#39;, hue=&#39;origin&#39;, palette=&#39;tab10&#39;) &lt;seaborn.axisgrid.FacetGrid object at 0x7fca3af8bb70&gt; plt.show() corr = d.corr() sns.heatmap(corr) # simple plt.show() sns.heatmap(corr, cmap=plt.cm.RdYlGn, annot=True, linewidths=1, square=True, vmin=-1, vmax=1) # designed plt.show() 6.5.2 Preparing the data from sklearn.model_selection import train_test_split Creates the input points and the output columns label = &#39;mpg&#39; features = [column for column in d.columns if column != label] Setting the train / test split X = d[features] y = d[label] X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25) len(X) # Size of the dataset 392 len(X_train) # Size of the training set 294 len(X_test) # Size of the test set (In this case 25% of the dataset) 98 There is also a way to avoid the randomness of the test selection: X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0) # We fix the random state to a certain value. 6.5.3 Creating the models Linear regression (no hyperparameters) Quadratic regression (no hyperparameters) Quadratic regression with LASSO (1 hyperparameter) from sklearn.linear_model import LinearRegression, Lasso from sklearn.preprocessing import StandardScaler, PolynomialFeatures # automatically takes into account standardization from sklearn.model_selection import GridSearchCV, ShuffleSplit from sklearn.pipeline import make_pipeline from sklearn.metrics import mean_squared_error Pipeline A pipeline is a sequence of steps applied to the data. f.e. the first step is scaling the data and is training the model on the training data. Due to the package sklearn (StandardScaler), the training data is first standardized and then this is used for the training data and test data. # Linear regression linreg = make_pipeline(StandardScaler(), LinearRegression() ) # Quadratic regression quadreg = make_pipeline(StandardScaler(), PolynomialFeatures(degree=2), LinearRegression() ) # Quadratic regression with LASSO quadlasso_empty = make_pipeline(StandardScaler(), PolynomialFeatures(degree=2), Lasso(max_iter=10000) ) Training (+HP tuning) linreg.fit(X_train, y_train) # fitting linear regression Pipeline(steps=[(&#39;standardscaler&#39;, StandardScaler()), (&#39;linearregression&#39;, LinearRegression())]) quadreg.fit(X_train, y_train) # fitting the quadratic regression Pipeline(steps=[(&#39;standardscaler&#39;, StandardScaler()), (&#39;polynomialfeatures&#39;, PolynomialFeatures()), (&#39;linearregression&#39;, LinearRegression())]) Logarithmic scale: Standard scale but raise to the power of e. # Creating the grid, and deciding which hyper parameters to try np.linspace(-3, 0, 4) # Linearly spaced array([-3., -2., -1., 0.]) np.logspace(-3, 0, 4) # Logarithm scale array([0.001, 0.01 , 0.1 , 1. ]) quadlasso_empty.get_params() # tells us all the parameters that we have {&#39;memory&#39;: None, &#39;steps&#39;: [(&#39;standardscaler&#39;, StandardScaler()), (&#39;polynomialfeatures&#39;, PolynomialFeatures()), (&#39;lasso&#39;, Lasso(max_iter=10000))], &#39;verbose&#39;: False, &#39;standardscaler&#39;: StandardScaler(), &#39;polynomialfeatures&#39;: PolynomialFeatures(), &#39;lasso&#39;: Lasso(max_iter=10000), &#39;standardscaler__copy&#39;: True, &#39;standardscaler__with_mean&#39;: True, &#39;standardscaler__with_std&#39;: True, &#39;polynomialfeatures__degree&#39;: 2, &#39;polynomialfeatures__include_bias&#39;: True, &#39;polynomialfeatures__interaction_only&#39;: False, &#39;polynomialfeatures__order&#39;: &#39;C&#39;, &#39;lasso__alpha&#39;: 1.0, &#39;lasso__copy_X&#39;: True, &#39;lasso__fit_intercept&#39;: True, &#39;lasso__max_iter&#39;: 10000, &#39;lasso__normalize&#39;: False, &#39;lasso__positive&#39;: False, &#39;lasso__precompute&#39;: False, &#39;lasso__random_state&#39;: None, &#39;lasso__selection&#39;: &#39;cyclic&#39;, &#39;lasso__tol&#39;: 0.0001, &#39;lasso__warm_start&#39;: False} We want to use lasso__alpha as the name. grid = {&#39;lasso__alpha&#39;: np.logspace(-3, 0, 20)} # Log space tells it how many parameters to try. The first number: lowest value, second number: highest value, third number how many we want to generate holdout = ShuffleSplit(test_size=.25, n_splits=1, random_state=0) # validation method holdout Grid search object: quadlasso = GridSearchCV(estimator = quadlasso_empty, param_grid = grid, cv = holdout, scoring = &#39;neg_mean_squared_error&#39; # negative to understand which is the best value ) quadlasso.fit(X_train, y_train) #automatically refits GridSearchCV(cv=ShuffleSplit(n_splits=1, random_state=0, test_size=0.25, train_size=None), estimator=Pipeline(steps=[(&#39;standardscaler&#39;, StandardScaler()), (&#39;polynomialfeatures&#39;, PolynomialFeatures()), (&#39;lasso&#39;, Lasso(max_iter=10000))]), param_grid={&#39;lasso__alpha&#39;: array([0.001 , 0.00143845, 0.00206914, 0.00297635, 0.00428133, 0.00615848, 0.00885867, 0.01274275, 0.01832981, 0.02636651, 0.0379269 , 0.05455595, 0.078476 , 0.11288379, 0.16237767, 0.23357215, 0.33598183, 0.48329302, 0.6951928 , 1. ])}, scoring=&#39;neg_mean_squared_error&#39;) 6.6 Model selection Mean Squared Error Method mean_squared_error(y_test, linreg.predict(X_test)) 8.857781021481647e-29 mean_squared_error(y_test, quadreg.predict(X_test)) 8.126877243991158e-29 mean_squared_error(y_test, quadlasso.predict(X_test)) 3.5002060278057675e-06 ‘We choose the model with the lowest error. In this example, it is the last model.’ IMPORTANT It is important to note that before using the model in production, I must retrain it on the entire data set. We use the model on the entire data set: winner = quadlasso.best_estimator_ # selects the best hyper parameter winner.fit(X, y) Pipeline(steps=[(&#39;standardscaler&#39;, StandardScaler()), (&#39;polynomialfeatures&#39;, PolynomialFeatures()), (&#39;lasso&#39;, Lasso(alpha=0.001, max_iter=10000))]) As an example, below a car has been included which was not part of our data set. We can test the model on the prediction accuracy. seat_marbella = [4, 899 * 0.061, 41, 680 * 2.2, 19.2, 83, 2] # features loaded to compare y_marbella = (100 * 3.78) / (1.61 * 5.1) # labels loaded Now we ask the model to predict the full efficiency of the car: #prediction = winner.predict([seat_marbella]) #prediction[0] The real value is: #y_marbella We can compare the accuracy of the prediction and the real value. Leave-One-Out Cross validation (LOOCV): from sklearn.model_selection import LeaveOneOut, cross_val_score d = pd.read_csv(&#39;~/Documents/Pompeu Fabra BSM/Practical data science/auto-mpg.csv&#39;) label = &#39;mpg&#39; features = [col for col in d.columns if col != label] X = d[features] y = d[label] linreg = make_pipeline(StandardScaler(), LinearRegression() ) quadreg = make_pipeline(StandardScaler(), PolynomialFeatures(degree=2), LinearRegression() ) quadlasso_empty = make_pipeline(StandardScaler(), PolynomialFeatures(degree=2), Lasso(max_iter=10000) ) Creating a function: a set of instructions that we combine to provide some inputs to retrieve some outputs. This functions computers the \\(\\hat {ERR}_{\\hat f}\\) and will take as its input the model \\(\\hat f\\): def get_mse(model): mses = cross_val_score(estimator=model, X=X, y=y, cv=LeaveOneOut(), scoring=&#39;neg_mean_squared_error&#39; ) return np.average(-mses) # takes the average of the mean square errors Calculating the mean squared error through the leave one out method: get_mse(linreg) 11.371126332686618 get_mse(quadreg) 7.97288701609446 K-Fold Cross validation K = 5 folds. The model used is the same as above. 6.6.1 SVC Make list to create the model svc = make_pipeline( transformer, GridSearchCV( estimator=SVC(kernel=“linear”), param_grid={ “C”: np.logspace(-3, 3, 50) }, cv=kfold, n_jobs=4 ) ) The default of scoring = the estimator’s score method, the accuracy Fit model #svc.fit(X_tv, y_tv) Gives us the best estimator: #svc[1].best_estimator_ If it is at the edge of the grid, make another grid closer to the extremes. #pred = svc.predict(X_test) from sklearn.metrics import accuracy_score from sklearn.metrics import recall_score #accuracy_score(y_test, pred) #recall_score(y_test, pred) For parameter tuning, we ask scoring to be: svc = make_pipeline( transformer, GridSearchCV( estimator=SVC(kernel=“linear”), param_grid={ “C”: np.logspace(-3, 3, 50) }, cv=kfold, scoring = ‘recall,’ n_jobs=4 ) ) svc.fit(X_tv, y_tv) svc[1].best_estimator_ pred = svc.predict(X_test) accuracy_score(y_test, pred) recall_score(y_test, pred) RBF Kernel = svc_rbf = make_pipeline( transformer, GridSearchCV( estimator=SVC(kernel=“rbf”), param_grid={ “C”: np.logspace(-3, 3, 20), “gamma”: np.logspace(-3,3,20) }, cv=kfold, scoring = ‘recall,’ n_jobs=4 ) ) svc_rbf.fit(X_tv, y_tv) svc_rbf[1].best_estimator_ pred = svc_rbf.predict(X_test) accuracy_score(y_test, pred) recall_score(y_test, pred) Polynomial Kernel = svc_poly = make_pipeline( transformer, GridSearchCV( estimator=SVC(kernel=“poly”), param_grid={ “C”: np.logspace(-3, 3, 20), “gamma”: np.logspace(-3,3,20), “degree”: [2,3] }, cv=kfold, scoring = ‘recall,’ n_jobs=4 ) ) svc_poly.fit(X_tv, y_tv) svc_poly[1].best_estimator_ pred = svc_poly.predict(X_test) accuracy_score(y_test, pred) recall_score(y_test, pred) "],["practical-data-science.html", "Chapter - 7 Practical data science 7.1 Machine Learning 7.2 Bias-variance trade off 7.3 Gradient descent 7.4 Model Selection 7.5 Validations methods 7.6 Classification Problems 7.7 Regularization", " Chapter - 7 Practical data science Class given by: Alberto Santini 7.1 Machine Learning There are multiple types of machine learning: Supervised Unsupervised Reinforcement learning Reinforcement learning: Reinforcement learning (RL) is an area of machine learning concerned with how intelligent agents ought to take actions in an environment in order to maximize the notion of cumulative reward. An example of application is by playing chess where through trial and error, the algorithm was eventually able to beat any player. First they are taught the simple rules and then asked to train by themselves and learn from their mistakes. The algorithms are asked to make a move and either receive: A reward or A penalty As a result, they learn which moves are good and continue to try something else. However, machine learning still does not understand casual relationships. Unsupervised learning: Unsupervised learning is a type of algorithm that learns patterns from unlabeled data. The hope is that, through mimicry, the machine is forced to build a compact internal representation of its world and then generate imaginative content. While there are no labels available, the algorithm can still observe patterns and understand that there are commonalities. Again, it won’t be able to understand causality. It is possible to combine labeled data, for example from the passed and look for patterns with the unlabeled data. Supervised learning Supervised learning is an approach to creating artificial intelligence (AI), where a computer algorithm is trained on input data that has been labeled for a particular output. Therefore, it is extracting patterns from data and making predictions based on passed behavior. An example is a picture of an animal and the algorithms predicts which animal it is. Hereby we use training data to train the algorithm. However, the data must be labeled meaning we already know the correct answer. This method does not include trial or error. For example, first showing examples of cats and then it can make predictions. Then we show a new picture and it can predict whether it is a cat or not a cat. There are also multiple types of models than can be applied for supervised learning such as: Regression tasks: label is a continuous number. Hereby what we want to predict is continuous, not necessarily the data given to predict. F.E. House prices Classification tasks: Label is one of discrete set of possible values. F.E. Is it a dog or a cat. Or it is a score from 1 to 5, integer. 7.1.1 Theory Throughout this chapter, only supervised learning is explored in the two methods: regression and classification. Symbol Explanation y The real value. Generic output. The label. (ML) The thing I want to predict. Dependent variable (statistics). ŷ Prediction y-ŷ Absolute error ( y-ŷ)^2 Squared error x Generic input / Features. The things I use to predict y. The independent variable (statistics). p Number of features (machine learning). Number of independent variables I have (statistics). n Size of the data set If I know the inputs, I could try to “predict” the output. This would state that in the real world, the outputs (y) are a function of the inputs (x). In mathematical terms: \\(y = f(x)\\) For example, if we know multiple features of a house, we could try to predict whether a person would like it. (based on f.e. square meters, number of bedrooms etc.) However, realistically and economically we cannot always find all the possible features that would predict an output. It is always affected by some uncertainty. Therefore, y is a function of x but adding some noise. Which leads to: Deterministic function: \\(y = f(x) + ϵ\\) ϵ = (error) noise –&gt; a random variable which models some unpredictable events that happens in the real world, that we do not have a corresponding input for to take into account. We assume that ϵ obeys at least a couple of properties: ϵ is not correlated with any of the features Expected value of the random variable, ϵ[ϵ] = 0 Assumption 1 can be F.E. that someone really wants to buy a house because there is a good place to put a dog bed. This cannot be predicted based on the other inputs (features) that are in my data set. They should not be correlated. Assumption 2 says that the \\(e\\) doesn’t ALWAYS cause either a increase or a decrease in the output. It has to be truly random. In the real word we have the following: \\(y = f(x) + E\\). Next we want to try to learn more about this function f. In order to be able to do so, we have to find an estimator. The estimator is the thing that I want to use to approximate as best as possible the real relationship between the inputs and outputs in the real world. Estimator = \\(\\hat f\\) If we do a good job, we are able to find a \\(\\hat f\\) that is similar to the true value of f. If I am able to find a \\(\\hat f\\), I can plug in the input into the estimator and make a prediction. In mathematical terms: \\(\\hat f (x) = \\hat y\\) = prediction If the model is accurate the prediction is accurate = \\(\\hat f\\) is similar to \\(f\\) and therefore \\(\\hat y\\) is similar to \\(y\\) –&gt; We have accurate predictions. What is key for the data scientist -&gt; Out-Of-Sample Accuracy. This means that your model is accurate with your sample but also with out-of-sample data. How can I measure how accurate \\(\\hat y\\) is compared to \\(y\\)? We can have a look at the error. Squared error: Most classical error measure = \\((y-\\hat y)^2\\) Multiple reasons on why squared errors are used: - Taking square means I forget about the sign of the error (negative vs positive) - Taking square penalizes more ‘extreme’ errors Alternative way of valuing the error is the absolute error: \\(y-\\hat y\\) 7.1.2 Data set Typically we will call our data: x &amp; y. One data point looks like: \\((x1, x2, …., xp, y)\\) Taken the example of predicting house prices, the X1 can be independent variable 1 for example square meters. X2 can be number of rooms and XP the year it is build. Y is the price of the house. 7.1.3 Finding the expected value of the error A random variable is a variable whose value is unknown or a function that assigns values to each of an experiment’s outcomes. A random variable can be either discrete (having specific values) or continuous (any value in a continuous range). It is described informally as a variable whose values depend on outcomes of a random phenomenon. The error is a random variable. Therefore, y is also a random variable because some of its expression is: \\(f(x) + ϵ = y\\). \\(Eϵ[y-\\hat y]^2\\) = expected value of the squared error Because we know that the formula for y is = \\(f(x) + ϵ = y\\), we can re-write this expression as: \\(Eϵ[(f(x)+ ϵ-\\hat y]^2\\) We continue to solve the equation. We can re-write \\(\\hat y\\) as \\(\\hat f(x)\\): \\(Eϵ[(f(x)+ ϵ-\\hat f(x)]^2\\) We rearrange these terms: \\(Eϵ[(f(x)-\\hat f(x)+ ϵ]^2\\) For ease of notation, \\(f(x)-\\hat f(x)\\) becomes alpha \\(Eϵ[\\alpha+ ϵ]^2\\) We expand: \\(Eϵ[\\alpha^2 + 2\\alphaϵ + ϵ^2]\\) Make use of it being linear: \\(Eϵ[\\alpha^2] + 2Eϵ[\\alphaϵ] + Eϵ[ϵ^2]\\) Now we can see that alpha does not have the random element ϵ making it not a random variable and is deterministic term (constant). As it is constant and without error, it is already the expected value. We re-write again: \\(\\alpha^2 + 2Eϵ[\\alphaϵ] + Eϵ[ϵ^2]\\) As previously explained in the theory, the expect value of the noise should be 0. This is the assumption made. \\(\\alpha^2 + 0 + Eϵ[ϵ^2]\\) The variance of a random variable is, for example variable z = \\(Var[z] = E[z^2] - (E[z])^2\\) If we apply this definition to the above ϵ: \\(Var[ϵ] = E[ϵ^2] - (E[ϵ])^2\\) As we said before, the expected value of ϵ is 0 and therefore the variance is: \\(Var[ϵ] = E[ϵ^2] - 0 = E[ϵ^2]\\) To combine this with the previous equation: \\(\\alpha^2 + Eϵ[ϵ^2] = [f(x) - \\hat f(x)]^2 + Var[ϵ]\\) This final expression can be separated in two parts: \\([f(x) - \\hat f(x)]^2\\) \\(Var[ϵ]\\) Reducible error: Real relation - estimator Irreducible error Intrinsic property of the error If the model is really good, the estimator is similar to the real relation and I can “reduce” the error. If the model is extremely precise, the estimator can even be exactly the real relation. It therefore depends on the accuracy of the estimator. \\(\\hat f = f\\) However, even when this happens, I still cannot affect the “irreducible” error because it is noise from the real world. It is intrinsic property / characteristics of the data, not the estimator. To reiterate, we can only affect the reducible error. 7.1.4 Finding the error Data point i = \\(Xi ϵR^p\\) The estimator is a function that takes a p dimensional and produces a real values output. \\(\\hat f:R^p –&gt; R\\) If I have a concrete set of observations, I can estimate the expected value. For example, I can take the average height of a class to estimate the expected value of the height of the class. Mean squared error (MSE): The empirical average of the expected value of the error term. The average squared difference between the estimated values and the actual value. In mathematical terms: \\(MSE(\\hat f) = \\frac{1}{n} \\sum_{i=1}^{n}(y_i-\\hat y_i)^2\\) Considering that \\(\\hat y_i\\) is nothing else than the prediction for the i input = \\(\\hat y_i = \\hat f(x_i)\\), it can be transformed again: \\(MSE(\\hat f) = \\frac{1}{n} \\sum_{i=1}^{n}(y_i-\\hat f(x_i))^2\\) I can apply \\(\\hat f\\) to one row, calculate the p features and look at the real label, to compute the MSE. Mean absolute error (MAE): Here the only difference is that it is not squared. \\(MAE(\\hat f) = \\frac{1}{n} \\sum_{i=1}^{n}[y_i-\\hat f(x_i)]\\) 7.1.5 Loss function We can use a loss function which takes as input two numbers: the real and the predicted value and gives as an output another real number. This is the formula: \\(L(y, \\hat y): R^2 = R\\) Squared error Absolute error \\(L(y, \\hat y) = (y-\\hat y)^2\\) \\(L(y, \\hat y) = (y-\\hat y)\\) I want the loss function to obey two properties: If I make a correct prediction \\((\\hat y = y)\\), then I have 0 loss \\(L(y, \\hat y) = 0\\) if \\((\\hat y = y)\\). For most loss function, I want \\(L(y, \\hat y)\\) to be large than the “wronger” my prediction . The loss function should not become smaller when the prediction becomes “wronger.” Wronger = the more different my prediction is than the true number. The loss function should be small when my prediction is close to the true value and it should be large when it is not. Estimating the error on existing data on which I know the label: \\(Error(\\hat f) = \\frac{1}{n} \\sum_{i=1}^{n}L(y_i-\\hat f(x_i))\\) 7.1.6 Training vs test set Now it is being calculated how accurate the model is based on the available data set. However, we want the model to work well on new previously unseen data that is out of the data set / sample. In other words: Out of sample accuracy. Therefore, the data is separated randomly in two parts: the training set and the test set. Training set: A subset to train a model. Data we show our model to have it learn a good estimator. \\(\\hat f \\approx f\\). The training set will be used to derive to a estimator. Test set: A subset to test the trained model . Data which we hide from our model. After the model has been trained, we will simulate it to the test data to evaluate the model’s performance. The test set will be used to estimate the error of the \\(\\hat f\\). The training set is called: N = \\((x_1,y_1)....,(x_n,y_n)\\) The test set is called: M = \\((x_1,y_1)....,(x_m,y_m)\\) Therefore, to calculate the estimate of the error of the model: \\(Err(\\hat f) = \\frac{1}{n-m} \\sum_{i=m+1}^{n}L(y_i-\\hat f(x_i))\\) To train a model means to find good values for its parameters. First the shape of the model has to be fixed. As previously stated, there are several options. The below offers some of the options for regression problems: Linear: \\(\\hat f(x_1-,x_p) = \\beta_0+\\beta_1,.....+\\beta_p X_p\\) Beta’s are the linear coefficients and the X1, XP are the variables. The \\(\\beta\\) ’s are parameters. I can train a model to find a good estimator by finding good parameters. Quadratic: \\(\\hat f(x_1-,x_p) = \\beta_0+\\beta_1,.....+\\beta_p X_p + \\beta_1X_1^2+....\\beta_1pX_1p + etc.\\) Prior to analysis, it isn’t always clear what model will perform better. Subsequently, the data is trained on multiple models and the final selection is based on the lowest error. \\(x\\) = an input (p) \\(\\beta\\) = a vector of parameters (k) \\(\\beta^*\\) = optimal solution of the betas This is a optimization problem the goal is to minimize the empirical error of the model on the training set. Once the model is fitted to the data, the optimal values for the \\(\\beta\\)’s are found which will therefore be the estimator. Once the estimator is found, it is applied to the test data and an error for the model estimated. In mathematical terms: Training error =\\(\\frac{1}{n} \\sum_{i=1}^{n}L(y_i-\\hat f(\\beta_ix_i))\\) Test error = \\(Err(\\hat f) = \\frac{1}{n-m} \\sum_{i=m+1}^{n}L(y_i-\\hat f(\\beta^*_i,x_i))\\) 7.2 Bias-variance trade off Splitting the test and training set is performed in a random manner. Thus, if the split is made twice, it will most likely get different results. As there are now different training sets with different data points, an different optimal value for \\(B*\\) is found leading to additionally a different estimator and estimate of the error. Nevertheless, there is a second source of randomness. Bias-variance trade-off = \\(E_{Traintestsplit} [(y-\\hat y)^2] = Var[\\hat y] + (Bias[\\hat y])^2 + Var [ϵ]\\) How much does the estimator (random variable) defer from the real values \\(Bias[\\hat y] = E[\\hat y]-f(x)\\) =\\(Bias^2[\\hat y] = (E[\\hat y] -f(x))^2\\) Recalling: Variance = \\(Var[Z] = E[Z^2] - (E[Z])^2\\) Rewrite \\(E[(Z - E[Z])^2\\) \\(\\mu = E[Z]\\) \\(E[(Z-\\mu)^2]\\) The expected value of a random variable is a constant. Therefore, I can transform: \\(E[E[Z]] = E[Z]\\) Training test split = the expected value in the following scenario. \\(E[(y-\\hat y)^2] = E[(y-f(x) + f(x) - \\hat y)^2]\\) Next we can solve this equation: \\(E[(y-f(x))^2] + 2E[(y-f(x))(f(x)-\\hat y)] + E[(f(x)-\\hat y)^2]\\) Definition of \\(y = f(x)+ ϵ\\) \\(E[ϵ^2] + 2E[yf(x)-(f(x))^2 - y\\hat y + \\hat yf(x)] + E[(f(x)- \\hat y )^2]\\) We know that \\(E[ϵ^2]\\) is \\(var[ϵ]\\) The aim is to make the error of the model as low as possible. Naturally, the noise cannot be affected. Therefore, the variance and the trade off needs to be as low as possible. The best scenario is that they are both 0. Unfortunately if the variance is reduced, naturally there will be at least some biased introduced and vice verse. The variance of \\(\\hat y\\) = How much the prediction \\(\\hat y\\) varies when the training set is changed. Remember that the ultimate goal is that the predictor is similar to the real function = \\(\\hat f \\approx f\\). If the predictor varies a lot based on the training data, it cannot really be similar to the real function. Hence, I want the variance of \\(\\hat y\\) to be low. Model with high variance = Overfitted model (too complex). A small change in the training set leads to a large change in the predictions. This has a low bias. If the model learns the noise of the training set, it can make perfect predictions. Although in this instance, when the model is applied to a different data set, it will give bad predictions. Having too many parameters allows the model to learn the noise. If a model is very complicated, it is more likely to be overfitted (high variance, low bias). Model with high bias = Underfitted model (too simple). Large error even in the training set. This typically has a low variance. It is stable but it will give bad predictions and wont be accurate. It can be visualized by plotting a linear regression model. We have made a small change in the training set (one point has moved) and there has been a small variance in the regression line. So we can conclude that there is a low variance in this model. Now we will look at a polynomial model where there is 0 bias. Therefore, the model must intercept perfectly all the points in the training set. If I want to intercept 4 points in \\(R^2\\), what is the smallest degree of a polynomial that 1s need? –&gt; N-1 The green model has a high variance, but the bias = 0. This is a overfitted model. 7.2.1 Training (fitting) a model Training a model: Finding the parameters (of a parametric model) which minimize the loss function over the training set. Training a model simply means learning (determining) good values for all the weights and the bias from labeled examples. Training set has N points –&gt; \\((X_1,Y_1),....,(X_n,Y_n)\\). Minimize our parameters: \\(\\beta :=(\\beta_1....,\\beta_k)\\) = The K parameters of my model. The goal is to minimize the error -&gt; \\(\\frac{1}{n} \\sum_{i=1}^{n}L(y_i-\\hat f(\\beta_ix_i))\\) k = p+1. These are all the parameters plus the intercept in the example of a regression model. Renamed function of betas: \\(g(\\beta)\\) Not all functions are born equal. There are some functions that are easier to deal with. F.E. if they are continuous. In the case if I am in luck there is a good chance that I can find the minimum. If these conditions don’t hold, it will be very difficult. There is a chance that a algorithm finds not necessarily the global minimum but it does find the local minimum. This can be visualized below: A convex function: a continuous function whose value at the midpoint of every interval in its domain does not exceed the arithmetic mean of its values at the ends of the interval. Hereby, the global minimum is automatically also the local minimum as there is only one. Convex vs concave: Whilst trying to find a minimum for a function, a point must be found where the first derivative vanishes. Multiple things could happen: Finding a local min (might be global) Finding a local max (might be global) You find a saddle point 7.3 Gradient descent Instead, we present an algorithm that tries to find the local minimum using the first derivative. A vector that has all the partial derivatives at that point. It is a multivariate function as: \\(g:R^k -&gt; R^+_0\\) = \\(\\nabla g\\) \\(\\nabla g(\\beta_1,....,\\beta_k)\\) = \\((\\frac{\\partial g}{\\partial\\beta_1}(\\beta_1,...,\\beta_k),....,\\frac{\\partial g}{\\partial\\beta_1}(\\beta_1,...,\\beta_k))\\) Gradient = a measure of how steep a slope is Derivative = a function of a real variable measures the sensitivity to change of the function value (output value) with respect to a change in its argument (input value). Function = something that will take a input and will produce a given output. We want to go against the gradient (-) to find the minimum by following the blue steps to arrive at the minimum (pink). Newton’s Algorithm / Gradient Descent Algorithm = Gradient descent is a first-order iterative optimization algorithm for finding a local minimum of a differentiable function. The idea is to take repeated steps in the opposite direction of the gradient (or approximate gradient) of the function at the current point, because this is the direction of steepest descent. Only uses one parameter -&gt; alpha (\\(\\alpha\\)), can be any positive number. Below are the steps to find the solution: \\(\\alpha\\) = The step size / learning rate. Fix an arbitrary \\(\\beta_0R_k\\) to start from. For t=0,….,T: the maximum number of steps that I will do (going from \\(\\beta_1 - \\beta_2\\)). The equation is: \\(\\beta^{t+1} = \\beta^t-\\alpha\\nabla g(\\beta^t)\\) If the \\(\\nabla g(\\beta^{t+1})=0\\) then a break. The global minimum was found if the function is convex. Return the last \\(\\beta^t\\) available. If our function looks like below: We will find a local minimum and will be stuck in the equation as the gradient is 0. The above equation considers a fixed parameter \\(\\alpha\\). It is also possible to consider a changing parameter = \\(\\alpha_t\\). If \\(\\alpha\\) grows, then we move more at each iteration. This could mean that we move faster as there are fewer iterations. Alternatively, we start jumping all over the space of the parameters without converging or converging slowly. Here is an example of the set size (learning rate) being too large and experiencing “jumping”: How to choose \\(\\alpha\\)? Trying different values and finding the best solution. “Typically” \\(\\alpha = 10^{-3} = 0.01\\). At each iteration of the gradient descent algorithm we do: \\(\\beta^{t+1} = \\beta^t-\\alpha\\nabla g(\\beta^t)\\) \\(\\nabla g(\\beta^t) = \\nabla(\\frac{1}{n} \\sum_{i=1}^{n}g_i(\\beta^t))\\) Rewrite: \\(\\nabla g(\\beta^t) = \\frac{1}{n} \\sum_{i=1}^{n}\\nabla g_i(\\beta^t)\\) To compute \\(\\nabla g\\) there occur N gradients. These are one for each point in the training set. This must happen at each iteration. In big data settings, it is not unlikely that we have a training set that has millions of data points. This would require too many gradients (too slow). 7.3.1 Stochastic gradient descent In the previous computation we had to compute n gradients = \\(\\nabla g(\\beta^t) = \\frac{1}{n} \\sum_{i=1}^{n}\\nabla g_i(\\beta^t)\\) However, we can replace this with something that only needs to compute 1 gradient, no matter how large the data set is. Fix an arbitrary \\(\\beta^0 e R^k\\) to start from For \\(t=0,....T:\\) Draw \\(jϵ\\) {1,….,m} with uniform random distribution \\(\\beta^{t+1} = \\beta^t - \\alpha \\nabla g_j(\\beta^t)\\) etc. everything else is like in gradient descent The only difference is how we jump from the iterations. Instead of computing the entire gradient. Gradient descent = \\(\\beta^{t+1}= \\beta^t - \\alpha * \\frac{1}{n} \\sum_{i=1}^{n}\\nabla g_i(\\beta^t)\\) vs Stochastic gradient descent = \\(\\beta^{t+1}= \\beta^t - \\alpha \\nabla g_j(\\beta^t)\\) Where J is taken uniformly at random = j~\\(U({1,....,m})\\) GD vs SGD \\(\\beta=(\\beta_0, \\beta_1)\\) initial \\(\\beta^0=(\\beta^0_0, \\beta^0_1)\\) next \\(\\beta^1=(\\beta^1_0, \\beta^1_1)\\) Here the MSE\\((\\beta^0)\\) &lt; MSE \\(\\beta^1\\) In linear regression the: \\(g(\\beta) = \\frac{1}{n} \\sum_{i=1}^{n}y_i-(\\beta^0+\\beta^1x_i))^2 = MSE(\\beta)\\) In stochastic gradient descent: \\(g_j(\\beta) = y_j-(\\beta^0+\\beta_1x_j))^2 = SE_j(\\beta)\\) Here the squared error is only regarding the random point j. The regression line is changing so that the squared error of j becomes smaller. By doing so over and over and choosing random uniformly j, it is as if I was optimizing for all the squared errors. This can be proven by: Instead of using: \\(\\nabla g(\\beta)= \\frac{1}{n} \\sum_{i=1}^{n}\\nabla g_i(\\beta)\\) We use: \\(\\nabla g_j(\\beta)\\) Therefore, in expectation of a discrete random variable: \\(E [\\nabla g_j(\\beta)] = \\frac{1}{n} \\sum_{i=1}^{n}\\nabla g_i(\\beta) = \\nabla g(\\beta)\\) In expectation the gradient computed on that function \\(g_j\\) is equal to the big gradient (the gradient with respect to all the points). Subsequently, on average, the squared error of j is equal to the mean squared error of all the points. SGD needs more iterations then GD. However, each iteration is much quicker. In the end, you can expect that SGD takes less time than GD. Even if you have a convex differential function, you have less guarantee. GD with any alpha in 0,1 is guaranteed to converge to the optimum (global minimum). Stochastic gradient descent you only guarantee that you will arrive at a ball around the global minimum. But inside this ball there is no guarantee anymore. GD SGD SGD could jump around in the ball and never arrive to the global minimum. If this happens, you could go back to classic gradient descent and try to converge. Mini-Batch Gradient descent Takes the average of a subset of n of size k. These points are mostly not taken at random. Takes k points for each iteration. 1 epoch has passed per iteration. 7.4 Model Selection Model selection = choosing the “best” model out of a set of possible models that we are trying to consider. The “best” is considering the out-of-sample predictive accuracy = lowest error (loss) on the point NOT used for training = test set. While doing predictions, we want to come up with appropriate estimators (\\(\\hat f\\)) which are similar to our real word function (\\(f\\)), that links our features to our label. Again it is repeated = \\(\\hat f \\approx f\\). Typically there are many models that can be used. Therefore, we need to find out which model is the closest to the real value \\(f\\). Some models have hyper parameters that we need to choose which would give us even more different models. In machine learning, a hyper parameter is a parameter whose value is used to control the learning process. By contrast, the values of other parameters (typically node weights) are derived theough training. These are either not trainable through gradient descent or it would not be effective as it would defeat their purpose. If we include the hyper parameter through gradient descent, it would be set to 0. Moreover, setting the alpha to anything other than 0, would give us a worse model on the training set. Perhaps, this would translate to a better model on the test set. As we care mainly about the out-of-sample-accuracy, this would be optimal. Therefore, we need a different / better way on determining alpha which is different than gradient descent –&gt; Hyper parameter tuning procedure. In the models that include hyper parameters, the are a infinite number of models for each possible value of \\(\\alpha &gt; 0\\) . This complicates the model selection as we have to decide the best value of \\(\\alpha\\) . In order to avoid this, we could choose a model that has 0 hyper parameters and afterwards add a layer of complexity. 7.5 Validations methods 7.5.1 Hold-Out validation method “Simple” if no hyper parameters (training + test) “Nested” if there are hyper parameters (training + validation + test) Average error of the test set = An estimate of the real error which the model will exhibit on new unseen data. It is an substitute of the infinitely many data points that my model will classify when I will use it for real / in production. We compare this estimate of the error of the test set (\\(ERR_1\\)) with the multiple models and choose the lowest value by definition. We can additionally do model selection with hyper parameter tuning recursively. 7.5.1.1 Grid Search: The traditional way of performing hyperparameter optimization has been grid search (Parameter sweep), which is simply an exhaustive searching through a manually specified subset of the hyperparameter space for a learning algorithm. The first step is specifying a set of parameters which I believe are reasonable and try all in sequence. I believe (intuition or proof of concept) any value large than 1 is unlikely to be a good value for alpha because it would overemphasize the penalty of the parameter. In other words, it would cause too much bias for the reduction of the variance. Hyper parameter tuning = Hyperparameter tuning is choosing a set of optimal hyperparameters for a learning algorithm. A hyperparameter is a model argument whose value is set before the learning process begins. This entails holdout validations + Grid search. Nested procedure: (i) For each value of hyper parameters in the grid \\(\\alpha\\) Train the model with the \\(\\alpha\\) on the training set Estimate the error on the validation set Choose the hyper parameter \\(\\alpha\\) which gives the lowest error estimate on the validation set Therefore, the chosen \\(\\alpha\\) is going to be the hyper parameter configuration that I am going to use. With this value, I will evaluate the quality model on the test set. However, a model that is trained by more data typically performs better than with little data. By splitting again, the training set becomes smaller leading to a scarcityof data. (ii) Before passing to the next phase, with the fixed parameter, there is an intermediate step. We retrain \\(\\hat f_{\\alpha*}\\) on the entire training + validation set. (iii) Estimate the \\(ERR_{\\alpha*,1}\\) using the test set. 7.5.2 Standardization We do not want our model to see the test data during training as we do not want any information about the test data to be accessible by the model during training. Standardize my data set before starting. Still in the data exploration phase. If we standardize the data in the beginning before splitting the data into training/test, the mean and scaling will be on the entire data. Therefore there is some information of the test set that would flow to the model –&gt; Information leakage Alternatively: We first split the data. We standardize the training set. We apply the mean standard deviation on the test set The “real” error of my estimator: In the limit when I use an infinite long data set. Considering we do not have this data set, we find the empirical estimated error of my estimator using the data in the test set. Here 1 is the real error and the 2’s are the estimates. The first has higher variance than the second. What is the variance of the estimated error \\(\\hat {ERR}\\)? A standard trick to reduce the variance, is to increase the sample size and take the average. f.e. bootstrap method. What is the bias of estimated error \\(\\hat {ERR}\\)? We can try a larger training set. Is my \\(\\hat {ERR}\\) consistently an overestimation, underestimation of \\(ERR\\). \\(\\hat f\\) is training on the training set: just a smaller subset of the entire data set. We use it to compute the \\(\\hat {ERR}\\)(on the test set). The \\(\\hat f\\) trained on the training set is worse than the \\(\\hat f\\) on the entire data-set. The \\(\\hat {ERR}\\) obtained using the \\(\\hat f\\) training on the training set is worse than the \\(ERR\\) (the error made by \\(\\hat f\\) trained on the entire data set, in the limit). When I will use \\(\\hat f\\) in production, I will re-train it on the entire data-set. Therefore, \\(\\hat {ERR}\\) is likely going to be larger than the \\(ERR\\). \\(\\hat {ERR}\\) will be an overestimation of the \\(ERR\\). For an error to be worse, it implies it is larger than the actual error. Bias is due to the overestimation of the true error. 7.5.2.1 Reducing bias Training set selection probabilities \\(\\frac{1}{n} =\\) Probability that a point is chosen for the training set during 1 draw \\(1-\\frac{1}{n} =\\) Probability that a point is NOT chosen for the training set during 1 draw \\((1-\\frac{1}{n})^n =\\) The probability that a point is NOT chosen during all the n draws Limitation n -&gt; infinite -&gt; \\((1-\\frac{1}{n})^n = e^{-1} \\approx 0.3678\\) Subsequently, 36.78% of the points are never chosen. 100-36.78% = 63.2% go into the training set. These are the only values that are unique. The larger the data set, the better the model as the estimate of the error is less bias. However, the estimate of the error is affected by the size of the test set. Therefore, we want to balance the test-training proportions. The trick to reduce the variance is to average observations. There are several methods to reduce the variance such applying different validation methods. Below K-fold and leave on out methods are further explained. 7.5.3 Leave-One-Out Cross-validation method (LOOCV): We leave one out of the training set n times and find an estimator. Then we average the estimations: \\(\\frac{1}{m} \\sum_{i=1}^{m}\\hat {ERR}^{(i)}\\) Disadvantages LOOCV: Crazy amount of computing power. Extremely time consuming. If n is huge, it will take too much time. In HPT, we train each model n(n-1) *. This takes even longer. Does not scale with: big data + complex model There is still typically high variance compared with estimates obtained with other validation methods. This depends on the data &amp; models used. 7.5.4 K-Fold Cross Validation The model will be trained k times. K = between 2 and N. Primarily the data set is split k times into what is called “folds”, that are roughly the sames size. In each iteration we take a different fold as the test set and the rest as the training set. At every iteration we receive an \\(\\hat {ERR}\\) and the final used is the average of k iterations. These are estimated on the test fold (fold #1) when \\(\\hat f\\) is trained on all other folds (#2…#k). This method trades off the computing power vs bias. If K = small: Smaller training set -&gt; more bias Fewer iterations -&gt; less computing power If K = large: Larger training set -&gt; less bias More iterations -&gt; more computing power In practice, empirically using k=5 or 10 already has very accurate results. This is typically used. If the model has hyper parameters, the k-fold can be applied for the model selection and the k-fold hyper parameter tuning. Therefore it will be \\(k^2\\) folds. It might be that this is not computationally feasible. It can also be that holdout validation is used for the models selection and k-fold for hyper parameter tuning. Comparing methods: Holdout Validation LOOCV K-Fold Cross validation High bias Low bias Depends on K Reasonable computation time Crazy computation time (high) Depends on K 7.6 Classification Problems A classification model attempts to draw some conclusion from observed values. Given one or more inputs a classification model will try to predict the value of one or more outcomes. Outcomes are labels that can be applied to a data set. It is applied when the label is not continuous but it is a member of a discrete set. Examples: Classification Label y {Spam, Not Spam} Binary Classification y {0, 1] Binary with one-hot encoding Negative vs Positive y {Dog, Cat, Rabbit} Multi-class classification Real class -1 +1 Predicted class -1 True Negative False Negative +1 False Positive True Positive The features are: \\(XϵR^p\\) . If the feature is categorical, it is converted to numerical values by one-hot-encoding. We have to define appropriate loss functions. We ask to output the probability that the observation is either -1 or +1. Classifier \\(\\hat f:R^p\\) -&gt; {-1, +1} Classifier \\(\\hat f:R^p\\) -&gt; [0,1]. \\(\\hat y = P[y=+1]\\) At which threshold do we want to consider the observation to be 0 or 1 depends on the confidence of the results. The most typical manner: If \\(\\hat y &gt; 0.5\\) -&gt; 1 If \\(\\hat y &lt; 0.5\\) -&gt; 0 The threshold depends on the cost of making a mistake in a false positive vs false negative. Loss function In classification problems there are two loss functions: Surrogate loss function Loss function Minimizing the average surrogate loss function will lower the loss function on the test. Binary loss \\(l(y,\\hat y)\\) = 1+ if \\(\\hat y \\neq y\\) &amp; 0 if \\(\\hat y = y\\) We want to use the binary loss for selecting the model but not for training the model. 7.6.1 Evaluating model quality To see which measures are commonly applied to evaluate the quality of the model and perform model selection, the table below can be used: Recall = True positive rate (TPR). Probability of prediction. \\(\\frac {y+and\\hat y+}{y+}\\) False positive rate. Fall out = Probability of false alarm Precision = positive predictive value \\(\\frac {y+and\\hat y+}{\\hat y+}\\) False negative rate (FNR). Miss rate The perfect prediction is recall = 1 &amp; precision = 1. The bad prediction is recall = 0 &amp; precision = 0. Accuracy = # times the model is correct / number of predictions -&gt; \\(\\frac {(correct \\hat y)}{predictions}\\) or \\(TP/TN\\) We have to be careful with a unbalanced data set. This means that the amount of 0 are much more than the amounts of 1’s or vice verse. If we are only concerned about the accuracy, it might be that there is a very low discriminate power. F.e. if a data set has 99 positives and 1 negative, the model could adopt always classify as positive and it would be accurate 99% of the time. What is more important, is to identify correctly that 1 observation that is negative. \\(\\hat f(x) = p\\) t = threshold if \\(p &gt; t\\) -&gt; Positive (+1) if \\(p &lt; t\\) -&gt; Positive (+1) if t=0 -&gt; only prediction positive, where TPR = 1, FPR = 1 if t=1 -&gt; only prediction negative, where TPR = 0, FPR = 0 TPR = # true positives / # positive obs FPR = # false positives / # negatives obs 7.6.2 Receiver Operating Characteristics (ROC) ROC is a graphical plot that illustrates the diagnostic ability of a binary classifier system as its discrimination threshold is varied. A perfect model = green Random model = orange The light blue is closer to the perfect model. Therefore, we would select this model. AUC = area under the curve 7.6.3 Multi-class classification Most models only support binary classification. Some models natively support multiple classification such as neural networks. However, there are ways to transforms models to support multi-class classifiers. There are two methods proposed below: 1 vs 1 method: \\(y{A,B,C}\\) \\(\\hat f_{A,B} (x)[A,B]\\) \\(\\hat f_{B,C} (x)[B,C]\\) \\(\\hat f_{A,C} (x)[A,C]\\) After training each model, we use an input (x,y) and predict the classification: - \\(\\hat f_{A,B} (x)= A\\) - \\(\\hat f_{B,C} (x)= B\\) - \\(\\hat f_{A,C} (x)= A\\) Which ever group is voted most, will be the one classified. In this case, we have the most A’s. Advantage of this method is that the binary classifiers can be of any type. The drawback is that there are k possible classes. If there are many, classes, this will be a lot of work. 1 vs All (other) method: \\(y{A,B,C}\\) \\(\\hat f_A(x)[A, -A]\\) -&gt; \\(P_a\\) \\(\\hat f_B(x)[B, -B]\\) -&gt; \\(P_b\\) \\(\\hat f_C(x)[C, -C]\\) -&gt; \\(P_C\\) Then we choose the highest probability as it has the highest confidence. The advantages is that it only needs k classifiers. However, there needs to be models that outputs probabilities. It is more likely in this approach that the estimators are more precise to the 1-vs-1 classification. As each model here can be trained on the entire data set. 7.6.4 Maximal Margin Classifier An easiest model is when there is only one feature with two classes. A linear classifier can split these classes into two and classify the two groups perfectly. However, typically there is some overlap where the variables are not linearly classifiable. Below are examples from both options: On a two dimensional spaces this looks like: \\(R^p = H: B_0+B_1X_1+B_2X_2.....B_pX_p=0\\) Robustness maximize the “distance” from the 2 classes. 7.6.5 Support vector classifier It is also possible that there is non-linearity where you cannot draw a hyper plane and draw a line to separate the classes. If the objection is to still make this separation in a hyper plane, there has to be some miss-classifications. For example, the below it might be better to classify the one “outlier” wrong and keep the linear separator as it might prove better out-of-sample accuracy. We want to pick up the true plus difference instead of the noise in the data. therefore, we could use a maximum margin classified which allows for a few miss-classification. This could be within a “budget.” Points that are correct classified on the hyper plane, are additionally further away than the margin and therefore with confidence classified. 7.6.5.1 We want to associate with a certain variable, \\(ϵ_1\\). Somewhat like a penalty of miss-classifying objects. Outside of the margin -&gt; Well-classified = \\(ϵ_1=0\\) = \\(y_i(….)&gt; M(1-ϵ_i)=M\\) Correctly classified but within margin -&gt; Small penalty = \\(ϵ_1=[0,1]\\) = \\(y_i(….)&gt; M(1-ϵ_i)[0,1][0,M]\\) Miss-classified -&gt; Large penalty = \\(ϵ_1=1&gt;\\) = \\(y_i(….)&gt; M(1-ϵ_i)[&lt;0]\\) The goal is to maximize the margin. Now we calculate: \\(y_i(\\beta_0+\\beta_1X_i+....\\beta_pX_i&gt; M(1-ϵ_i)\\) Type 1: If ϵ is 0, the quantity will be larger or equal to M. So if y is larger than M, it is correctly classified because the distance between y and the hyper plane is larger than the margin. Type 2: If ϵ is between 0-1, the margin will also be between 0-1. now we cannot guarantee that the distance between the hyper plane is larger than 1. Only that it is between 0 and M. Type 3: If ϵ is larger than 1, the M becomes negative. This means that y is a negative number and cannot be classified correctly. As for example the real class was positive but the predicted class was negative or vice verse. However, we only want to allow for a few of the observations to be miss-classified to maximize the model. \\(ϵ_i=0&gt; u_i(....)&gt;M\\) Therefore, there will be “budget” Where the sum of the ϵ cannot go above a certain value. This will be the hyper parameter C. \\(ϵ_1 +ϵ_2+....+ϵ_m&lt;C\\) If C=0, we do not allow for any miss-classification (maximum margin classifier). If C=small bias will be low but the variance could be small. The outlier could change the model a lot. If C=large, the bias is large and variance small. There will be many miss-classification. Therefore, C is a trade-off of bias and variance. We can use again the grid search to find the hyper parameter. 7.7 Regularization When a model has very high variance and we want to reduce it at the expense of increasing the bias a little bit (Bias trade off). We want to do this to have a higher out of sample accuracy (also for interpretation we will see it later). The two of the components of the accuracy of the model: bias square and the variance. Two components we can have an effect on. We use Regularization for models that are complex with many parameters because they extend to exhibit high variance. We will see it in the context of regression mainly (polynomial normally). Regularization term = \\(\\alpha R(\\beta)\\) where $\\alpha $ = hyperparameter. RIDGE REGULARIZATION (Here I am penalizing a minimization problem, but I am adding a not negative cost and what I am penalizing is the square, so if beta is small not penalizing much but if it big, we are penalizing a lot). This works better if the data is penalized. Doing this, I am trying to reduce the variance of the parameters and the model. VARIANCE DECREASE BUT THE BIAS INCREASE (because with the regularization term I am finding other parameters than those minimizing the loss function I am also penalizing large parameters → large penalty). So this will not perform as good as the other, so there is a trade-off between variance and bias (alpha can go to extremes, so a good parameter will stay somewhere in the middle) Hyperparameter Alpha Green line: Variance of the model Black Line: Bias square of the model Pink line: Mean square error (the total: sum of the other two) Using logarithm scale What happens to the parameters in the model: the possible values for the alpha (-2,5 and use 50 values) then we create a list of the parameters of my model for each possible value of alpha. So I have a polynomial model (scale also as logarithmic). Each line corresponding to a parameter of my model is a polynomial so we have the linear and the quadratic terms. Here when I increase the hyperparameter alpha we can see that the parameters are squeezed towards 0. An interesting thing is to count the number of parameters that are equal to 0 or non 0. How to do that (the number of nonzero) All will be different from 0. That is why we obtained a flat line for ridge regularization. So why is this the case and why do we care about the non zero coefficients? I want that my models are accurate (that is why we introduced regularization)) but also due to interpretability. This is a human factor. If I find a 0, the feature is not needed to predict good predictions and vice versa. So we can infer that the feature is not correlated with the label. 7.7.1 Interpretability Is a key feature to help machine learning models in real life. Interpretability is required in real life and not just having a high accuracy. We need to know why. Besides, if we select the good features, it will probably be better. (the features had signal and not notice). This is called variable selection. Best subset of variables is a very old problem. Used to obtain the lowest out of sample loss, → the highest accuracy. F is considering some features. If I have p larger than n it will be very difficult to know the noise. Reducing the number can help despite the ones that we drop are correlated. Naive approach: try all subset of possible features. Forward Heuristic: (Consider the empty set), then the sets with one feature. Assume that feature 3 is better. So these features will be fixed for all the procedures, this means that will never get out of my set F. The same with 2 features and always with 3. If any of the values is better, we are going to fix it with the next iteration. At some point you don’t obtain something better so you stop. Backwards stepwise heuristic selection: Which feature to remove. For example, 3. So if I remove 1 and 3. And when I can’t improve anymore I take the last combination. Lasso Regularization We can consider using Lasso regularization. It is a bridge between regularization and feature selection. Set some parameters in the model equal to 0. Implicitly it helps in interpretability and indirectly to variable selection. I have p + 1 parameters. We penalize the large parameters (positive or negative) and squeeze them equal to 0. Using the same as in for ridge, this time when we plot the non zero is not a straight line as before. There is a point, that increase probably is because it didn’t find the global optimum. Empirically it’s true that many parameters are equal to 0. Why does this happen? RIDGE AND LASSO REGULARIZATION (Unconstrained minimization problems as betas can take any values. Remember large betas are penalized and squeezed to 0) There is an equivalent problem that is constrained (minimization problem) as we can see below in the image above. Ridge: All the points inside the blue circle are valid points and outside not valid as violates the constraint. The Beta * test a good training MSE = 2 but i cannot accept it. So let’s see 2.1. (light blue beta*). Lasso: All the points inside valid and outside not valid. This once has spiky regios and for that reason, the probability to intersect in one of the spikes first is very probable. These points have some of the betas= 0. Lasso is more likely to produce optimal parameters where beta is equal to 0. Finally: ISL book: In general neither of them will dominate the other (ridge and lasso) We will have to do first model selection. In general, we expect lasso to perform better for low predictors, on the other hand perform better when there are many predictors. However, the number of predictors is never known a priori. "],["advanced-statistical-methods.html", "Chapter - 8 Advanced Statistical Methods 8.1 Clustering 8.2 Multi-Dimensional Scaling 8.3 Principal component analysis 8.4 Correspondence analysis 8.5 Multiple linear regression &amp; classification trees 8.6 Text analytics", " Chapter - 8 Advanced Statistical Methods Class given by: Michael Greenacre library(lavaan) library(formattable) library(kableExtra) library(knitr) 8.1 Clustering Clustering has the aim to classify similar objects (cases, people, companies, products, texts, advertising campaigns…) into groups so that within each group the objects are relatively similar, while the groups are different between one another. This is also called segmentation or partitioning. The basic concept in clustering, as well as in most areas of statistics where there are many variables being measured simultaneously on the objects, is that of distance. If we can define a distance between objects then we can devise algorithms to put those “close” to one another in the same group, and those “far” from one another in different groups. The same underlying concept is used in several methods of big data analysis, for example, finding people in a large database that are “close” to you as a client in terms of your purchasing choices. Three types of hierarchical clustering: Complete linkage: maximum method: take the maximum level of the two options Average linkage: average method Ward linkage Significance clustering. Visualized by a dendogram. 8.1.1 Hierachical clustering Argentina Brazil Canada China Germany Spain Italy Netherlands Poland Turkey USA Argentina 0 2 7 6 8 4 6 7 8 4 6 Brazil 2 0 6 6 7 4 5 7 8 4 6 Canada 7 6 0 5 3 4 4 2 4 5 2 China 6 6 5 0 7 7 7 6 6 7 5 Germany 8 7 3 7 0 3 3 1 2 5 4 Spain 4 4 4 7 3 0 1 3 4 3 5 Italy 6 5 4 7 3 1 0 3 4 3 5 Netherlands 7 7 2 6 1 3 3 0 3 5 4 Poland 8 8 4 6 2 4 4 3 0 5 4 Turkey 4 4 5 7 5 3 3 5 5 0 6 USA 6 6 2 5 4 5 5 4 4 6 0 8.1.1.1 Complete Linkage DK.clust &lt;- hclust(as.dist(table)) plot(DK.clust) + abline(h = 6, col=&quot;red&quot;) integer(0) 8.1.1.2 Average linkage plot(hclust(as.dist(table), method=&quot;average&quot;), main=&quot;Average linkage&quot;) + abline(h = 5, col=&quot;red&quot;) integer(0) Significance clustering Total sum of squares (total variance): Each subset has a different mean. We calculate the within-group SS. ANOVA: We look at the distance between groups and compare it with the total sum of square. Then we check if this is significant or if it is random. 8.1.1.3 Ward linkage Ward clustering: related to variance criteria &amp; ANOVA. Of particular interest for all Euclidean and weighted Euclidean distance functions. It decomposed the total variance of the points in space into parts: “Within-clusters” and “between-clusters.” The idea can be illustrated by analysis-of-variance (ANOVA) for one variable (SS= sum[or average] of squared deviations from the mean), when observations are grouped. plot(hclust(as.dist(table), method=&quot;ward.D2&quot;), main=&quot;Ward linkage&quot;, xlab=&quot;Countries&quot;, hang=-1) + abline(h = 8, col=&quot;red&quot;) integer(0) Ward clustering: Ward clustering in multivariate space: For any set of n points in (weighted) Euclidean space: Centroid = overall mean For successful clustering, we want the between-group variance to be as high as possible, which is equivalent to the within-group variance being as low as possible. This is what the hierarchical form of ward clustering does at each merging step of the dendogram. 8.1.2 Non-Hierachical clustering K-means: K = the number of groups. To achieve a simple partitioning of a data set (usually very large) into pre-chosen number of homogeneous groups. Suppose G groups and select 3 points as seeds: the initial point (team leader). Seeds can be chosen at random or by intelligent selection. Next these new groups will calculate its mean. The centroids are now the seeds of the next step. The groups are recalculated and a new centroid is found. This continues until there is no further improvement to make, this is the solution. In these steps, the between-group variance becomes large and the within-group variance decreases. Because the seed is randomly chosen, you should repeat it multiple times to have different starts, and therefore solutions and get the optimal result. You can choose to use the Euclidean distance or the Manhattan distance. Because of the bigger differences in the variables with wide ranges compared to those with narrower ranges, one way to get rid of this effect is to simply divide each variable by its maximum value. Now, there is a range between 0-1. This is classed range-standardized data. 8.2 Multi-Dimensional Scaling Scaling Factorial methods Principal components analysis (PCA) Correspondence analysis (CA) Multidimensional scaling (MDS) Metric MDS Non-metric MDS [1] 0.7181458 0.7909089 [1] 0.09199945 8.3 Principal component analysis satis &lt;- read.csv(&quot;www/satisfaction.csv&quot;, header = TRUE, sep = &quot;,&quot;, dec = &quot;.&quot;) attach(satis) data &lt;- satis[,c(4:12)] row.names(data) &lt;- satis[,2] DK.diss &lt;- dist(data) DK.data &lt;- cmdscale(DK.diss, eig=TRUE) 8.3.1 Biplots DK.PCA &lt;- PCA(as.matrix(data), weight=FALSE) PLOT.PCA(DK.PCA, map=&quot;asymmetric&quot;, rescale=0.5, axes.inv=c(-1,1), dim=c(2,3)) 8.4 Correspondence analysis Within correspondence analysis (CA) we do not analyze absolute values but relative values (profiles) that are visualized. This means relative to the row sums and column sums. Row &amp; column profiles are weighted by marginal values (masses). A equivalent way of think about CA is in two ways: Finding the optimal scale for response categories that discriminates between explanatory variables. We are interested in the relationship between these two sets. Response variable = f.e. attitudes Explanatory variable = f.e. demographics Optimal scale values: standard column coordinates standard column coordinates on first CA axis. Group mean scores for optimal scale values: first optimal scale values: first principal row coordinates We want to maximize the between-group variance. Therefore, we introduce the identification conditions of individual scores on the response variable. An example is to adopt a mean 0 and variance 1. Where are variables are standardized to -1 between +1. Alternatively, finding optimal scales for the two (or more) variables that maximize their correlation. We are interested in their inter-relationship within a single set of variables Optimal scale values: standard row and columns coordinates of the respective questions on first CA axis. Squared correlation is equal to the first principal inertia (eigenvalue). Higher correlation achieved by opposing those with strong opinions against those with moderate ones. Creating cross-tabulation in r: my_table(head(age.V18.US)) SA A NN D SD 15-24 20 59 15 24 7 25-34 31 84 19 88 18 35-44 29 104 22 71 16 45-54 13 91 20 70 25 55-64 16 75 23 66 17 65-74 4 54 10 56 16 8.4.0.1 Concatenating tables The approach of stacking or concatenating tables, interactively codes categories. For example: Variables gender and country become: Canada 1 Canada 2, etc. Where 1 represents male and 2 represents female. Such as below: However, if there are many different explanatory categories, this could result in a extremely large data set. It is also possible to stack the results for each response variable by explanatory variable although that does not provide a interaction. Concatenating tables in r: 8.4.0.2 Plot The red points are the vertices of a simplex in the full space in which these row profiles lie. my.ca &lt;- ca(age.V18.US) plot(my.ca, main=&quot;One parent can bring up a child as well as two parents together&quot;, mass=c(TRUE,TRUE)) The inertia measures the amount of variance in the table or how much association between the columns and the rows variables.’ Inertia = \\(x^2/n=0\\) Correlation between the row and columns = \\(\\sqrt\\lambda_1\\) Contingency coefficient = \\(C_{max} = \\sqrt{\\frac{\\sigma^2}{1+\\sigma^2}}\\) After standardizing (normalizing) where \\(C* = C/C_{max}\\) which lies between 0-1 = \\(C_{max} = \\sqrt{\\frac{K}{1+K}}\\) Blaikie proposed: 0.00 None 0.01-0.09 Negligible 0.10-0.29 Weak 0.30-0.59 Moderate 0.60-0.74 Strong 0.75-0.99 Very Strong 1.00 Perfect summary(my.ca) Principal inertias (eigenvalues): dim value % cum% scree plot 1 0.033590 76.7 76.7 ******************* 2 0.007499 17.1 93.8 **** 3 0.002111 4.8 98.7 * 4 0.000589 1.3 100.0 -------- ----- Total: 0.043789 100.0 Rows: name mass qlt inr k=1 cor ctr k=2 cor ctr 1 | 1524 | 99 996 320 | -360 914 381 | 107 81 152 | 2 | 2534 | 189 990 129 | -61 122 21 | -161 868 656 | 3 | 3544 | 191 922 94 | -141 922 113 | 2 0 0 | 4 | 4554 | 173 920 78 | 100 513 52 | 89 407 184 | 5 | 5564 | 155 128 25 | 25 85 3 | 18 43 6 | 6 | 6574 | 110 931 181 | 258 931 220 | 4 0 0 | 7 | 75 | 83 936 173 | 292 934 211 | 11 1 1 | Columns: name mass qlt inr k=1 cor ctr k=2 cor ctr 1 | SA | 91 995 460 | -450 919 551 | -130 76 205 | 2 | A | 396 868 94 | -64 389 48 | 70 479 262 | 3 | NN | 95 408 62 | -58 119 10 | 91 289 105 | 4 | D | 329 996 233 | 149 719 218 | -93 277 377 | 5 | SD | 89 938 151 | 255 880 173 | 65 58 51 | 8.4.1 Multiple Correspondence Analysis Rather than look at the relationships between the demographics and the questions, we now look at the relationships within the questions. This is also reminiscent of factor analysis. Also we look at individual-level data, although we will soon see that this has something to do with the Burt matrix as well. The total inertia of an indicator matrix is \\(Z = (J-Q)/Q\\). Where the categories are the J and the Q are the variables. The standard coordinates of the categories are identical The principal inertias of the Burt analysis are the squares of those of the indicator matrix: thus the percentages in Burt analysis are higher! First principle: the inertia of a Burt matrix should ignore the sub matrices down the diagonal. Second principle: the “explained inertias” should be explaining the off-diagonal matrices (=adjusted inertia). The adjusted singular values (square roots of the adjusted inertias) are (where adjusted inertias) are (where’ s are the inertias of the Burt matrix) 8.4.1.1 Subset analysis CA CA is subcompositionally incoherent because if we extract a subsection of the CA then the profiles will be relative frequencies, relative to the total. 8.5 Multiple linear regression &amp; classification trees Assumptions: The conditional distribution is normal X,Y is a straight line The variance is the same. Homoscedastic However, in reality this is not always true. The transformation fo the mean is called the link function. The conditional distribution it is appropriate to the type of response variable. 8.5.0.1 Regression tree 8.6 Text analytics "],["marketing-analytics.html", "Chapter - 9 Marketing Analytics 9.1 Logistic regressions 9.2 Conjoint Analytics", " Chapter - 9 Marketing Analytics Class given by: Mohammad Ghaderi 9.1 Logistic regressions Logistic regression: a specialized form of regression that is formulated to predict and explain a binary (two-group) categorical variable rather than a metric dependent measure. May be described as estimating the relationship between a single-non-metric binary dependent variable and a set of metric or non-metric independent variables: Classification matrix: means of assessing the predictive ability of the logistic regression model. Created by cross-tabulating actual values with predicted values. Shows incorrect and correct classifications. Cross-validation: Procedure of dividing the data in two parts. Avoids over fitting. Analysis sample: Used in estimating the logistic regression model Holdout sample (validation sample): Used to validate the results Hit ratio: Percentage of objects correctly classified by the model. TP + TN / N True Positive + True Negative / Number of observations Two types of the logistics coefficient: Logistic coefficient: (original). A positive relationship means that an increase in independent variable is associated with an increase in the predicted probability. A negative value implies a decrease in the predicted probability A positive values implies a increase of the predicted probability Value 0.0 means a probability of 50%. Exponentiated logistic regression: stated in terms of odds. There won’t be negative values. Coefficient -1.0 = percentage change in the odds F.e. Coefficient of 0.20 = a negative 80 percent change in the odds –&gt; 0.20-1.0 = -0.80 for each unit change in the independent variable A value of 1.0 means there is no change in the odds. The odds are 50% to predict either group. There is a relationship with no direction. A value above 1.0 means a positive increase in the predicted odds Assessing magnitude of change: Percentage change in odds = (Exponentiated coefficient - 1.0) * 100 Logistic curve: represent the probability of an event. Logit transformation: transforms values into a discrete binary dependent variable –&gt; probability of an event. This probability is transformed into the odds ratio which acts as the depend variable. Maximum chance criteria (MCC) = Measure of predictive accuracy that is calculated as the percentage of respondents in the largest group. - N largest group / total number of observation. - If your hit rate is larger than this, you are having some value. Odds: the ratio of the probability of an event occurring to the probability of the event not happening. Used as the dependent variable. Model estimation fit: Likelihood value: the lower the -2LL, the better for of the model. Perfect fit is 0. Pseudo R^2 measures: values from 0.0-1.0. Cox &amp; Snell R^2 = the higher value, a greater model fit. The amount of variation accounted for by the model. 1.0 is a perfect fit model. 9.1.1 Sensitivity vs Specificity Sensitivity = TP / TP + FN, second number Specificity = TN / FP + TN Hit rate = TP + TN / N True positive rate = TP / TP + FN Actual negative = TN / TN + FP Validation of the results: Establishing external validity is done through assessment of hit ratios through a separate holdout sample. It is supported when the hit ratio of the selected approach exceeds the comparison standards that represent the predictive accuracy expected by chance. 9.2 Conjoint Analytics Quick summary **‘Conjoint Analytics** enables us to understand, describe, and predict consumers’ choices in the contexts where the items/products/services that they have to choose among could be described based on a collection of attributes; and that individuals make a trade-off among these attributes to choose the most appealing offering.’ Products are composed of multiple attributes or features. Purchasing decisions involve complex and subtle trade-offs between different features. It is difficult to articulate the value that attribute to a particular feature in isolation. Conjoint analytics can give a good indication of how customers perception of value is built up. It is possible to have an idea regarding the “value premium” that consumers derive from a feature. This can be applied to decision making such as which features to include to increase market share. Customers make trade-offs in selecting a particular product configuration if: Different combinations of products are provided Customers are asked to choose Now it is possible to learn from the relative importance of attributes. Steps of Conjoint Analytics Showing customers various hypothetical product configurations and price points and asking them to evaluate them (or choose between them) Regression analytics is then applied to their responses to isolate the effects of individual features on the customers perception of product value Result: an increment “perceived value” for each of a products features Assessing functional value Measure customers’ overall preferences for a selected number of product configurations Decompose these overall preferences into the values that customers attach to each level of each attribute This can be performed through regression analytics. Elicitation process Identifying the set of relevant attributes Assign levels to the attributes Combine levels to generate profiles Generate questions &amp; collect data A product configuration is composed of one specific level for each of the attribute in the bundle where. Attributes must be relevant to the consumers choices and be easy to measure. Profile is a combinations of levels. As an example we take a pizza where: Attributes –&gt; cheese, crust, toppings Levels -&gt; Topping = tuna, mushroom, salami. Crust = thick or thin etc. Profile -&gt; Pizza with mozzarella cheese, tuna and a thick crust The amounts of profiles possible are the configurations. Each categorical variable is transformed into dummy variables where 0 indicates not chosen while 1 indicates chosen. This leads to a regression model for consumer preferences. Applying to the example of a pizza: Score: \\(U = b_0 + b_1MOZ+b_2THIN+b_3TUN+b_4SALAM\\) Conjoint analytics works when consumers choice decision process is a compensatory process where a highly valued option on one attribute can compensate for an unattractive option on another attribute. After calculating the betas for the regression model, we can evaluate what attributes are more important to the individual respondent. This can then be used to crate benefit segments to group individuals on the basis of the utilities they attach to different product attributes. Hereby cluster analytics is used to create the benefit segments using the attribute importance. The last step is to use this information to: Estimate the most cost-efficient way to deliver a desired set of features Identify the “sweet spots” where margin is maximized Obtain the optimal configuration of functionalities to best compete in a target segment Apply to product re-featuring, product line extension Setting up the survey In the full factorial design, all possible combinations of attributes and levels are asked to the consumer. However, there might be too many profiles to question and retrieve relative answers. It should be under 20 combinations. A method to subset these profiles is: Fractional factorial design: Ratings are only asked on a scientifically selected fraction of combinations Orthogonal design Adaptive design Different method of asking how to value the profiles such as: Ranking Metric conjoint: Ratings Choice-based conjoint (CBC) You should include the option of no choice in the choice method. The amount of choice tasks should not exceed 20 and be ideally less than 15 if possible. The consumer choses the product based on the highest utility. 9.2.1 Random utility model The choice behavior of individuals is based on the random utility model (RUM). Each alternative generates a utility for the individual. Let us denote by \\(U_i\\) that utility that the profile \\(i\\) generates to the individual. In this case, in a choice task with the choice set: \\(C = [i,j]\\), the respondent will choose \\(i\\) if \\(U_i &gt; U_j\\). The utility can be decomposed into two main components A deterministic part that explains the contribution of different observable attributes to the choices A random component that simply is the difference between the “true” utility of the profile for the individual and the deterministic part This leads to: \\(U_i = V_i + e_i\\) The random component is due to additional factors that influence choices that are not observable to the analyst. Therefore, the preferences are based on choice probabilities. \\(Pr(i|i,j) = Pr(U_i &gt; U_j) = Pr(V_i+e_i&gt;V_j+e_j) = Pr(e_j-e_i&lt;V_i-V_j)\\) Different choice models will be derived by making different assumptions on the distribution of the random component. Choice models Probit model = A conventional assumption on the random term in the above model is to assume that \\(e_i\\) and \\(e_j\\) are distributed normally. Logit model = Can be derived by assuming Extreme Value of Type I (EV) distribution on ε, and by assuming that the random components are independent from each other. Example using the logit model for a hotel: \\(U = 12 + 15*Gym + 10*SwimmingPool - 20*Price150\\) Here the baseline for price 100. Therefore, the difference going from a price 100 to price 150 (50 euros increase) is a decrease in utilities of 20 units. Moreover, the equivalent of having a gym is 15 unites of utility. The monetary equivalence of having a gym is therefore = \\((15*50)/20=€35\\) In other words, by including a gym in the offer, the hotel can charge €35 more. "],["references.html", "References", " References Gray, kevin. 2017. Structural Equation Model. kdnuggets. https://www.kdnuggets.com/2017/03/structural-equation-modeling.html. "]]

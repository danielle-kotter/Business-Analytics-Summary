[["index.html", "R-coding Cheatsheets &amp; Summary Preface", " R-coding Cheatsheets &amp; Summary 16 April, 2021 Preface The following document has been prepared to have a prompt link to the code learned and used throughout several projects in the last year. As a beginner in programming, it is always useful to have input accessible to avoid having to look through numerous repositories. The bookdown package that has been used for this format can be installed from CRAN or Github: install.packages(&quot;bookdown&quot;) # or the development version # devtools::install_github(&quot;rstudio/bookdown&quot;) To compile this example to PDF, you will need XeLaTeX. You are recommended to install TinyTeX (which includes XeLaTeX): https://yihui.org/tinytex/. Prepared by: Dani√´lle Kotter "],["basics-r.html", "Chapter - 1 Basics R 1.1 Data sets", " Chapter - 1 Basics R Mathematical values \\(\\mu\\) = $\\mu$ = Population mean \\(\\sigma\\) = $\\sigma$ = Population sd \\(\\bar{x}\\) = $\\bar{x} = Sample mean \\({e}\\) = ${e}$ = Standard error \\(\\pi\\) = $\\pi$ = Pie \\(\\ge\\) = $\\ge$ = Bigger than \\(\\le\\) = $\\le$ = Smaller than Sample mean, standard deviation mean &lt;- mean(variable) sd &lt;- sd(variable) Removes values NA in a data set: mean &lt;- mean(variable, na.rm = TRUE) sd &lt;- sd(variable, na.rm = TRUE) Weighted mean &amp; standard deviation library(Hmisc) weightedmean &lt;- wtd.mean(x,y) weightedsd &lt;- sqrt(wtd.var(x,y))/sqrt(n) Variance var(variable) [1] 9.166667 1.0.1 Tables, frames &amp; Matrices As matrix = library(data.table) matrix(c(1:8), nrow = 4, byrow = TRUE) #organized by row [,1] [,2] [1,] 1 2 [2,] 3 4 [3,] 5 6 [4,] 7 8 matrix(c(1:8), ncol = 4, byrow = FALSE) #organized by col [,1] [,2] [,3] [,4] [1,] 1 3 5 7 [2,] 2 4 6 8 As data frame = data.frame(Column1 = c(1:5), Column2 = c(1:5)) Column1 Column2 1 1 1 2 2 2 3 3 3 4 4 4 5 5 5 data.table(Column1 = c(1:5), Column2 = c(1:5)) Column1 Column2 1: 1 1 2: 2 2 3: 3 3 4: 4 4 5: 5 5 As data table = data.table(matrix(c(1:8), nrow = 4)) # or V1 V2 1: 1 5 2: 2 6 3: 3 7 4: 4 8 data.table(Variablex = 1:5, Variabley = 1:5) Variablex Variabley 1: 1 1 2: 2 2 3: 3 3 4: 4 4 5: 5 5 Transforming tables from to other formats = table1 &lt;- data.table(matrix(c(1:8), nrow = 4)) as.data.frame(table1) V1 V2 1 1 5 2 2 6 3 3 7 4 4 8 table2 &lt;- data.frame(Column1 = c(1:5), Column2 = c(1:5)) as.data.table(table2) Column1 Column2 1: 1 1 2: 2 2 3: 3 3 4: 4 4 5: 5 5 Binding and setting names = rbind(table, newvariable) cbind(data, newvariable) rownames(table1) &lt;- c(&quot;One&quot;, &quot;Two&quot;, &quot;Three&quot;, &quot;Four&quot;) colnames(table2) &lt;- c(&quot;One&quot;, &quot;Two&quot;) Changing the order of a frequency table and factor value &lt;- c(&quot;one&quot;, &quot;three&quot;, &quot;five&quot;, &quot;one&quot;, &quot;two&quot;, &quot;three&quot;, &quot;four&quot;, &quot;two&quot;, &quot;three&quot;) table &lt;- as.data.table(table(value)) table value N 1: five 1 2: four 1 3: one 2 4: three 3 5: two 2 table[,`value`:= factor( `value`, levels = c( &quot;one&quot;, &quot;two&quot;, &quot;three&quot;, &quot;four&quot;, &quot;five&quot; ) )] setorder(table, `value`) table value N 1: one 2 2: two 2 3: three 3 4: four 1 5: five 1 Frequencies values &lt;- c(1:10) table(values) values 1 2 3 4 5 6 7 8 9 10 1 1 1 1 1 1 1 1 1 1 prop.table(table(values)) values 1 2 3 4 5 6 7 8 9 10 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 round(prop.table(table(values)) * 100, 2) values 1 2 3 4 5 6 7 8 9 10 10 10 10 10 10 10 10 10 10 10 1.1 Data sets Displaying head or tail of a data set: head(cars) # first 6 rows of the data set speed dist 1 4 2 2 4 10 3 7 4 4 7 22 5 8 16 6 9 10 tail(cars) # last 6 rows of the data set speed dist 45 23 54 46 24 70 47 24 92 48 24 93 49 24 120 50 25 85 Reading excel library(readxl) data &lt;- read.xls(\"data.xlsx\", stringsAsFactors = TRUE) names(cars) # shows the column names of the data set [1] &quot;speed&quot; &quot;dist&quot; attach(cars) # saves the names to be used as variables 1.1.1 Removing infinite + NA values Removing Infinite values Removing NA values Changing Infinite values to NA values variable[is.finite(variable)] [1] 1 2 3 4 5 6 7 8 9 10 variable[is.na(variable)] integer(0) variable[is.infinite(variable)] &lt;- NA 1.1.2 Transforming variable types as.numeric(value) [1] NA NA NA NA NA NA NA NA NA as.character(value) [1] &quot;one&quot; &quot;three&quot; &quot;five&quot; &quot;one&quot; &quot;two&quot; &quot;three&quot; &quot;four&quot; &quot;two&quot; &quot;three&quot; as.factor(value) [1] one three five one two three four two three Levels: five four one three two 1.1.3 Markdown # Putting words in bold: **Word** Result # Putting words in italic: *Word* Result # dashes like this `here` Show up like this: here # dashes like this with the letter r: `r 4+4` Asks r to have inline code. We can see the results here: 8. # \\newpage Will start a new page for example in a pdf document # &gt; # In here we can put a quote # &gt; In here we can put a quote 1.1.4 Setup rmarkdown &amp; code chunks Call Description Warning = TRUE/FALSE Include / exclude warnings Echo = TRUE/FALSE Include / exclude r chunks but show output Include = TRUE/FALSE Run code but do not include in the knitted document Comment = \"\" | Include / exclude ## in output code chunks Message = TRUE/FALSE Includes / excludes message from code out.width=‚Äò100%‚Äô Adjusts size of figure / chart fig.width = Set specific size of figure / chart width fig.height = Set specific size of figure / chart height fig.cap= Adds a title to a figure fig.align= ‚Äòcenter,‚Äô ‚Äòleft,‚Äô ‚Äòright,‚Äô adjust figure / chart at page 1.1.5 Miscellaneous round(0.50, 2) # rounds a value with two decimals [1] 0.5 rep(5,5) #repeats the number 5, 5 times [1] 5 5 5 5 5 describe(variable) vars n mean sd median trimmed mad min max range skew kurtosis se X1 1 10 5.5 3.03 5.5 5.5 3.71 1 10 9 0 -1.56 0.96 fivenum(variable) [1] 1.0 3.0 5.5 8.0 10.0 summary(variable) Min. 1st Qu. Median Mean 3rd Qu. Max. 1.00 3.25 5.50 5.50 7.75 10.00 str(variable) # describing the variable int [1:10] 1 2 3 4 5 6 7 8 9 10 dim(cars) # amount of rows and amount of columns [1] 50 2 1.1.6 Subsetting cars[,1] # subsets by columns [1] 4 4 7 7 8 9 10 10 10 11 11 12 12 12 12 13 13 13 13 14 14 14 14 15 15 [26] 15 16 16 17 17 17 18 18 18 18 19 19 19 20 20 20 20 20 22 23 24 24 24 24 25 cars[1,] # subsets by row speed dist 1 4 2 cars[cars$speed == 4,] # subsets by specific value speed dist 1 4 2 2 4 10 cars[cars$speed &gt; 4,] # subsets over specific value speed dist 3 7 4 4 7 22 5 8 16 6 9 10 7 10 18 8 10 26 9 10 34 10 11 17 11 11 28 12 12 14 13 12 20 14 12 24 15 12 28 16 13 26 17 13 34 18 13 34 19 13 46 20 14 26 21 14 36 22 14 60 23 14 80 24 15 20 25 15 26 26 15 54 27 16 32 28 16 40 29 17 32 30 17 40 31 17 50 32 18 42 33 18 56 34 18 76 35 18 84 36 19 36 37 19 46 38 19 68 39 20 32 40 20 48 41 20 52 42 20 56 43 20 64 44 22 66 45 23 54 46 24 70 47 24 92 48 24 93 49 24 120 50 25 85 cars[cars$speed &lt; 5,] # subsets under specific value speed dist 1 4 2 2 4 10 "],["charts-templates-r.html", "Chapter - 2 Charts templates - R", " Chapter - 2 Charts templates - R library(gridExtra) library(hrbrthemes) library(ggplot2) paletteDani &lt;- c( &quot;#ffa500&quot;, &quot;#DAF7A6&quot;, &quot;#5F7992&quot;, &quot;#69b3a2&quot;, &quot;#ffd561&quot;, &quot;#ee5c42&quot;, &quot;#C8A2C8&quot;, &quot;#5c3170&quot;, &quot;#990000&quot;, &quot;#C70039&quot;, &quot;#34495E&quot;, &quot;#909497&quot;) Several basic options: Pie chart data &lt;- ToothGrowth dani_theme &lt;- theme( axis.title.x = element_blank(), axis.title.y = element_blank(), panel.border = element_blank(), panel.grid = element_blank(), axis.ticks = element_blank(), axis.text.x=element_blank(), legend.title = element_text(face = &quot;bold&quot;), plot.title = element_text(hjust = 0.5, size = 12, face = &quot;bold&quot;) ) ggplot(data, aes(x=&quot;&quot;,y = dose, fill = supp)) + geom_bar(width = 1, stat = &quot;identity&quot;) + coord_polar(&quot;y&quot;, start = 0) + scale_fill_manual(&quot;Legendname:&quot;, values = paletteDani) + dani_theme + labs(title = &quot;Title&quot;, x = &quot;variablX&quot;, y = &quot;variableY&quot; ) Bar chart ggplot(data, aes(x = dose, y = supp)) + geom_bar(stat = &quot;identity&quot;, fill=&quot;#69b3a2&quot;, color=&quot;#e9ecef&quot;) + theme(legend.position=&quot;none&quot;) Histogram ggplot(data = data, aes(len) ) + geom_histogram(fill=&quot;#69b3a2&quot;, color=&quot;#e9ecef&quot;, alpha=0.9) + ggtitle(&quot;Title&quot;) + xlab(&quot;variablex&quot;) + ylab(&quot;variabley&quot;) + theme(plot.title = element_text(size = 11)) `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Boxplot ggplot(data=ToothGrowth, aes(x=&quot;&quot;, y=len, fill=&quot;&quot;)) + geom_boxplot(fill=&quot;#69b3a2&quot;, outlier.colour=&quot;red&quot;, outlier.shape=8, outlier.size=4) + theme_ipsum() + theme( legend.position=&quot;none&quot;, plot.title = element_text(size=12) ) + ggtitle(&quot;Title&quot;) + theme(plot.title = element_text(hjust = 0.5)) + xlab(&quot;&quot;)+ ylab(&quot;&quot;) Scatter plot ggplot(data= ToothGrowth, aes(y = len, x = dose)) + geom_point(size=2) + geom_smooth(method=&quot;lm&quot;, color=&quot;#69b3a2&quot;, fullrange=TRUE, formula = y ~ x) + theme(plot.title = element_text(hjust = 0.5)) + labs(title = &quot;Title&quot;, y = &quot;yname&quot;, x = &quot;xname&quot; ) Scatter plot with dummies ggplot(data = ToothGrowth, aes(y = len, x = supp, colour=factor(supp))) + geom_point(size=2) + geom_smooth(method=&quot;lm&quot;, fill = NA, fullrange=TRUE, formula = y ~ x) + theme(plot.title = element_text(hjust = 0.5)) + scale_colour_manual(name=&quot;Legendtitle&quot;, labels=c(&quot;value1&quot;, &quot;value2&quot;),values = c(&quot;#69b3a2&quot;, &quot;#F6726A&quot;))+ labs(title = &quot;Title&quot;, y = &quot;Yname&quot;, x = &quot;Xname&quot; ) Arrange charts next to each other on a page chart1 &lt;- ggplot(data=ToothGrowth, aes(x=&quot;&quot;, y=len, fill=&quot;&quot;)) + geom_boxplot(fill=&quot;#69b3a2&quot;, outlier.colour=&quot;red&quot;, outlier.shape=8, outlier.size=4) + theme_ipsum() + theme( legend.position=&quot;none&quot;, plot.title = element_text(size=12) ) + ggtitle(&quot;Title&quot;) + theme(plot.title = element_text(hjust = 0.5)) + xlab(&quot;&quot;)+ ylab(&quot;&quot;) chart2 &lt;- ggplot(data=ToothGrowth, aes(x=&quot;&quot;, y=len, fill=&quot;&quot;)) + geom_boxplot(fill=&quot;#69b3a2&quot;, outlier.colour=&quot;red&quot;, outlier.shape=8, outlier.size=4) + theme_ipsum() + theme( legend.position=&quot;none&quot;, plot.title = element_text(size=12) ) + ggtitle(&quot;Title&quot;) + theme(plot.title = element_text(hjust = 0.5)) + xlab(&quot;&quot;)+ ylab(&quot;&quot;) grid.arrange(chart1, chart2, nrow=1, widths=c(0.9,1)) Shows the amounts of missing values (NA) in a data set library(VIM) aggr(cars, numbers = TRUE, prop = c(TRUE, FALSE), cex.axis = 0.5) Density plots with semi-transparent fill ggplot(data = ToothGrowth, aes(x = len, fill = supp)) + geom_density(alpha=.3) + theme( plot.title = element_text(size=15) ) + ggtitle(&quot;Density plot&quot;) + theme(plot.title = element_text(hjust = 0.5, face= &quot;bold&quot;)) + xlab(&quot;&quot;)+ ylab(&quot;Density&quot;) Correlation matrix - pairs panel library(psych) pairs.panels(iris, method = &quot;pearson&quot;, hist.col = &quot;#00AFBB&quot;, density = TRUE, ellipses = TRUE ) Simple plots: model &lt;- lm(dist ~ speed, data = cars) plot(model) plot(model, 4) # cook distance library(car) avPlots(model) "],["probability.html", "Chapter - 3 Probability 3.1 Bayes Theorem 3.2 Discrete Probablity 3.3 Samples, estimation &amp; confidence intervals 3.4 Significance level 3.5 Non-Parametric testing", " Chapter - 3 Probability library(prob) library(LaplacesDemon) # Bayes Theorem library(BSDA) #tsumtest library(actuar) out &lt;- c(&quot;Red&quot;, &quot;White&quot;, &quot;Black&quot;, &quot;Blue&quot;, &quot;Green&quot;) freq &lt;- c(1,2,3,4,5) s &lt;- probspace(out, probs = freq) print(s) x probs 1 Red 0.06666667 2 White 0.13333333 3 Black 0.20000000 4 Blue 0.26666667 5 Green 0.33333333 If you toss two fair coins, what is the probability of two heads? space &lt;- tosscoin(2, makespace = TRUE) p &lt;- Prob(space, toss1 == &quot;H&quot; &amp; toss2 == &quot;H&quot;) The probability is: 0.25 When two dice are thrown, what is the probability of a 3 followed by a 5? space &lt;- rolldie(2, makespace = TRUE) p &lt;- Prob(space, X1 == 3 &amp; (X2 == 5) ) The probability is: 0.03 Sampling from an urn with or without replacement. 3 balls and sample size of 2: sample1 &lt;- urnsamples(1:3, size = 2, replace = TRUE, ordered = TRUE) sample2 &lt;- urnsamples(1:3, size = 2, replace = FALSE, ordered = TRUE) sample3 &lt;- urnsamples(1:3, size = 2, replace = FALSE, ordered = FALSE) sample4 &lt;- urnsamples(1:3, size = 2, replace = TRUE, ordered = FALSE) 3.1 Bayes Theorem Unconditional probability: P(S) and P(NS) Success or no success prS &lt;- c(0.4, 0.6) Conditional probability: P(P | S ) and P( P | NS) Predicted given it is successful Predicted given it is not successful prNS &lt;- c(0.6, 0.2) Bayes prob, posterior probabilities P(S | P) &amp; P(NS | P) BayesTheorem(prS, prNS) [1] 0.6666667 0.3333333 attr(,&quot;class&quot;) [1] &quot;bayestheorem&quot; 3.2 Discrete Probablity 3.2.1 Uniform discrete probability distribution Sample space with a set probability. Size = amount of tries Density function: Individual probability. F.E. Getting a 4 Cumulative density: Uniform for a certain value distribution. F.E. 4 or less. 4 or more? 1-punif 3 Inverse cumulative density: Uniform for a certain probability ( up until a certain value). F.E. up to 25% of the tries Default = # or less. For # or more do: 1-probability of # or less 3.2.2 Binomial distribution Binomial for a specific value for a certain sample. F.E. 2 from the sample are successful. Binomial for a certain distribution of the sample. F.E. At most 2 in the sample are successful. Or 5 or more. Binomial for a certain percentage of the sample. F.E. 25% of the sample has x value or less. Difference between two binomial values. F.E. Prob there are between 4 and 5 of the trials successful. one &lt;- dbinom(x, size = n, prob = y) two &lt;- pbinom(x, size = n, prob = y) three &lt;- qbinom(p, size = n, prob = y) four &lt;- diff(pbinom(c(X,Y), size = n, prob = y)) Default = # or less (left area of the distribution). For # or more do: 1-probability of # or less 3.2.3 Poisson distribution Expected value = \\(n * p = LAMDA\\) Poisson for a certain value. Lambda = n*p. F.E. Prob of having a 5 Poisson for a certain value distribution. F.E. Prob of having less than 5. More than 5? = 1- Ppois(4, lambda) Poisson for a certain probability to capture a certain value. F.E. Poisson value for 25%. one &lt;- dpois(x,lambda) two &lt;- ppois(x,lambda) three &lt;- qpois(x,lambda) Default = # or less (left area of the distribution). For # or more do: 1-probability of # or less 3.2.4 The normal distribution Empirical rule For all normal distributions: 68-95-99.7 rule 99.7% of observations are located between: -3 mu and 3 95% of observations are located between: -2 mu 2 68% of observations are located between: -1 mu 1 Normal distribution Z-value # z &lt;- (x - mean) / sd. For example: (11 - 10) / 2 [1] 0.5 Normal distribution for a certain proportion. Pi = population proportion mean%. Normal distribution for a certain value distribution. F.E. Prob of value above 5. FALSE Prob less than 9. TRUE Normal distribution for a certain probability to capture a certain value. F.E. Value that is given at 25% point. Difference between two values on the normal distribution. F.E. between 5 and 10. one &lt;- pnorm(X, pi, sd, lower.tail = FALSE) two &lt;- pnorm(X, mu, sd, lower.tail = FALSE) three &lt;- qnorm(p, mu, sd, lower.tail = FALSE) four &lt;- diff(pnorm(c(X,Y), mu, sd, lower.tail = FALSE)) lower.tail = TRUE: The area of the left side of the slope lower.tail = FALSE: The area of the right side of the slope Confidence interval for normal distribution z.test(x, sd=sigma) binconf(x = x, n = n) &lt;- proportions t.test(variable) &lt;- t-distribution for conf.inv 3.2.4.1 Plotting the normal distribution \"With mean = 3 and standard deviation = 7 Limits: mean +/- 3 * standard deviation = 3*7 = 21 Lower limit = 3 ‚Äì 21 = -18 Upper limit = 3 + 21 = 24\" Example: x &lt;- seq(15, 45, length=50) y &lt;- dnorm(x, 30, 5) plot(x,y,type=&quot;l&quot;,lwd=2,col=&quot;black&quot;) x &lt;- seq(15,35, length=100) y &lt;- dnorm(x, 30,5 ) polygon(c(15,x,35),c(0,y,0), density = c(15, 35), col = &quot;black&quot;) p &lt;- pnorm(35, mean = 30, sd = 5,lower.tail = TRUE) text(0,0.15,&quot;68%&quot;) 3.2.4.2 Binomial It will be possible to use the Normal distribution as an approximation to the Binomial if: n is large and p &gt; 0.1 Density function (individual probability). Cumulative density (between certain values). Difference between two binomial values Inverse cumulative density. For a certain prob. one &lt;- dbinom(x, mu, sd) two &lt;- pbinom(x, mu, sd, lower.tail = FALSE) three &lt;- diff(pbinom(c(X,Y), mu, sd, lower.tail = FALSE)) four &lt;- qbinom(p, mu, sd, lower.tail = FALSE) 3.3 Samples, estimation &amp; confidence intervals The standard error of the sampling distribution of the mean se &lt;- sigma / sqrt(n) Probability sample To find the probability that X is larger than mu To find the probability that X is smaller than mu p &lt;- pnorm(X, mu, se, lower.tail = TRUE) p &lt;- pnorm(X, mu, se, lower.tail = FALSE) Probability proportions sample sd &lt;- sqrt((pi*(n-pi))/n) z &lt;- (p - pi)/sd p &lt;- pnorm(X, pi, se, lower.tail =FALSE) Sample size Package = ‚Äúsamplingbook.‚Äù Provides the sample size needed to have a 95% confidence to estimate the population mean. Level = confidence level. Se is required standard error. sample.size.mean(se, sigma, level=0.95) 3.4 Significance level 3.4.1 Critical values Critical value for normal distribution, sample &gt; 30 Two-sided: Critical value, 5% significance level = 1.96 Two-sided: Critical value, 1% significance level = 2.58 Two-sided: Critical value, 10% significance level = 1.96 One-sided: Critical value, 5% significance level = 1.64 One-sided: Critical value, 1% significance level = 2.33 One-sided: Critical value, 10% significance level = 1.28 qnorm(0.975) [1] 1.959964 qnorm(0.995) [1] 2.575829 qnorm(0.95) [1] 1.644854 qnorm(0.95) [1] 1.644854 qnorm(0.99) [1] 2.326348 qnorm(0.90) [1] 1.281552 Critical values t-distribution One-sided: critical value at a 5% significance level One-sided: critical value at a 10% significance level One-sided: critical value at a 1% significance level Two-sided: critical value at a 5% significance level Two-sided: critical value at a 10% significance level Two-sided: critical value at a 1% significance level cv &lt;- qt(0.95, df) cv &lt;- qt(0.90, df) cv &lt;- qt(0.99, df) cv &lt;- qt(0.975, df) cv &lt;- qt(0.95, df) cv &lt;- qt(0.995, df) Confidence interval cv &lt;- cv mu &lt;- mu sd &lt;- sd se &lt;- sd / (sqrt(n)) n &lt;- n conf_int95 &lt;- cv * sd / (sqrt(n)) mu_plus &lt;- mu + conf_int95 mu_min &lt;- mu - conf_int95 Large sample significance testing Two-sided One-sided: X is greater than the population mean One-sided: X is less than the population mean library(BSDA) one &lt;- tsum.test(mean.x = X, s.x = sd, n.x = n, mu = mu, alternative = &quot;two.sided&quot;, var.equal = TRUE) two &lt;- tsum.test(mean.x = X, s.x = sd, n.x = n, mu = mu, alternative = &quot;greater&quot;, var.equal = TRUE) three &lt;- tsum.test(mean.x = X, s.x = sd, n.x = n, mu = mu, alternative = &quot;less&quot;, var.equal = TRUE) For proportions: prop.test(x = X, n = n, p = p, correct = TRUE, alternative = ‚Äútwo.sided‚Äù) Same goes for above: two.sided, greater, less 3.4.2 Test of equality - two samples H_0 &lt;- \\(\\mu1 = \\mu2\\) or \\((\\mu1 - \\mu2) = 0\\) H_a &lt;- \\(\\mu1 \\neq \\mu2\\) or \\(\\mu1 - \\mu2 \\neq 0\\) Difference in two means with a certain confidence level confidence interval. Default = 95% tsum.test(mean.x = X, s.x = sd, n.x = n, mean.y = X, s.y = sd, n.y = n, var.equal=FALSE) Welch Modified Two-Sample t-Test data: Summarized x and y t = 0, df = 58, p-value = 1 alternative hypothesis: true difference in means is not equal to 0 95 percent confidence interval: -0.8492568 0.8492568 sample estimates: mean of x mean of y 15 15 2-sample test for equality of proportions without continuity correction. prop.test(data, correct=FALSE, alternative=‚Äúgreater‚Äù) 3.5 Non-Parametric testing 3.5.1 Contengency table / frequencies Obtain contingency table table(ToothGrowth$dose) 0.5 1 2 20 20 20 3.5.2 Chi-square Chi-square test Get the expected value Probability for chi-square data &lt;- matrix(c(27,373,33,567),byrow=TRUE,nrow=2) chisq.test(data,correct=FALSE) Pearson&#39;s Chi-squared test data: data X-squared = 0.66489, df = 1, p-value = 0.4148 chisq.test(data,correct=FALSE)$expected [,1] [,2] [1,] 24 376 [2,] 36 564 prop.table(chisq.test(data,correct=FALSE)$expected,1) [,1] [,2] [1,] 0.06 0.94 [2,] 0.06 0.94 prop.table(chisq.test(data,correct=FALSE)$expected,2) [,1] [,2] [1,] 0.4 0.4 [2,] 0.6 0.6 Degree of freedom = # of row - 1 * # of columns = fixed All expected frequencies must be above five! If not, categories must be combined! 3.5.3 Goodness of fit Uniform: Degree of freedom = number of categories - number of parameters - 1. x &lt;- c(1,2,3,4,5) p &lt;- rep(1/5, 5) chisq.test(x, p = p) Chi-squared test for given probabilities data: x X-squared = 3.3333, df = 4, p-value = 0.5037 All expected frequencies must be above five! If not, categories must be combined! Binomial: dbinom(x, size = n, prob = y) For example: library(actuar) cj &lt;- c(-0.5, 0.5, 1.5, 2.5, 3.5, 4.5, 5.5) #or cj &lt;- seq(from = -0.5, to=5, by=1) nj &lt;- c(15,20,20,18,13) data &lt;- grouped.data(Group = cj, Frequency = nj) p &lt;- mean(data)/5 pr &lt;-c(dbinom(0,5,p),dbinom(1,5,p),dbinom(2,5,p),dbinom(3,5,p),dbinom(4,5,p),dbinom(5,5,p)) nj2 &lt;- c(35,20,18,23) pr2 &lt;- c(dbinom(0,5,p)+dbinom(1,5,p),dbinom(2,5,p),dbinom(3,5,p),dbinom(4,5,p)+dbinom(5,5,p)) chisq.test(nj2,p=pr2) Chi-squared test for given probabilities data: nj2 X-squared = 38.736, df = 3, p-value = 0.00000001975 All expected frequencies must be above five! If not, categories must be combined! Poisson Degree of freedom = number of categories - number of parameters - 1. NOTE! Distribution goes to infinity. Counter for one value that is X or more. 1 - until X. Example: cj &lt;- c(-0.5, 0.5, 1.5, 2.5, 3.5, 4.5, 5.5) #or cj &lt;- seq(from = -0.5, to=6, by=1) nj &lt;- c(16, 30, 37, 7, 10, 5) data &lt;- grouped.data(Group = cj, Frequency = nj) m &lt;- mean(data) pr &lt;- c(dpois(0, m),dpois(1,m),dpois(2, m), dpois(3, m), dpois(4, m), + (1-ppois(4,m)) ) chisq.test(nj, p = pr) Chi-squared test for given probabilities data: nj X-squared = 9.7845, df = 5, p-value = 0.08158 Normal distribution Example: cv &lt;- qchisq(0.90, 2) cj &lt;- c(0, 1, 3, 10, 15, 30) nj &lt;- c(16, 30, 37, 7, 10) data &lt;- grouped.data(Group = cj, Frequency = nj) m &lt;- mean(data) s &lt;- sqrt(emm(data,2)) pr &lt;- c(pnorm(1,m,s), diff(pnorm(c(1,3),m,s)), diff(pnorm(c(3,10),m,s)), diff(pnorm(c(10,15),m,s)), 1 - pnorm(c(15),m,s) ) chisq.test(nj,p=pr) Chi-squared test for given probabilities data: nj X-squared = 77.503, df = 4, p-value = 0.0000000000000005887 ###Mann-whitney test N = Number of pairs - number of draws For small tests c1 values sample 1 c2 values sample 2 wilcox.text(x, c2) Larger sample test &gt; 10 You can use a approximation based on the normal distribution. Therefore critical values will be 1.96 for this two sided test. ###Wilcoxon test Two options - Do not predict direction ‚Äì&gt; two sided - Predict direction ‚Äì&gt; one sided wilcox.test(w1, w2, paired=TRUE,correct=FALSE) ###Run test library(randtests) pers &lt;- c(0,1,1,0,0,0,0,1,1,0,1) pers.f &lt;- factor(pers,labels=c(&quot;Male&quot;,&quot;Female&quot;)) runs.test(pers) Runs Test data: pers statistic = NaN, runs = 1, n1 = 5, n2 = 0, n = 5, p-value = NA alternative hypothesis: nonrandomness 3.5.4 P-value Find p value: Probability of getting this test statistic or more: pchisq(ts, df, lower.tail=FALSE) [1] 0.5578254 "],["simple-regressions.html", "Chapter - 4 Simple regressions 4.1 Basics regressions 4.2 Prediction 4.3 Data problems", " Chapter - 4 Simple regressions 4.1 Basics regressions Regressions, correlation and dummy‚Äôs Y = Dependent X = Explanatory Correlation cor(x, y) [1] 0.8068949 Creating the regression: To plot the regression model Evaluates the coefficient of the model Only the first colum estimattion model &lt;- lm(y~x, data = data) summary(model)$coef Estimate Std. Error t value Pr(&gt;|t|) (Intercept) -17.579095 6.7584402 -2.601058 0.012318816153809090 x 3.932409 0.4155128 9.463990 0.000000000001489836 est &lt;- summary(model)$coef[,1] 4.1.1 Summarizing regressions: Using stargazer package library(stargazer) stargazer(lm(y~x, data=data), type=&quot;text&quot;) =============================================== Dependent variable: --------------------------- y ----------------------------------------------- x 3.932*** (0.416) Constant -17.579** (6.758) ----------------------------------------------- Observations 50 R2 0.651 Adjusted R2 0.644 Residual Std. Error 15.380 (df = 48) F Statistic 89.567*** (df = 1; 48) =============================================== Note: *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01 # Multiple models adjacent model1 &lt;- lm(y~x, data=data) model2 &lt;- lm(x~y, data=data) stargazer(model1, model2, type=&quot;text&quot;) ========================================================== Dependent variable: ---------------------------- y x (1) (2) ---------------------------------------------------------- x 3.932*** (0.416) y 0.166*** (0.017) Constant -17.579** 8.284*** (6.758) (0.874) ---------------------------------------------------------- Observations 50 50 R2 0.651 0.651 Adjusted R2 0.644 0.644 Residual Std. Error (df = 48) 15.380 3.156 F Statistic (df = 1; 48) 89.567*** 89.567*** ========================================================== Note: *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01 Using summary function: summary(lm(y~x)) Call: lm(formula = y ~ x) Residuals: Min 1Q Median 3Q Max -29.069 -9.525 -2.272 9.215 43.201 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) -17.5791 6.7584 -2.601 0.0123 * x 3.9324 0.4155 9.464 0.00000000000149 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 15.38 on 48 degrees of freedom Multiple R-squared: 0.6511, Adjusted R-squared: 0.6438 F-statistic: 89.57 on 1 and 48 DF, p-value: 0.00000000000149 Regressions Plotting regression plot(y~x,data=data, main=&quot;Title&quot;, ylab=&quot;yname&quot;, xlab=&quot;xname&quot; ) Including regression line: plot(y~x,data=data, main=&quot;Title&quot;, ylab=&quot;yname&quot;, xlab=&quot;xname&quot; ) abline(lm(y~x, data=data), col=&quot;blue&quot;) Confidence interval around slope confint(lm(y~x), level=0.95) 2.5 % 97.5 % (Intercept) -31.167850 -3.990340 x 3.096964 4.767853 Sub-sampling regression Specify dimensions [,]. First is row. Column, second. Selects the rows where age is larger than 5. Lower than 5. sub1 &lt;- summary(lm(y~x, data=data[&quot;speed&quot;&gt;=5,])) sub2 &lt;- summary(lm(y~x, data=data[&quot;speed&quot;&lt;=5,])) 4.1.2 Dummy variables, diff in means 4.1.3 Regression + dummy Y = Constant0 + B0 * X - Diff in means + B1 * variable1*2 Omitting the intercept: Shows the means separately and not the difference between means. Tests whether the expected counts are different from zero. lm(y ~ x - 1, data = data) Call: lm(formula = y ~ x - 1, data = data) Coefficients: x 2.909 Reorders group, to specific value to be first. variable2 &lt;- relevel(variable, ‚ÄúC‚Äù) 4.2 Prediction model &lt;- lm(y~x) newdata &lt;- data.frame(variablename = c(1:50)) pred &lt;- predict(model, newdata = newdata) Prediction confidence interval: One value Multiple values from a existing data frame pred1 &lt;- predict(model, data.frame(valuename = x), interval = &quot;confidence&quot;, level=0.95) pred2 &lt;- predict(model, newdata = newdata, interval = &quot;confidence&quot;, level=0.95) Prediction interval One value Multiple values from a existing data frame pred1 &lt;- predict(model, data.frame(valuename = x), interval=&quot;predict&quot;,level=0.95) pred2 &lt;- predict(model, newdata, interval=&quot;predict&quot;,level=0.95) 4.2.1 Confidence and prediction plotting Adds: observed values, fitted line, conf interval, predicted interval library(HH) fit &lt;- lm(y~x, data = data) ci.plot(fit) 4.2.2 Prediction with dummy variables Prediction = ùõº1+ùõº2Constant Dummy+ùõΩ1ùëÜùëñùëßùëí+ùõΩ2Slope Dummy 4.2.3 Prediction intervals examples Prediction fit &lt;- lm(y ~ x + d + d, data = data) pred &lt;- predict(fit, data.frame(VariableX = c(10), Dummy = c(1), Slopedummy = c(10)) ) Confidence interval prediction fit &lt;- lm(y ~ x + d + d, data = data) pred &lt;- predict(fit, data.frame(VariableX = c(10), Dummy = c(1), Slopedummy = c(10), interval=&quot;confidence&quot;)) Prediction interval fit &lt;- lm(y ~ x + d + d, data = data) pred &lt;- predict(fit, data.frame(VariableX = c(10), Dummy = c(1), Slopedummy = c(10), interval=&quot;predict&quot;)) 4.3 Data problems Residual plot # residual.plots(fitted(fit), resid(fit), sigma.hat(fit), main=&quot;Title&quot;) Influential measure test im &lt;- influence.measures(fit) 4.3.1 Multicollinearity F-test fit &lt;- lm(y~x + d, data = data) anova(fit) Analysis of Variance Table Response: y Df Sum Sq Mean Sq F value Pr(&gt;F) x 1 21186 21185.5 89.567 0.00000000000149 *** Residuals 48 11354 236.5 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 4.3.2 Variance inflation factors The variance inflation factor (vif) is \\(1 / 1‚àíR2\\). A simple approach to identify collinearity among explanatory variables is the use of variance inflation factors (VIF). It is the ratio of variance in a model with multiple terms, divided by the variance of a model with one term alone. The higher the value, the higher the collinearity. A VIF for a single explanatory variable is obtained using the r-squared value of the regression of that variable against all other explanatory variables: A VIF is calculated for each explanatory variable and those with high values are removed. The definition of ‚Äòhigh‚Äô is somewhat arbitrary, but values in the range of 5-10 are commonly used for ‚Äòhigh.‚Äô If VIF value exceeding 4.0, or by tolerance less than 0.2 then there is a problem with multicollinearity (Hair et al., 2010). However, it depends on the researcher‚Äôs criteria. The lower the vif the better, but you shouldn‚Äôt be too concerned as long as your VIF is not greater than 10. vif(fit) x d 1 NaN 4.3.3 ANOVA One-way: one value res.aov &lt;- aov(y ~ x, data = data) summary(res.aov) Df Sum Sq Mean Sq F value Pr(&gt;F) x 1 21185 21185 89.57 0.00000000000149 *** Residuals 48 11354 237 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Two-way: more than two factors res.aov &lt;- aov(y ~ x + d, data = data) summary(res.aov) Df Sum Sq Mean Sq F value Pr(&gt;F) x 1 21185 21185 89.57 0.00000000000149 *** Residuals 48 11354 237 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 With interaction res.aov &lt;- aov(y ~ x * d, data = data) summary(res.aov) Df Sum Sq Mean Sq F value Pr(&gt;F) x 1 21185 21185 89.57 0.00000000000149 *** Residuals 48 11354 237 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Three-way Three way With interaction summary(aov(y ~ x + d, data=data)) Df Sum Sq Mean Sq F value Pr(&gt;F) x 1 21185 21185 89.57 0.00000000000149 *** Residuals 48 11354 237 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 summary(aov(y ~ x + d, data=data)) Df Sum Sq Mean Sq F value Pr(&gt;F) x 1 21185 21185 89.57 0.00000000000149 *** Residuals 48 11354 237 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 MANOVA: Multiple factors Test in difference Test separately test_manova &lt;- manova(cbind(y, d) ~ x, data = data) summary(test_manova) summary.aov(test_manova) 4.3.4 Linearizing variables logged &lt;- log(iris$Sepal.Length) # non-linear quad &lt;- cars$speed ^ 2 # quadratic "],["structure-equation-models.html", "Chapter - 5 Structure equation models 5.1 Path analysis (structural equations) 5.2 Coding the model 5.3 Factor model", " Chapter - 5 Structure equation models Structural Equation Modeling (SEM): is an extremely broad and flexible framework for data analysis, perhaps better thought of as a family of related methods rather than as a single technique. Measuring latent constructs is challenging and we must also incorporate estimates of measurement error into our models. SEM excels at both of these tasks. SEM is especially suited for causal analysis.(Gray 2017) VS Ordinary least-squares (OLS): models assume that the analyst is fitting a model of a relationship between one or more explanatory variables and a continuous or at least interval outcome variable that minimizes the sum of square errors, where an error is the difference between the actual and the predicted value of the outcome variable. The most common analytic method that utilizes OLS models is linear regression (with a single or multiple predictor variables). Latent variables = Unobserved variables (or unmeasured variables in SEM lingo). These are theoretical concepts which can be inferred but not directly measured. Linear regression model = \\(Y = alpha + betaX + error\\) The model has to account for randomization. If there is a factor that that cannot be explained, it is included in the error. When this unmeasured factor that is in the error is correlated to another independent variable, there is endogeneity. Endogeneity variables = correlated with the error terms. Arises when the marginal distribution of the independent variable is not independent of the conditional distribution of the dependent variable given the independent. Exogenous variables = not driven by other factors (observable or observable) Sources of endogeneity = 1. Omitted variables: relevant variables left out of the model, attributing to effect to those that were included. 2. Simultaneity: where x causes y and y causes x 3. Selection bias: sampling bias In the structural equation model we can effectively avoid endogeneity. 5.1 Path analysis (structural equations) Path diagrams = Communicates the structure of our model. Useful for structural equation models. The objects in the model mean: Rectangular = any variable that we can observe directly (observed variable), measured variables Circle / Ovals = Cannot be observed (Latent variable) Arrow = directed effect. One variable impacts the other. Hypothetical causal relationship. Numbers by the arrows = regression coefficient. Triangle is the constant in the Linear Model. Double handed arrows = indicate co-variances or correlations without a causal interpretation. Double handed arrow between two independent variables = they are correlated to each other. Residual error term = measurement errors. We expect that the factor will not perfectly predict the observed variables. The bi-directed arrows, the ones with two side arrows (in this representation, a connecting line with no arrows) represent co-variances among variables. 5.2 Coding the model library(semPlot) library(lavaan) Setting up the model and summarizing fit &lt;- &#39;dist ~ speed&#39; model &lt;- lavaan(fit, data = cars, estimator=&quot;MLM&quot;, auto.var = TRUE) summary(model) lavaan 0.6-8 ended normally after 15 iterations Estimator ML Optimization method NLMINB Number of model parameters 2 Number of observations 50 Model Test User Model: Standard Robust Test Statistic 0.000 0.000 Degrees of freedom 0 0 Parameter Estimates: Standard errors Robust.sem Information Expected Information saturated (h1) model Structured Regressions: Estimate Std.Err z-value P(&gt;|z|) dist ~ speed 3.932 0.399 9.864 0.000 Variances: Estimate Std.Err z-value P(&gt;|z|) .dist 227.070 54.619 4.157 0.000 Estimator = MLM in the model: protects for non-linearity, non-normality and elasticity of the raw data. This needs the raw data, not only the covariance matrix. Setting up the path diagram semPaths(model, &quot;std&quot;, title = FALSE, weighted = FALSE, sizeInt = 4, sizeMan = 5, edge.label.cex = 1.3, asize = 2) title(&quot;SEM path&quot;, line = 3) ‚Äústd‚Äù = standardizing the variables ‚Äúest‚Äù = true estimation style=‚Äúram‚Äù = to get circles around the measurement arrows With latent variable &amp; SEM model &lt;- &quot; # regression Petal =~ Petal.Width + Petal.Length Sepal.Length ~ Sepal.Width + Petal &quot; fit &lt;- sem(model, data = iris, sample.cov = S, sample.nobs = 122) semPaths(fit, &quot;std&quot;, sizeInt = 4, sizeMan = 3, edge.label.cex = 1, asize=3, weighted=TRUE, exoCov = TRUE) Testing the model fitMeasures(fit)[ c(&quot;chisq&quot;,&quot;df&quot;, &quot;pvalue&quot; ,&quot;rmsea&quot;)] chisq df pvalue rmsea 35.94875570001079 2.00000000000000 0.00000001562525 0.33639637185920 Chisq is a chi-squared test statistic for. H0: moment restrictions implied by the model hold true The degrees of freedom P-value of the chi-square test Rmsea is a fit index, the root mean square of approximation. It should be small for a good fit of the model. Threshold of .05 is often applied to declare good fit. See later chapter for further explanation Modification indices This tells us how we could improve your model. If i liberate one parameter, then _ would change. modi = modindices(fit) modi[order(modi[,4], decreasing=T), ] lhs op rhs mi epc sepc.lv sepc.all sepc.nox 13 Sepal.Width ~ Sepal.Length 30.061 -0.229 -0.229 -0.499 -0.499 14 Sepal.Width ~ Petal 30.061 -0.263 -0.189 -0.435 -0.435 16 Petal ~ Sepal.Width 30.061 -0.719 -1.002 -0.435 -1.002 15 Petal ~ Sepal.Length 30.061 -1.104 -1.539 -1.456 -1.456 Parameters fit parameterestimates(fit, standardized = TRUE, rsquare=TRUE, ci=FALSE)[1:4,] # showing the top 4 lhs op rhs est se z pvalue std.lv std.all std.nox 1 Petal =~ Petal.Width 1.000 0.000 NA NA 0.717 0.944 0.944 2 Petal =~ Petal.Length 2.500 0.065 38.745 0 1.794 1.020 1.020 3 Sepal.Length ~ Sepal.Width 0.651 0.058 11.158 0 0.651 0.299 0.688 4 Sepal.Length ~ Petal 1.149 0.052 22.015 0 0.824 0.871 0.871 + 5.2.1 Covariance Covariance is a measure of how much two random variables vary together. It‚Äôs similar to variance, but where variance tells you how a single variable varies, co variance tells you how two variables vary together. Cov(1, 2) = \\(sum (1- mean(1) * (2-mean(2) / n\\) Moment matrix: Covariance matrix: cov(cars) speed dist speed 27.95918 109.9469 dist 109.94694 664.0608 Creating the covariance matrix when only having the lower half: lower &lt;- &quot; 0.03300863 0.15894229 5.0185561 0.15670560 0.9841531 1.2142232 &quot; S &lt;- getCov(lower, names = c(&quot;variable1&quot;, &quot;variable2&quot;, &quot;variable3&quot;)) print(S) variable1 variable2 variable3 variable1 0.03300863 0.1589423 0.1567056 variable2 0.15894229 5.0185561 0.9841531 variable3 0.15670560 0.9841531 1.2142232 5.2.2 Reliability In statistics reliability is the consistency of a set of measurements or measuring instrument, often used to describe a test. This can either be whether the measurements of the same instrument give or are likely to give the same measurement (test-retest), or in the case of more subjective instruments, such as personality or trait inventories, whether two independent assessors give similar scores (inter-rater-reliability). Reliability is inversely related to random error. In words, reliability is defined as a proportion of observed variance that is true variance. Reliability is interpreted as a proportion and therefore cannot be negative. Various kinds of reliability coefficients, with values ranging between 0.00 (much error) and 1.00 (no error), are usually used to indicate the amount of error in the scores. The reliability is expressed as k and there are several options to calculate: Option 1: reliability of a latent variable 1 - Measurement error variable 1 / observed variable 2 (latent variable). Dividing the true variance by the observed variance. Here is an example: Option 2: based on the standardized path diagram The square of the standardized loading is the reliability of the variable. Example: Here the reliability of the variable distance (dst) = \\(0.35 ^ 2 = 0.12 = k\\) ____________________________________________________ Testing the model fitMeasures(fit) [ c(\"chisq\", \"df\", pvalue\", \"rmsea\")] Model chi-square test = We test whether the fitted model is correct. HO: moment restrictions implied by the model hold. The fit is correct If &gt; 0.05 we cannot reject the model. I accept the model. Therefore, the chi-square test allows researchers to evaluate the fitness of a model by using the null hypothesis significance test approach. Degrees of freedom (df) = Number of observations available for model estimation - Number of observations used to estimate parameters. ‚ÄúNumber of free parameters‚Äù refers to all of the things that this model estimated freely. Parameter is a regression coefficient when standardized is called a beta coefficient. The Root Mean Square Error of Approximation (RMSEA) = fit index: how the covariance fit in the model. Difference between observed and the fitted. The RMSEA is widely used in Structural Equation Modeling to provide a mechanism for adjusting for sample size where chi-square statistics are used. Measures the discrepancy due to the approximation per degree of freedom. The objective is to have the RMSEA as low as possible. 5.3 Factor model Factor analysis: a statistical method used to describe variability among observed, correlated variable in terms of a potentially lower number of unobserved variables called factors. For example, it is possible that variations in six observed variables mainly reflect the variations in two unobserved (underlying) variables. Factor analysis searches for such joint variations in response to unobserved latent variables. The observed variables are modeled as linear combinations of the potential factors, plus ‚Äúerror‚Äù terms. It is including a common return that has a impact to multiple variables. Then, you can find how much variance is due to permanent and depended on these variables. Below we can find the factor model including means based on the Spearman model from 1903. Difference in regression equations: Basic model - Y, X are centered : \\(Y = beta X + error\\) Y, X NOT centered - including constant: \\(Y = alpha*1 + beta X + error\\) Factor model: \\(Y = lambda*F + error1\\) Lamba = the weights F = common factor Error = Specific to the factors Here instead of setting the alpha to 1 which we do in the basic model, now we add a weight which is the lambda. When the data is standardized, beta becomes the standardized beta coefficient. Because y, x are centered, you do not need to present the constant in the model. Next step in factor models with simultaneous equations ‚Äì&gt; ML estimation of a general model. Linear structural relations You have to fix the variance of a variable that you do not observe. If you do not do this, the model is not identified. Meaning there is no minimum. Including latent variables two options: A) Var(F) = 1, F Standardized (F typically has mean zero) B) Lambda_1 = 1 They are equivalent regarding degrees of freedom or model fit. When you put 1* variable1, you force the true beta to be 1. In r setting the model = #regression equation option 1 Model &lt;- &quot;dist =~ 1 * speed &quot; #regression equation option 2 Model &lt;- &quot;dist =~ NA * speed &quot; When using: Default NA = not available. We are asking R to calculate the beta instead of setting it to 1. This gives the equation: \\(Y = Lambda*F + Error\\). Degrees of freedom When you bring variables into the model, the degrees of freedom increases. When there are 0 degrees of freedom, you cannot test but you can fit the model. Now you cannot reject the model, but you can check the reliability. 5.3.1 Setting covariance &amp; variances Independent variables have variances and co-variances, unless the model specification puts them to zero. Variance is to be estimated in the model and if necessary we should be imposing restrictions. When you include all variances, you put a lot of tension on your model. Y ~~ 1* Y = forces the variable to be standardized X ~~ X = Gives two explanatory variables the possibility to covariance Equality with error variances (multiple independent variables) X~~X *A = setting a restriction. Auto regressive = one variables keeps impacting the following. For example: 2011 impacts 2012 which impacts 2013 etc. In this case, the variables need to be set to allow for correlation. "],["basics-python.html", "Chapter - 6 Basics Python 6.1 Data set 6.2 Matrixes 6.3 Creating our own functions: 6.4 Bar plot", " Chapter - 6 Basics Python import numpy as np import pandas as pd import matplotlib.pyplot as plt import seaborn as sns [1] &quot;.Rhistory&quot; &quot;rsconnect&quot; [3] &quot;.DS_Store&quot; &quot;R-Summaries.Rproj&quot; [5] &quot;01-Basics.Rmd&quot; &quot;preamble.tex&quot; [7] &quot;index.Rmd&quot; &quot;images&quot; [9] &quot;02-Charts.Rmd&quot; &quot;Summary_files&quot; [11] &quot;08-references.Rmd&quot; &quot;MIT-Coding-Brain-01-press_0.jpeg&quot; [13] &quot;packages.bib&quot; &quot;03-Probability.Rmd&quot; [15] &quot;_output.yml&quot; &quot;docs&quot; [17] &quot;R-Summaries.Rmd&quot; &quot;07-PracticalDataScience.Rmd&quot; [19] &quot;www&quot; &quot;04-Simpleregressions.Rmd&quot; [21] &quot;_bookdown_files&quot; &quot;README.md&quot; [23] &quot;R-Summaries_files&quot; &quot;05-SEM.Rmd&quot; [25] &quot;_bookdown.yml&quot; &quot;06-Python.Rmd&quot; [27] &quot;.gitignore&quot; &quot;.RData&quot; [29] &quot;style.css&quot; &quot;_book&quot; [31] &quot;book.bib&quot; &quot;.git&quot; [33] &quot;.Rproj.user&quot; variable using instead of &lt;-, equal sign =. y = 5 + 5 y [1] 10 Printing characters print(&#39;Hello, readers!&#39;) [1] &quot;Hello, readers!&quot; Printing numbers print(15) [1] 15 Printing length of a value Length = len(&#39;Danielle&#39;) print(Length) 8 6.1 Data set Loading the data set &amp; viewing head + tails: sns.set_context(&#39;paper&#39;) tips = sns.load_dataset(&#39;tips&#39;) tips.head() total_bill tip sex smoker day time size 0 16.99 1.01 Female No Sun Dinner 2 1 10.34 1.66 Male No Sun Dinner 3 2 21.01 3.50 Male No Sun Dinner 3 3 23.68 3.31 Male No Sun Dinner 2 4 24.59 3.61 Female No Sun Dinner 4 tips.tail() total_bill tip sex smoker day time size 239 29.03 5.92 Male No Sat Dinner 3 240 27.18 2.00 Female Yes Sat Dinner 2 241 22.67 2.00 Male Yes Sat Dinner 2 242 17.82 1.75 Male No Sat Dinner 2 243 18.78 3.00 Female No Thur Dinner 2 Length of the variable Shape: Number of rows and number of columns Type of variables + basic info Descriptive statistics variable len(tips) 244 tips.shape (244, 7) tips.info() &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 244 entries, 0 to 243 Data columns (total 7 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 total_bill 244 non-null float64 1 tip 244 non-null float64 2 sex 244 non-null category 3 smoker 244 non-null category 4 day 244 non-null category 5 time 244 non-null category 6 size 244 non-null int64 dtypes: category(4), float64(2), int64(1) memory usage: 7.3 KB tips.describe() total_bill tip size count 244.000000 244.000000 244.000000 mean 19.785943 2.998279 2.569672 std 8.902412 1.383638 0.951100 min 3.070000 1.000000 1.000000 25% 13.347500 2.000000 2.000000 50% 17.795000 2.900000 2.000000 75% 24.127500 3.562500 3.000000 max 50.810000 10.000000 6.000000 6.2 Matrixes Series: Panda series method: Serie1 = pd.Series( [4200, 8000, 6500], index=[&quot;Amsterdam&quot;, &quot;Toronto&quot;, &quot;Tokyo&quot;] ) Serie1 Amsterdam 4200 Toronto 8000 Tokyo 6500 dtype: int64 Python dictionary method: Serie2 = pd.Series({&quot;Amsterdam&quot;: 5, &quot;Tokyo&quot;: 8}) Serie2 Amsterdam 5 Tokyo 8 dtype: int64 Data frame: Combined_serie = pd.DataFrame({ &quot;Revenue&quot;: Serie1, &quot;Employee_count&quot;: Serie2 }) Combined_serie Revenue Employee_count Amsterdam 4200 5.0 Tokyo 6500 8.0 Toronto 8000 NaN Sub-setting by row: Combined_serie[&quot;Tokyo&quot;:] Revenue Employee_count Tokyo 6500 8.0 Toronto 8000 NaN 6.3 Creating our own functions: Saying hello + name def printing_name(name): print(&#39;Good morning,&#39;, name) printing_name(&#39;Danielle&#39;) Good morning, Danielle Multiple arguments: Saying hello + name + location def welcome(name, location): print(&quot;Good morning&quot;, name, &quot;Welcome to&quot;, location) welcome(&quot;Danielle,&quot;, &quot;class.&quot;) Good morning Danielle, Welcome to class. 6.4 Bar plot sns.set_context(&#39;paper&#39;) tips = sns.load_dataset(&#39;tips&#39;) tips.head() total_bill tip sex smoker day time size 0 16.99 1.01 Female No Sun Dinner 2 1 10.34 1.66 Male No Sun Dinner 3 2 21.01 3.50 Male No Sun Dinner 3 3 23.68 3.31 Male No Sun Dinner 2 4 24.59 3.61 Female No Sun Dinner 4 sns.barplot(x = &#39;day&#39;, y = &#39;total_bill&#39;, hue = &#39;sex&#39;, data = tips, palette = &#39;Blues&#39;, edgecolor = &#39;w&#39;) tips.groupby([&#39;day&#39;,&#39;sex&#39;]).mean() total_bill tip size day sex Thur Male 18.714667 2.980333 2.433333 Female 16.715312 2.575625 2.468750 Fri Male 19.857000 2.693000 2.100000 Female 14.145556 2.781111 2.111111 Sat Male 20.802542 3.083898 2.644068 Female 19.680357 2.801786 2.250000 Sun Male 21.887241 3.220345 2.810345 Female 19.872222 3.367222 2.944444 plt.show() "],["practical-data-science.html", "Chapter - 7 Practical data science 7.1 Machine Learning", " Chapter - 7 Practical data science 7.1 Machine Learning There are multiple types of machine learning: Supervised Unsupervised Reinforcement learning 7.1.0.1 Reinforcement learning: Alpha go. ML for chess. Trained by trial and error. First they are taught the simple rules and then asked to train by themselves and learn from their mistakes. The algorithms are asked to do something and either Get a rewards or A penalty As a result, they learn which moves are good and continue to try something else. Machine learning still does not understand casual relationships. 7.1.0.2 Unsupervised learning: We do not have labels on the data. Can still observe patterns, it understands there are commonalities but not with a reason. You can combine labeled data, for example from the passed and look for patterns with the unlabeled data. 7.1.0.3 Supervised learning Supervised learning: Extracting patterns from data and making predictions based on passed behavior. An example is a picture of an animal and the algorithms predicts which animal it is. Hereby we use training data to train the algorithm. However, the data must be labeled: we already know the correct answer. This method does not include trial or error. For example, first showing examples of cats and then it can make predictions. Meaning, we show a new picture and it can predict whether it is a cat or not a cat. Regression tasks: label is a continuous number. Hereby what we want to predict is continuous, not necessarily the data given to predict. F.E. House prices Classification tasks: Label is one of discrete set of possible values. F.E. Is it a dog or a cat 7.1.1 Theory Supervised learning - Regression - Classification Symbol Explanation y Real value ≈∑ Prediction y-≈∑ Absolute error ( y-≈∑)^2 Squared error | x Generic input / Features. The things I use to predict y. The independent variable (statistics). y Generic output. The label. (ML) The thing I want to predict. Dependent variable (statistics). p Number of features (machine learning). Number of independent variables I have (statistics). n Size of the data set If I know the inputs, I could try to ‚Äúpredict‚Äù the output. This would state that in the real world the outputs (y) are a function of the inputs (x). In mathematical terms: \\(y = f(x)\\) For example, if we know multiple features of a house, we could try to predict whether a person would like it. (based on f.e. Square meters, number of bedrooms etc.) However, realistically and economically we cannot always find all the possible features that would predict an output. It is always affected by some uncertainty. Therefore, y is a function of x but adding some noise. Which leads to: Deterministic function: \\(y = f(x) + E\\) E = (error) noise, a random variable which models some unpredictable events that happens in the real world that we do not have a corresponding input for to take into account. We assume that E obeys at least a couple of properties: E is not correlated with any of the features Expected value of the random variable, E[E] = 0 Assumption 1 can be F.E. that someone really wants to buy a house because there is a good place to put a dog bed. This cannot be predicted based on the other inputs (features) that are in my data set. They should not be correlated. Assumption 2 says that the \\(e\\) doesn‚Äôt ALWAYS cause either a increase or a decrease in the output. It has to be truly random. The real world: \\(y = f(x) + E\\). We want to try to learn more about this function f. Estimator = \\(\\hat f\\). If we do a good job, we are able to find a \\(\\hat f\\) that is similar to the true value of f. If I am able to find a \\(\\hat f\\), I can plug in the input into the estimator and make a prediction. The estimator is the thing that I want to use to approximate as best as possible the real relationship between the inputs and outputs in the real world. In mathematical terms: \\(\\hat f (x) = \\hat y\\) = prediction If the model is accurate the prediction is accurate = \\(\\hat f\\) is similar to \\(f\\) and therefore \\(\\hat y\\) is similar to \\(y\\) and therefore We have accurate predictions. What is key for the data scientist is: Out-Of-Sample Accuracy. This means that your model is accurate with your sample but also with out-of-sample data. How can I measure how accurate \\(\\hat y\\) is compared to \\(y\\)? We look at the error. Squared error: Most classical error measure = \\((y-\\hat y)^2\\) Multiple reasons on why squared errors are used: - Taking square means I forget about the sign of the error (negative vs positive) - Taking square penalizes more ‚Äòextreme‚Äô errors Alternative way of valuing the error is the absolute error: \\(y-\\hat y\\) 7.1.1.1 Data Visualized Typically we will call our data: x &amp; y One data point looks like: \\((x1, x2, ‚Ä¶., xp, y)\\) Here X1 can be independent variable 1 for example square meters. X2 can be number of rooms and Xp the year it is build. Y is the price of the house. 7.1.2 Finding the expected value of the error Random variable: is described informally as a variable whose values depend on outcomes of a random phenomenon. The error is a random variable. Therefore, y is also a random variable because some of its expression is: \\(f(x) + œµ = y\\). \\(Eœµ[y-\\hat y]^2\\) = expected value of the squared error Because we know that the formula for y is = \\(f(x) + œµ = y\\), we can re-write this expression as: \\(Eœµ[(f(x)+ œµ-\\hat y]^2\\) We continue to solve the equation. We can re-write \\(\\hat y\\) as \\(\\hat f(x)\\): \\(Eœµ[(f(x)+ œµ-\\hat f(x)]^2\\) We rearrange these terms: \\(Eœµ[(f(x)-\\hat f(x)+ œµ]^2\\) For ease of notation, \\(f(x)-\\hat f(x)\\) becomes alpha \\(Eœµ[\\alpha+ œµ]^2\\) We expand: \\(Eœµ[\\alpha^2 + 2\\alphaœµ + œµ^2]\\) Make use of it being linear: \\(Eœµ[\\alpha^2] + 2Eœµ[\\alphaœµ] + Eœµ[œµ^2]\\) Now we can see that alpha does not have the random element œµ making it not a random variable and is deterministic term (constant). As it is constant and without error, it is already the expected value. We re-write again: \\(\\alpha^2 + 2Eœµ[\\alphaœµ] + Eœµ[œµ^2]\\) As previously explained in the theory, the expect value of the noise should be 0. This is the assumption made. \\(\\alpha^2 + 0 + Eœµ[œµ^2]\\) The variance of a random variable is, for example variable z = \\(Var[z] = E[z^2] - (E[z])^2\\) If we apply this definition to the above œµ: \\(Var[œµ] = E[œµ^2] - (E[œµ])^2\\) As we said before, the expected value of œµ is 0 and therefore the variance is: \\(Var[œµ] = E[œµ^2] - 0 = E[œµ^2]\\) To combine this with the previous equation: \\(\\alpha^2 + Eœµ[œµ^2] = [f(x) - \\hat f(x)]^2 + Var[œµ]\\) This can be separated in two parts: \\([f(x) - \\hat f(x)]^2\\) \\(Var[œµ]\\) Reducible error: Real relation - estimator Irreducible error Intrinsic property of the error If the model is really good, the estimator is similar to the real relation and I can ‚Äúreduce‚Äù the error. If the model is extremely precise, the estimator can even be exactly the real relation. It therefore depends on the accuracy of the estimator. \\(\\hat f = f\\) However, even when this happens, I still cannot affect the ‚Äúirreducible‚Äù error because it is noise from the real world. It is intrinsic property / characteristics of the data, not the estimator. To conclude, we can only affect the reducible error. Data set = \\((x1, y1)‚Ä¶.,(xn, yn)\\) Data point i = \\(Xi œµR^p\\) The estimator is a function that takes a p dimensional and produces a real values output. \\(\\hat f:R^p ‚Äì&gt; R\\) \\(\\hat y = \\hat f (x)\\)= prediction or estimate \\(\\hat f\\) = estimator If I have a concrete set of observations, I can estimate the expected value. For example, I can take the average height of a class to estimate the expected value of the height of the class. Mean squared error (MSE): The empirical average of the expected value of the error term. In mathematical terms: \\(MSE(\\hat f) = \\frac{1}{n} \\sum_{i=1}^{n}(y_i-\\hat y_i)^2\\) Considering that \\(\\hat y_i\\) is nothing else than the prediction for the i input = \\(\\hat y_i = \\hat f(x_i)\\). Therefore, we can transform again: \\(MSE(\\hat f) = \\frac{1}{n} \\sum_{i=1}^{n}(y_i-\\hat f(x_i))^2\\) I can apply \\(\\hat f\\) to one row, calculate the p features and look at the real label, to compute the MSE. Mean absolute error (MAE): Hereby the only difference is that it is not squared. \\(MAE(\\hat f) = \\frac{1}{n} \\sum_{i=1}^{n}[y_i-\\hat f(x_i)]\\) 7.1.3 Loss function We can use a loss function which takes as input two numbers: the real and the predicted value and gives as output another real number. This is the formula: \\(L(y, \\hat y): R^2 = R\\) Squared error Absolute error \\(L(y, \\hat y) = (y-\\hat y)^2\\) \\(L(y, \\hat y) = (y-\\hat y)\\) I want the loss function to obey two properties: If I make a correct prediction \\((\\hat y = y)\\), then I have 0 loss \\(L(y, \\hat y) = 0\\) if \\((\\hat y = y)\\). For most loss function, I want \\(L(y, \\hat y)\\) to be large than the ‚Äúwronger‚Äù my prediction $$\\hat y$ is. The loss function should not become smaller when the prediction becomes ‚Äúwronger.‚Äù Wronger = the more different my prediction is than the true number. Loss function should be small when my prediction is close to the true value and it should be large when it is not. Estimating the error on existing data on which I know the label: \\(Error(\\hat f) = \\frac{1}{n} \\sum_{i=1}^{n}L(y_i-\\hat f(x_i))\\) Now I am calculating how accurate my model is based on my data set. However, we want our model to work well on new previously unseen data that is out of my data set / sample. In other words: Out of sample accuracy. We therefore, separate our data in two parts: the training set and the test set. Training set: Data we show our model to have it learn a good estimator. \\(\\hat f = +/- f\\). The training set will be used to derive to a estimator. Test set: Data which we hide from our model. After the model has been trained, we will simulate it to the test data to evaluate the model‚Äôs performance. The test set will be sued to estimate the error fo the \\(\\hat f\\). The training set is called: N = \\((x_1,y_1)....,(x_n,y_n)\\) The test set is called: M = \\((x_1,y_1)....,(x_m,y_m)\\) Therefore, to calculate the estimate of the error of the model: \\(Err(\\hat f) = \\frac{1}{n-m} \\sum_{i=m+1}^{n}L(y_i-\\hat f(x_i))\\) To train a model = to find good values for its parameters. First I have to fix the shape of the model. Linear: \\(\\hat f(x_1-,x_p) = \\beta_0+\\beta_1,.....+\\beta_p X_p\\) Beta‚Äôs are the linear coefficients and the X1, Xp are the variables. The \\(\\beta\\) ‚Äôs are parameters. I can train a model to find a good estimator by finding good parameters Quadratic: \\(\\hat f(x_1-,x_p) = \\beta_0+\\beta_1,.....+\\beta_p X_p + \\beta_1X_1^2+....\\beta_1pX_1p + etc.\\) I do not know what model is the good model. So I train each model and then I pick the model that has the lowest error. \\(x\\) = an input (p) \\(\\beta\\) = a vector of parameters (k) \\(\\beta^*\\) = optimal solution of the betas This is a optimization problem where I try to minimize the empirical error of the model on the training set. Once we solve the following model, I find the optimal values for the \\(\\beta\\)‚Äôs and this will find me the estimator. Once I have my estimator, I can take the test data and estimate an error for the model. Training error =\\(\\frac{1}{n} \\sum_{i=1}^{n}L(y_i-\\hat f(\\beta_ix_i))\\) Test error = \\(Err(\\hat f) = \\frac{1}{n-m} \\sum_{i=m+1}^{n}L(y_i-\\hat f(\\beta^*_i,x_i))\\) "],["references.html", "References", " References Gray, kevin. 2017. Structural Equation Model. kdnuggets. https://www.kdnuggets.com/2017/03/structural-equation-modeling.html. "]]

<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter - 8 How much does the estimator (random variable) defer from the real values | Business Analytics - Cheatsheets &amp; Summary</title>
  <meta name="description" content="This is a summary of r code learned throughout several courses of my master in management." />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter - 8 How much does the estimator (random variable) defer from the real values | Business Analytics - Cheatsheets &amp; Summary" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This is a summary of r code learned throughout several courses of my master in management." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter - 8 How much does the estimator (random variable) defer from the real values | Business Analytics - Cheatsheets &amp; Summary" />
  
  <meta name="twitter:description" content="This is a summary of r code learned throughout several courses of my master in management." />
  




  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="practical-data-science.html"/>
<link rel="next" href="references.html"/>
<script src="libs/header-attrs-2.7/header-attrs.js"></script>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />











<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">R-coding Cheatsheets & Summary</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="basics-r.html"><a href="basics-r.html"><i class="fa fa-check"></i><b>1</b> Basics R</a>
<ul>
<li class="chapter" data-level="1.0.1" data-path="basics-r.html"><a href="basics-r.html#tables-frames-matrices"><i class="fa fa-check"></i><b>1.0.1</b> Tables, frames &amp; Matrices</a></li>
<li class="chapter" data-level="1.1" data-path="basics-r.html"><a href="basics-r.html#data-sets"><i class="fa fa-check"></i><b>1.1</b> Data sets</a>
<ul>
<li class="chapter" data-level="1.1.1" data-path="basics-r.html"><a href="basics-r.html#removing-infinite-na-values"><i class="fa fa-check"></i><b>1.1.1</b> Removing infinite + NA values</a></li>
<li class="chapter" data-level="1.1.2" data-path="basics-r.html"><a href="basics-r.html#transforming-variable-types"><i class="fa fa-check"></i><b>1.1.2</b> Transforming variable types</a></li>
<li class="chapter" data-level="1.1.3" data-path="basics-r.html"><a href="basics-r.html#markdown"><i class="fa fa-check"></i><b>1.1.3</b> Markdown</a></li>
<li class="chapter" data-level="1.1.4" data-path="basics-r.html"><a href="basics-r.html#setup-rmarkdown-code-chunks"><i class="fa fa-check"></i><b>1.1.4</b> Setup rmarkdown &amp; code chunks</a></li>
<li class="chapter" data-level="1.1.5" data-path="basics-r.html"><a href="basics-r.html#miscellaneous"><i class="fa fa-check"></i><b>1.1.5</b> Miscellaneous</a></li>
<li class="chapter" data-level="1.1.6" data-path="basics-r.html"><a href="basics-r.html#subsetting"><i class="fa fa-check"></i><b>1.1.6</b> Subsetting</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="charts-templates-r.html"><a href="charts-templates-r.html"><i class="fa fa-check"></i><b>2</b> Charts templates - R</a></li>
<li class="chapter" data-level="3" data-path="probability.html"><a href="probability.html"><i class="fa fa-check"></i><b>3</b> Probability</a>
<ul>
<li class="chapter" data-level="3.1" data-path="probability.html"><a href="probability.html#bayes-theorem"><i class="fa fa-check"></i><b>3.1</b> Bayes Theorem</a></li>
<li class="chapter" data-level="3.2" data-path="probability.html"><a href="probability.html#discrete-probablity"><i class="fa fa-check"></i><b>3.2</b> Discrete Probablity</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="probability.html"><a href="probability.html#uniform-discrete-probability-distribution"><i class="fa fa-check"></i><b>3.2.1</b> Uniform discrete probability distribution</a></li>
<li class="chapter" data-level="3.2.2" data-path="probability.html"><a href="probability.html#binomial-distribution"><i class="fa fa-check"></i><b>3.2.2</b> Binomial distribution</a></li>
<li class="chapter" data-level="3.2.3" data-path="probability.html"><a href="probability.html#poisson-distribution"><i class="fa fa-check"></i><b>3.2.3</b> Poisson distribution</a></li>
<li class="chapter" data-level="3.2.4" data-path="probability.html"><a href="probability.html#the-normal-distribution"><i class="fa fa-check"></i><b>3.2.4</b> The normal distribution</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="probability.html"><a href="probability.html#samples-estimation-confidence-intervals"><i class="fa fa-check"></i><b>3.3</b> Samples, estimation &amp; confidence intervals</a></li>
<li class="chapter" data-level="3.4" data-path="probability.html"><a href="probability.html#significance-level"><i class="fa fa-check"></i><b>3.4</b> Significance level</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="probability.html"><a href="probability.html#critical-values"><i class="fa fa-check"></i><b>3.4.1</b> Critical values</a></li>
<li class="chapter" data-level="3.4.2" data-path="probability.html"><a href="probability.html#test-of-equality---two-samples"><i class="fa fa-check"></i><b>3.4.2</b> Test of equality - two samples</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="probability.html"><a href="probability.html#non-parametric-testing"><i class="fa fa-check"></i><b>3.5</b> Non-Parametric testing</a>
<ul>
<li class="chapter" data-level="3.5.1" data-path="probability.html"><a href="probability.html#contengency-table-frequencies"><i class="fa fa-check"></i><b>3.5.1</b> Contengency table / frequencies</a></li>
<li class="chapter" data-level="3.5.2" data-path="probability.html"><a href="probability.html#chi-square"><i class="fa fa-check"></i><b>3.5.2</b> Chi-square</a></li>
<li class="chapter" data-level="3.5.3" data-path="probability.html"><a href="probability.html#goodness-of-fit"><i class="fa fa-check"></i><b>3.5.3</b> Goodness of fit</a></li>
<li class="chapter" data-level="3.5.4" data-path="probability.html"><a href="probability.html#p-value"><i class="fa fa-check"></i><b>3.5.4</b> P-value</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="simple-regressions.html"><a href="simple-regressions.html"><i class="fa fa-check"></i><b>4</b> Simple regressions</a>
<ul>
<li class="chapter" data-level="4.1" data-path="simple-regressions.html"><a href="simple-regressions.html#basics-regressions"><i class="fa fa-check"></i><b>4.1</b> Basics regressions</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="simple-regressions.html"><a href="simple-regressions.html#summarizing-regressions"><i class="fa fa-check"></i><b>4.1.1</b> Summarizing regressions:</a></li>
<li class="chapter" data-level="4.1.2" data-path="simple-regressions.html"><a href="simple-regressions.html#dummy-variables-diff-in-means"><i class="fa fa-check"></i><b>4.1.2</b> Dummy variables, diff in means</a></li>
<li class="chapter" data-level="4.1.3" data-path="simple-regressions.html"><a href="simple-regressions.html#regression-dummy"><i class="fa fa-check"></i><b>4.1.3</b> Regression + dummy</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="simple-regressions.html"><a href="simple-regressions.html#prediction"><i class="fa fa-check"></i><b>4.2</b> Prediction</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="simple-regressions.html"><a href="simple-regressions.html#confidence-and-prediction-plotting"><i class="fa fa-check"></i><b>4.2.1</b> Confidence and prediction plotting</a></li>
<li class="chapter" data-level="4.2.2" data-path="simple-regressions.html"><a href="simple-regressions.html#prediction-with-dummy-variables"><i class="fa fa-check"></i><b>4.2.2</b> Prediction with dummy variables</a></li>
<li class="chapter" data-level="4.2.3" data-path="simple-regressions.html"><a href="simple-regressions.html#prediction-intervals-examples"><i class="fa fa-check"></i><b>4.2.3</b> Prediction intervals examples</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="simple-regressions.html"><a href="simple-regressions.html#data-problems"><i class="fa fa-check"></i><b>4.3</b> Data problems</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="simple-regressions.html"><a href="simple-regressions.html#multicollinearity"><i class="fa fa-check"></i><b>4.3.1</b> Multicollinearity</a></li>
<li class="chapter" data-level="4.3.2" data-path="simple-regressions.html"><a href="simple-regressions.html#variance-inflation-factors"><i class="fa fa-check"></i><b>4.3.2</b> Variance inflation factors</a></li>
<li class="chapter" data-level="4.3.3" data-path="simple-regressions.html"><a href="simple-regressions.html#anova"><i class="fa fa-check"></i><b>4.3.3</b> ANOVA</a></li>
<li class="chapter" data-level="4.3.4" data-path="simple-regressions.html"><a href="simple-regressions.html#linearizing-variables"><i class="fa fa-check"></i><b>4.3.4</b> Linearizing variables</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="structure-equation-models.html"><a href="structure-equation-models.html"><i class="fa fa-check"></i><b>5</b> Structure equation models</a>
<ul>
<li class="chapter" data-level="5.1" data-path="structure-equation-models.html"><a href="structure-equation-models.html#path-analysis-structural-equations"><i class="fa fa-check"></i><b>5.1</b> Path analysis (structural equations)</a></li>
<li class="chapter" data-level="5.2" data-path="structure-equation-models.html"><a href="structure-equation-models.html#coding-the-model"><i class="fa fa-check"></i><b>5.2</b> Coding the model</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="structure-equation-models.html"><a href="structure-equation-models.html#covariance"><i class="fa fa-check"></i><b>5.2.1</b> Covariance</a></li>
<li class="chapter" data-level="5.2.2" data-path="structure-equation-models.html"><a href="structure-equation-models.html#reliability"><i class="fa fa-check"></i><b>5.2.2</b> Reliability</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="structure-equation-models.html"><a href="structure-equation-models.html#factor-model"><i class="fa fa-check"></i><b>5.3</b> Factor model</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="structure-equation-models.html"><a href="structure-equation-models.html#setting-covariance-variances"><i class="fa fa-check"></i><b>5.3.1</b> Setting covariance &amp; variances</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="basics-python.html"><a href="basics-python.html"><i class="fa fa-check"></i><b>6</b> Basics Python</a>
<ul>
<li class="chapter" data-level="6.1" data-path="basics-python.html"><a href="basics-python.html#data-set"><i class="fa fa-check"></i><b>6.1</b> Data set</a></li>
<li class="chapter" data-level="6.2" data-path="basics-python.html"><a href="basics-python.html#matrixes"><i class="fa fa-check"></i><b>6.2</b> Matrixes</a></li>
<li class="chapter" data-level="6.3" data-path="basics-python.html"><a href="basics-python.html#filtering-a-data-set"><i class="fa fa-check"></i><b>6.3</b> Filtering a data set</a></li>
<li class="chapter" data-level="6.4" data-path="basics-python.html"><a href="basics-python.html#data-imputation"><i class="fa fa-check"></i><b>6.4</b> Data imputation</a></li>
<li class="chapter" data-level="6.5" data-path="basics-python.html"><a href="basics-python.html#data-visualization"><i class="fa fa-check"></i><b>6.5</b> Data visualization</a>
<ul>
<li class="chapter" data-level="6.5.1" data-path="basics-python.html"><a href="basics-python.html#other-categorical-plots"><i class="fa fa-check"></i><b>6.5.1</b> Other categorical plots</a></li>
<li class="chapter" data-level="6.5.2" data-path="basics-python.html"><a href="basics-python.html#preparing-the-data"><i class="fa fa-check"></i><b>6.5.2</b> Preparing the data</a></li>
<li class="chapter" data-level="6.5.3" data-path="basics-python.html"><a href="basics-python.html#creating-the-models"><i class="fa fa-check"></i><b>6.5.3</b> Creating the models</a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="basics-python.html"><a href="basics-python.html#model-selection"><i class="fa fa-check"></i><b>6.6</b> Model selection</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="practical-data-science.html"><a href="practical-data-science.html"><i class="fa fa-check"></i><b>7</b> Practical data science</a>
<ul>
<li class="chapter" data-level="7.1" data-path="practical-data-science.html"><a href="practical-data-science.html#machine-learning"><i class="fa fa-check"></i><b>7.1</b> Machine Learning</a>
<ul>
<li class="chapter" data-level="7.1.1" data-path="practical-data-science.html"><a href="practical-data-science.html#theory"><i class="fa fa-check"></i><b>7.1.1</b> Theory</a></li>
<li class="chapter" data-level="7.1.2" data-path="practical-data-science.html"><a href="practical-data-science.html#finding-the-expected-value-of-the-error"><i class="fa fa-check"></i><b>7.1.2</b> Finding the expected value of the error</a></li>
<li class="chapter" data-level="7.1.3" data-path="practical-data-science.html"><a href="practical-data-science.html#loss-function"><i class="fa fa-check"></i><b>7.1.3</b> Loss function</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="practical-data-science.html"><a href="practical-data-science.html#bias-variance-trade-off"><i class="fa fa-check"></i><b>7.2</b> Bias-variance trade off</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="how-much-does-the-estimator-random-variable-defer-from-the-real-values.html"><a href="how-much-does-the-estimator-random-variable-defer-from-the-real-values.html"><i class="fa fa-check"></i><b>8</b> How much does the estimator (random variable) defer from the real values</a>
<ul>
<li class="chapter" data-level="8.0.1" data-path="how-much-does-the-estimator-random-variable-defer-from-the-real-values.html"><a href="how-much-does-the-estimator-random-variable-defer-from-the-real-values.html#training-fitting-a-model"><i class="fa fa-check"></i><b>8.0.1</b> <strong>Training (fitting) a model</strong></a></li>
<li class="chapter" data-level="8.1" data-path="how-much-does-the-estimator-random-variable-defer-from-the-real-values.html"><a href="how-much-does-the-estimator-random-variable-defer-from-the-real-values.html#gradient-descent"><i class="fa fa-check"></i><b>8.1</b> Gradient descent</a>
<ul>
<li class="chapter" data-level="8.1.1" data-path="how-much-does-the-estimator-random-variable-defer-from-the-real-values.html"><a href="how-much-does-the-estimator-random-variable-defer-from-the-real-values.html#sarcastic-gradient-descent"><i class="fa fa-check"></i><b>8.1.1</b> Sarcastic gradient descent</a></li>
<li class="chapter" data-level="8.1.2" data-path="how-much-does-the-estimator-random-variable-defer-from-the-real-values.html"><a href="how-much-does-the-estimator-random-variable-defer-from-the-real-values.html#model-selection-1"><i class="fa fa-check"></i><b>8.1.2</b> Model Selection</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Business Analytics - Cheatsheets &amp; Summary</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="how-much-does-the-estimator-random-variable-defer-from-the-real-values" class="section level1" number="8">
<h1><span class="header-section-number">Chapter - 8</span> How much does the estimator (random variable) defer from the real values</h1>
<p><span class="math inline">\(Bias[\hat y] = E[\hat y]-f(x)\)</span> =<span class="math inline">\(Bias^2[\hat y] = (E[\hat y] -f(x))^2\)</span></p>
<p>Recalling:</p>
<p>Variance = <span class="math inline">\(Var[Z] = E[Z^2] - (E[Z])^2\)</span></p>
<p>Rewrite <span class="math inline">\(E[(Z - E[Z])^2\)</span></p>
<p><span class="math inline">\(\mu = E[Z]\)</span></p>
<p><span class="math inline">\(E[(Z-\mu)^2]\)</span></p>
<p>The expected value of a random variable is a constant. Therefore, I can
transform:</p>
<p><span class="math inline">\(E[E[Z]] = E[Z]\)</span></p>
<p>Training test split = the expected value in the following scenario.</p>
<p><span class="math inline">\(E[(y-\hat y)^2] = E[(y-f(x) + f(x) - \hat y)^2]\)</span></p>
<p>Next we can solve this equation:</p>
<p><span class="math inline">\(E[(y-f(x))^2] + 2E[(y-f(x))(f(x)-\hat y)] + E[(f(x)-\hat y)^2]\)</span></p>
<p>Definition of <span class="math inline">\(y = f(x)+ ϵ\)</span></p>
<p><span class="math inline">\(E[ϵ^2] + 2E[yf(x)-(f(x))^2 - y\hat y + \hat yf(x)] + E[(f(x)- \hat y )^2]\)</span></p>
<p>We know that <span class="math inline">\(E[ϵ^2]\)</span> is <span class="math inline">\(var[ϵ]\)</span></p>
<p><img src="images/paste-7FBD2164.png" width="387" /></p>
<p>We want the error of the model to be as low as possible. We cannot
change the noise. Therefore, we want the variance and the trade off to
be as low as possible. The best we can do is that they are both 0.</p>
<p>If you reduce the variance, naturally there will be at least some biased
introduced and vice versa.</p>
<p><img src="images/paste-607116A0.png" width="449" /></p>
<p>The variance of <span class="math inline">\(\hat y\)</span> = How much would the prediction <span class="math inline">\(\hat y\)</span> vary
when we change the training set.</p>
<p>My objective is that the predictor is similar to the real function =
<span class="math inline">\(\hat f \approx f\)</span>. If I have a predictor that varies a lot based on the
training data, it cannot really be similar to the real function.
Naturally, I want the variance of <span class="math inline">\(\hat y\)</span> to be lower.</p>
<p><strong>Model with high variance</strong> = Overfitted model (too complex). A small
change in the training set –&gt; large change in the predictions. This
has a low bias.</p>
<p>If my model learns the noise of the training set, it can make perfect
predictions. However, when you apply the model to a different data set,
it will give bad predictions. Having too many parameters allows the
model to learn the noise.</p>
<p>If a model is very complicated, it is more likely to be overfitted (high
variance, low bias).</p>
<p><strong>Model with high bias</strong> = Underfitted model (too simple). Large error
even in the training set. This typically has a low variance. It is
stable but it will give bad predictions and not accurate.</p>
<p>We can visualize this by plotting a linear regression model. We have
made a small change in the training set (one point has moved) and there
has been a small variance in the regression line.</p>
<p><img src="images/paste-B7BAF89D.png" /></p>
<p><br />
So we can conclude that there is a low variance in the model.</p>
<p><br />
Now we will look at a polynomial model where there is 0 bias. Therefore,
the model must intercept perfectly all the points in the training set.</p>
<p><img src="images/paste-E57B9766.png" /></p>
<p>If I want to intercept 4 points in <span class="math inline">\(R^2\)</span>, what is the smallest degree of
a polynomial that 1 need? –&gt; N-1</p>
<p>The green model has a high variance, but the bias - 0. This is a
overfitted model.</p>
<hr />
<div id="training-fitting-a-model" class="section level3" number="8.0.1">
<h3><span class="header-section-number">8.0.1</span> <strong>Training (fitting) a model</strong></h3>
<p><strong>Training a model:</strong> Finding the parameters (of a parametric model)
which minimize the loss function over the training set.</p>
<p>Training set has N points –&gt; <span class="math inline">\((X_1,Y_1),....,(X_n,Y_n)\)</span>.</p>
<p><em>Minimize our parameters:</em></p>
<p><span class="math inline">\(\beta :=(\beta_1....,\beta_k)\)</span> = The K parameters of my model.</p>
<p>I want to minimize the training error:
<span class="math inline">\(\frac{1}{n} \sum_{i=1}^{n}L(y_i-\hat f(\beta_ix_i))\)</span></p>
<p>k = p+1. These are all the parameters plus the intercept in the example
of a regression model.</p>
<p>Renamed function of betas: <span class="math inline">\(g(\beta)\)</span></p>
<p>Not all functions are born equal. There are some functions that are
easier to deal with. Fe. if they are continuous. In the case if I am in
luck there is a good chance that I can find the minimize. If these
conditions don’t hold it will be very difficult. There is a chance that
a algorithm finds not necessarily the <strong>global minimum</strong> but it does
find the <strong>local minimum.</strong> This can be visualized below:</p>
<p><img src="images/paste-600DC796.png" /></p>
<p><strong>A convex function:</strong> a continuous <strong>function</strong> whose value at the
midpoint of every interval in its domain does not exceed the arithmetic
mean of its values at the ends of the interval.mo</p>
<p>Hereby, the global minimum is automatically also the local minimum as
there is only one.</p>
<p><img src="images/paste-AAF7E2B1.png" width="240" /></p>
<p><strong>Convex vs concave:</strong></p>
<p><img src="images/paste-85CC3A24.png" /></p>
<p>If I want to find a minimum of a function, I need to find a point where
the first derivative vanishes. Multiple things could happen:</p>
<ul>
<li><p>You find a local min (might be global)</p></li>
<li><p>You find a local max (might be global)</p></li>
<li><p>You find a saddle point</p></li>
</ul>
</div>
<div id="gradient-descent" class="section level2" number="8.1">
<h2><span class="header-section-number">8.1</span> Gradient descent</h2>
<p>Instead, we present an algorithm that tries to find the local minimum
using the first derivative. It is a multivariate function as:</p>
<p><span class="math inline">\(g:R^k -&gt; R^+_0\)</span> = <span class="math inline">\(\nabla g\)</span></p>
<p><span class="math inline">\(\nabla g(\beta_1,....,\beta_k)\)</span> =
<span class="math inline">\((\frac{\partial g}{\partial\beta_1}(\beta_1,...,\beta_k),....,\frac{\partial g}{\partial\beta_1}(\beta_1,...,\beta_k))\)</span></p>
<p>a vector that has all the partial derivatives at that point.</p>
<p><strong>Gradient =</strong> a measure of how steep a slope is</p>
<p><strong>Derivative =</strong> of a function of a real variable measures the
sensitivity to change of the function value (output value) with respect
to a change in its argument (input value).</p>
<p><strong>Function =</strong> something that will take a input and will produce a given
output.</p>
<p><br />
We want to go against the gradient (-) to find the minimum. We want to
follow the blue steps to arrive at the minimum (pink).</p>
<p><img src="images/paste-962EC0DD.png" /></p>
<p><strong>Newton’s Algorithm / Gradient Descent Algorithm =</strong> only uses one
parameter = alpha (<span class="math inline">\(\alpha\)</span>), can be any positive number.</p>
<ol style="list-style-type: decimal">
<li><p>Fix an arbitrary <span class="math inline">\(\beta_0R_k\)</span> to start from.</p></li>
<li><p>For t=0,….,T: the maximum number of steps that I will do (going
from <span class="math inline">\(\beta_1 - \beta_2\)</span>). The equation is:</p>
<p><span class="math inline">\(\beta^{t+1} = \beta^t-\alpha\nabla g(\beta^t)\)</span><br />
If the <span class="math inline">\(\nabla g(\beta^{t+1})=0\)</span> then break (You have found the
global minimum if the function in convex)</p></li>
<li><p>Return the last <span class="math inline">\(\beta^t\)</span> available.</p></li>
</ol>
<p><span class="math inline">\(\alpha\)</span> <strong>= The step size / learning rate.</strong></p>
<p>If our function looks like below:</p>
<p><img src="images/paste-341F8419.png" width="318" /></p>
<p>We will find a local minimum and will be stuck in the equation as the
gradient is 0.</p>
<p>The above equation considers a fixed parameter <span class="math inline">\(\alpha\)</span>. It is also
possible to consider a <strong>changing parameter =</strong> <span class="math inline">\(\alpha_t\)</span>.</p>
<p>If <span class="math inline">\(\alpha\)</span> grows, then we move more at each iteration. This could mean
that we move faster as there are fewer iterations. Alternatively, we
start jumping all over the space of the parameters without converging or
converging slowly.</p>
<p>Here is an example of the set size (learning rate) being too large and
experiencing <strong>“jumping”</strong>:</p>
<p><img src="images/paste-A4685804.png" width="311" /></p>
<p><em>How to choose</em> <span class="math inline">\(\alpha\)</span><em>?</em> Trying different values and finding the best
solution. “Typically” <span class="math inline">\(\alpha = 10^{-3} = 0.01\)</span>.</p>
<p>Each iteration of the gradient descent algorithm we do:</p>
<p><span class="math inline">\(\beta^{t+1} = \beta^t-\alpha\nabla g(\beta^t)\)</span></p>
<p><span class="math inline">\(\nabla g(\beta^t) = \nabla(\frac{1}{n} \sum_{i=1}^{n}g_i(\beta^t))\)</span></p>
<p>Rewrite:</p>
<p><span class="math inline">\(\nabla g(\beta^t) = \frac{1}{n} \sum_{i=1}^{n}\nabla g_i(\beta^t)\)</span></p>
<p>To compute <span class="math inline">\(\nabla g\)</span> I used N gradients. These are one for each point
in the training set. This must happen at each iteration.</p>
<p>In big data settings, it is not unlikely that we have a training set
that have millions of data points. This would require too many gradients
(too slow).</p>
<div id="sarcastic-gradient-descent" class="section level3" number="8.1.1">
<h3><span class="header-section-number">8.1.1</span> Sarcastic gradient descent</h3>
<p>In the previous computation we had to computes <strong>n</strong> gradients =<br />
<span class="math inline">\(\nabla g(\beta^t) = \frac{1}{n} \sum_{i=1}^{n}\nabla g_i(\beta^t)\)</span></p>
<p>However, we can replace this with something that only needs to compute 1
gradient, no matter how large the data set is.</p>
<ol style="list-style-type: decimal">
<li><p>Fix an arbitrary <span class="math inline">\(\beta^0 e R^k\)</span> to start from</p></li>
<li><p>For <span class="math inline">\(t=0,....T:\)</span></p>
<ul>
<li><p>Draw <span class="math inline">\(jϵ\)</span> {1,….,m} with uniform random distribution</p></li>
<li><p><span class="math inline">\(\beta^{t+1} = \beta^t - \alpha \nabla g_j(\beta^t)\)</span></p></li>
<li><p>etc. everything else is like in gradient descent</p></li>
</ul></li>
</ol>
<p>The only difference is how we jump from the iterations. Instead of
computing the entire gradient.</p>
<p><strong>Gradient descent</strong> =
<span class="math inline">\(\beta^{t+1}= \beta^t - \alpha * \frac{1}{n} \sum_{i=1}^{n}\nabla g_i(\beta^t)\)</span></p>
<p>vs</p>
<p><strong>Sarcastic gradient descent</strong> =
<span class="math inline">\(\beta^{t+1}= \beta^t - \alpha \nabla g_j(\beta^t)\)</span></p>
<p>Where J is taken uniformly at random = j~<span class="math inline">\(U({1,....,m})\)</span></p>
<p><strong><em>GD vs SGD</em></strong></p>
<p><img src="images/paste-9CE815C2.png" /></p>
<p><span class="math inline">\(\beta=(\beta_0, \beta_1)\)</span> initial <span class="math inline">\(\beta^0=(\beta^0_0, \beta^0_1)\)</span></p>
<p>next <span class="math inline">\(\beta^1=(\beta^1_0, \beta^1_1)\)</span></p>
<p><strong>Here the MSE</strong><span class="math inline">\((\beta^0)\)</span> <strong>&lt; MSE</strong> <span class="math inline">\(\beta^1\)</span></p>
<p>In linear regression the:
<span class="math inline">\(g(\beta) = \frac{1}{n} \sum_{i=1}^{n}y_i-(\beta^0+\beta^1x_i))^2 = MSE(\beta)\)</span></p>
<p>In sarcastic gradient descent:</p>
<p><span class="math inline">\(g_j(\beta) = y_j-(\beta^0+\beta_1x_j))^2 = SE_j(\beta)\)</span></p>
<p>Here the squared error is only regarding the random point j. The
regression line is changing so that the squared error of j becomes
smaller. By doing so over and over and choosing random uniformly j, it
is as if I was optimizing for all the squared errors. This can be proven
by:</p>
<p>Instead of using:</p>
<p><span class="math inline">\(\nabla g(\beta)= \frac{1}{n} \sum_{i=1}^{n}\nabla g_i(\beta)\)</span></p>
<p>We use:</p>
<p><span class="math inline">\(\nabla g_j(\beta)\)</span></p>
<p>Therefore, in expectation of a discrete random variable:</p>
<p><span class="math inline">\(E [\nabla g_j(\beta)] = \frac{1}{n} \sum_{i=1}^{n}\nabla g_i(\beta) = \nabla g(\beta)\)</span></p>
<p>In expectation the gradient computed on that function <span class="math inline">\(g_j\)</span> is equal to
the big gradient (the gradient with respect to all the points).
Subsequently, on average, the squared error of j is equal to the mean
squared error of all the points.</p>
<ul>
<li><p>SGD needs more iterations then GD. However, each iteration is much
quicker. In the end, you can expect that SGD takes less time than
GD.</p></li>
<li><p>Even if you have a convex differential function, you have less
guarantee. GD with any alpha in 0,1 is guaranteed to converge to the
optimum (global minimum). Sarcastic gradient descent you only
guarantee that you will arrive at a ball around the global minimum.
But inside this ball there is no guarantee anymore.</p></li>
</ul>
<ol style="list-style-type: decimal">
<li>GD</li>
<li>SGD</li>
</ol>
<p><img src="images/paste-A761BC35.png" width="356" /></p>
<p>SGD could jump around in the ball and never arrive to the global
minimum. If this happens, you could go back to classic gradient descent
and try to converge.<br />
</p>
<p><strong>Mini-Batch Gradient descent</strong></p>
<p>Takes the average of a subset of n of size k.</p>
<p><img src="images/paste-C860906C.png" /></p>
<p>These points are mostly not taken at random. Takes k points for each
iteration. 1 epoch has passed per iteration.</p>
<hr />
</div>
<div id="model-selection-1" class="section level3" number="8.1.2">
<h3><span class="header-section-number">8.1.2</span> Model Selection</h3>
<p>Model selection = choosing the “best” model out of a set of possible
models that we are trying to consider. The “best” is considering the
out-of-sample predictive accuracy = lowest error (loss) on the point NOT
used for training = test set.</p>
<p>While doing predictions, we want to come up with appropriate estimators
(<span class="math inline">\(\hat f\)</span>) which are similar to our real word function (<span class="math inline">\(f\)</span>), that links
our features to our label. This is expressed in, we want that:
<span class="math inline">\(\hat f \approx f\)</span>.</p>
<p>Typically there are many models that can be used. Therefore, we need to
find out which model is the closest to the real value <span class="math inline">\(f\)</span>. Some models
have hyper parameters that we need to choose which would give us even
more different models.</p>
<p>In machine learning, a <strong>hyper parameter</strong> is a parameter whose value is
used to control the learning process. By contrast, the values of other
parameters (typically node weights) are derived via training. These are
either not trainable through gradient descent or it would not be
effective as it would defeat their purpose.</p>
<p>If we include the hyper parameter through gradient descent, it would be
set to 0. Moreover, setting the alpha to anything other than 0, would
give us a worse model on the training set. Perhaps, this would translate
to a better model on the test set. As we care mainly about the
out-of-sample-accuracy, this would be optimal. Therefore, we need a
different / better way on determining alpha which is different than
gradient descent –&gt; Hyper parameter tuning procedure.</p>
<p><img src="images/paste-4DA3F338.png" /></p>
<p>In the models that include hyper parameters, the are a infinite number
of models for each possible value of <span class="math inline">\(\alpha &gt; 0\)</span> . This complicates the
model selection as we have to decide the best value of $\alpha$.</p>
<p>In order to avoid this, we could choose a model that has 0 hyper
parameters and afterwards add a layer of complexity.</p>
<hr />
<p><strong>Hold-Out validation method</strong></p>
<ul>
<li><p>“simple” if no hyper parameters (training + test)</p></li>
<li><p>“nested” if there are hyper parameters (training + validation +
test)</p></li>
</ul>
<p>Average error of the test set = An estimate of the real error which the
model will exhibit on new unseen data. It is an substitute of the
infinitely many data points that my model will classify when I will use
it for real / in production.</p>
<p>We compare this estimate of the error of the test set (<span class="math inline">\(ERR_1\)</span>) with the
multiple models and choose the lowest value by definition.</p>
<p><img src="images/paste-21B7F2B9.png" /></p>
<p>We can additionally do model selection with hyper parameter tuning
recursively.</p>
<p><strong>Grid Search:</strong> specifying a set of parameters which I believe are
reasonable and try all in sequence.</p>
<p>I believe (intuition or proof of concept) any value large than 1 is
unlikely to be a good value for alpha because it would overemphasize the
penalty of the parameter. In other words, it would cause too much bias
for the reduction of the variance.<br />
</p>
<p><img src="images/paste-38135339.png" /></p>
<p><strong>Hyper parameter tuning =</strong> Holdout validations + Grid search</p>
<p><strong>Nested procedure:</strong></p>
<p><img src="images/paste-C053E1A0.png" /></p>
<p><strong>(i)</strong> For each value of hyper parameters in the grid <span class="math inline">\(\alpha\)</span></p>
<ol style="list-style-type: decimal">
<li>Train the model with the <span class="math inline">\(\alpha\)</span> on the training set</li>
<li>Estimate the error on the validation set</li>
<li>I choose the hyper parameter <span class="math inline">\(\alpha\)</span> which gives the lowest error
estimate on the validation set.</li>
</ol>
<p>Therefore, the chosen <span class="math inline">\(\alpha\)</span> is going to be the hyper parameter
configuration that I am going to use. With this value, I will evaluate
the quality model on the test set.</p>
<p>However, a model that is trained by more data typically performs better
than with little data. By splitting again, the training set becomes
smaller leading to a <em>scarcity</em> of data.</p>
<p><strong>(ii)</strong> Before passing to the next phase, with the fixed parameter,
there is an intermediate step. We retrain <span class="math inline">\(\hat f_{\alpha*}\)</span> on the
entire training + validation set.</p>
<p><strong>(iii)</strong> Estimate the <span class="math inline">\(ERR_{\alpha*,1}\)</span> using the test set.</p>
<hr />
<p><strong>Standardization</strong></p>
<ol style="list-style-type: decimal">
<li>We do not want our model to see the test data during training –&gt;
We do not want any information about the test data to be accessible
by the model during training.</li>
<li>Standardize my data set before starting. Still in the data
exploration phase.</li>
</ol>
<p>If we standardize the data in the beginning before splitting the data
into training/test, the mean and scaling will be on the entire data.
Therefore there is some information of the test set that would flow to
the model. = <strong>Information leakage</strong></p>
<p><em>Alternatively:</em></p>
<ol style="list-style-type: decimal">
<li>We first split the data.</li>
<li>We standardize the training set.</li>
<li>We apply the mean standard deviation on the test set</li>
</ol>
<p><img src="images/paste-90252CDE.png" /></p>
<hr />
<p>The “real” error of my estimator: In the limit when I use and infinite
long data set. Considering we do not have this data set, we find the
empirical estimated error of my estimator using the data in the test
set.</p>
<p>Here 1 is the real error and the 2’s are the estimates. The first has
higher variance than the second.</p>
<p><img src="images/paste-CE33D3FC.png" width="357" /></p>
<ol style="list-style-type: decimal">
<li><p>What is the variance of the estimated error <span class="math inline">\(\hat {ERR}\)</span>? A standard
trick to reduce the variance, is to increase the sample size and
take the average. f.e. bootstrap method.</p></li>
<li><p>What is the bias of estimated error <span class="math inline">\(\hat {ERR}\)</span>? We can try a
larger training set.</p>
<p><img src="images/paste-C4165CF2.png" width="270" height="100" /></p></li>
<li><p>Is my <span class="math inline">\(\hat {ERR}\)</span> consistently an <em>overestimation</em>,
<em>underestimation</em> of <span class="math inline">\(ERR\)</span>.</p></li>
</ol>
<p><span class="math inline">\(\hat f\)</span> is training on the training set: just a smaller subset of the
entire data set. We use it to compute the <span class="math inline">\(\hat {ERR}\)</span>(on the test set).
The <span class="math inline">\(\hat f\)</span> trained on the training set is worse than the <span class="math inline">\(\hat f\)</span> on
the entire data-set. The <span class="math inline">\(\hat {ERR}\)</span> obtained using the <span class="math inline">\(\hat f\)</span>
training on the training set is worse than the <span class="math inline">\(ERR\)</span> (the error made by
<span class="math inline">\(\hat f\)</span> trained on the entire data set, in the limit).</p>
<p>When I will use <span class="math inline">\(\hat f\)</span> in production, I will re-train it on the entire
data-set.</p>
<p>Therefore, <span class="math inline">\(\hat {ERR}\)</span> is likely going to be larger than the <span class="math inline">\(ERR\)</span>.
<span class="math inline">\(\hat {ERR}\)</span> will be an overestimation of the <span class="math inline">\(ERR\)</span>. For an error to be
worse, it implies it is larger than the actual error.</p>
<p>Bias is due to the overestimation of the true error. How can we reduce
the bias?</p>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="practical-data-science.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="references.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["R-Summaries.pdf", "R-Summaries.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
